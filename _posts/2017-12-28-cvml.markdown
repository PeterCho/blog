---
layout: post
title:  "Super-Resolving Single Images via Fully Convolutional Neural Networks"
date:   2017-12-28 12:00:00
categories: Deep Learning
use_math: true
---

## Image reconstruction

Restoring degraded images to their original forms represents a classic
challenge in computer vision.  Image deblurring, denoising, inpainting and
colorization all require recovering and/or hallucinating lost information.
Image reconstruction is generally an ill-defined inverse problem, for
downgraded input can map onto many possible upgraded outputs.  We
consequently must be content to generate reconstructions that appear
visually reasonable to a human eye.

Before the widespread adoption of deep learning techniques in computer
vision, researchers attempted to recover high resolution, RGB-colored
and/or defect-free pictures from low resolution, greyscale and/or defective
inputs via hand-crafted rules.  Today, Convolutional Neural Network
$($CNN$)$ approaches can automatically identify structure present in
real-world imagery.  Moreover, the internet provides a nearly limitless
source for unsupervised training data which is inexpensive to harvest.
Starting with crisp, clean and fully colored photos, one simply blurs,
introduces artifacts and/or removes color channels to produce degraded
versions.  The downgraded pictures become CNN inputs, while their
progenitors enter into training loss functions.  Neural networks can
consequently exploit vast quantities of data to learn typical patterns that
may be used to fill in missing imagery information.

In this blog post, we investigate single image super-resolution via fully
convolutional neural networks.  In particular, we want to upsample small
images to larger versions which are as sharp as possible.  As figure 1
illustrates, pixel interpolation establishes a relatively poor baseline for
image upsampling.  In contrast, the original progenitor for the subsampled
image represents an ideal reconstruction.  We will compare various
super-resolution results to these two limiting cases in order to
qualitatively and quantitatively evaluate restoration performance.

![SuperRes]({{site.url}}/blog/images/superres/padded_flower_upsampled.jpg)
*Fig. 1.  Different upsamplings of an input 64x64 flower image to an output
picture with 256x256 pixel size.*

## CNN architecture and loss functions

A number of authors have recently explored feed-forward models to
super-resolve single images.  For example, the Super-Resolution
Convolutional Neural Network $($SRCNN$)$ of Dong et al utilized a 3 layer
convolutional network which imports low resolution images upscaled via
bicubic interpolation [1].  Johnson et al worked with end-to-end ResNets
containing 12 - 13 convolutional layers [2,3].  Tai et al investigated even
deeper ResNet architectures with up to 52 convolutional layers which were
trained via recursive learning [4].  These investigators and others have
reported impressive super-resolution results using CNNs.

Drawing inspiration from earlier studies, we experimented with a variety of
fully convolutional networks.  We found the following "super-layer"
combinations of convolutions with 5x5 kernels, batch normalization [5],
leaky ReLU activation [6], DenseNets with 3x3 kernels [7], and transposed
convolutions with 3x3 kernels [8,9] to be useful CNN building blocks:

$\qquad$ CBL: Conv5, Batch norm, Leaky ReLU

$\qquad$ Dense-n: n-layer DenseNet with Batch norm and Leaky ReLU but no bottleneck layer

$\qquad$ UP-conv: Tranposed Conv3

We assembled these building blocks into the fully convolutional
architecture outlined in table 1 which performs 4X super-resolution of RGB
images and contains 1.17M trainable parameters:

<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: left">Super layer</th>
      <th style="text-align: center">Activation volume [width x height x
channels] </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Input image</td>
      <td style="text-align: center">64 x 64 x 3</td>
    </tr>
    <tr>
      <td style="text-align: left">CBL</td>
      <td style="text-align: center">64 x 64 x 64</td>
    </tr>
    <tr>
      <td style="text-align: left">Dense-14</td>
      <td style="text-align: center">64 x 64 x 32</td>
    </tr>
    <tr>
      <td style="text-align: left">UP-conv, stride 1/2</td>
      <td style="text-align: center">128 x 128 x 64</td>
    </tr>
    <tr>
      <td style="text-align: left">UP-conv, stride 1/2</td>
      <td style="text-align: center">256 x 256 x 64</td>
    </tr>
    <tr>
      <td style="text-align: left">CBL</td>
      <td style="text-align: center">256 x 256 x 3</td>
    </tr>
</tbody>
</table>

*Table 1.  CNN architecture for 4X super-resolution of input RGB imagery.*

In addition to trying various network architectures, we also explored
different loss functions.  The first is a per-pixel Euclidean loss which is
commonly used in image reconstruction problems.  It computes the L2 norm of
the difference between a ground-truth progenitor image and its
reconstructed counterpart.  The second is a perceptual loss which measures
the L1 norm of the difference between descriptors for progenitor and
reconstructed images computed via pretrained CNNs.  Following Johnson et al
[2], we employ the relu2_2 layer from the VGG-16 model [10] to generate
image descriptors.

A third loss function we consider is based upon residual differences
between high-resolution progenitor images and their low-resolution
interpolated counterparts.  As figure 2 illustrates, such differences
contain high spatial frequency components present in the former and missing
in the latter.  We are consequently motivated to train a CNN to output high
frequency contents given low frequency inputs.  Our third loss function
computes the L2 norm of differences between high spatial frequency residual
images like that appearing in figure 2 and their predicted CNN analogues.

![SuperRes]({{site.url}}/blog/images/superres/hires_lores_residual2.jpg)
*Fig. 2.  The difference between a progenitor image and its interpolated
low-resolution counterpart contains high spatial frequency content.*

## CNN training and validation

Programming in python, we build the CNN architecture and loss functions
within the TensorFlow deep learning framework [11].  We also download a
subset of the Open Images Dataset to serve as training, validation and
testing data [12].  CNN training depends upon a number of hyperparameters
which must be empirically set.  Table 2 summarizes the hyperparameter
values that we settled upon after running 30+ training experiments.

<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: left">Hyperparameter description</th>
      <th style="text-align: center">Value    </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Input image size</td>
      <td style="text-align: center">64 x 64</td>
    </tr>
    <tr>
      <td style="text-align: left">Output image size</td>
      <td style="text-align: center">256 x 256</td>
    </tr>
    <tr>
      <td style="text-align: left">Number of training images</td>
      <td style="text-align: center">108000</td>
    </tr>
    <tr>
      <td style="text-align: left">Number of validation images</td>
      <td style="text-align: center">4320</td>
    </tr>
    <tr>
      <td style="text-align: left">Minibatch size</td>
      <td style="text-align: center">25</td>
    </tr>
    <tr>
      <td style="text-align: left">Optimizer type</td>
      <td style="text-align: center">SGD with momentum</td>
    </tr>
    <tr>
      <td style="text-align: left">Momentum coefficient</td>
      <td style="text-align: center">0.9</td>
    </tr>
    <tr>
      <td style="text-align: left">Base learning rate</td>
      <td style="text-align: center">0.01</td>
    </tr>
    <tr>
      <td style="text-align: left">Learning rate decay coefficient</td>
      <td style="text-align: center">0.333</td>
    </tr>
    <tr>
      <td style="text-align: left">Learning rate decay period</td>
      <td style="text-align: center">27000 training steps</td>
    </tr>
    <tr>
      <td style="text-align: left">Leaky ReLU slope coefficient</td>
      <td style="text-align: center">0.1</td>
    </tr>
</tbody>
</table>

*Table 2.  List of hyperparameters employed for 4X super-resolution CNN
training and validation.*

Working with these hyperparameters and the high-spatial frequency residual
loss function, we first train a CNN to perform 4X super-resolution.  Figure
3 plots the network's training and validation learning curves generated
after 18 hours on a single GTX 1080 Ti GPU.  The Tensorboard traces
appearing in the figure exhibit significant fluctuations about their
temporally smoothed means.  Yet both learning curves asymptote to
approximately the same average regression error.  So our CNN does not
appear to significantly overfit or underfit.

![SuperRes]({{site.url}}/blog/images/superres/train_valid_error.png)
*Fig. 3.  Tensorboard display of mean regression errors between predicted
and ground truth high-spatial frequency contents for training $($orange$)$
and validation $($green$)$ image samples.*

Once training concludes, it is instructive to compare CNN inputs and
outputs.  Figure 4 illustrates four selected examples of 4X super-resolved
validation images.  In the figure's upper-left quadrants, 64x64 inputs to
the CNN are displayed after they are interpolated up to 256x256.  High
spatial frequency contents predicted by the CNN appear in the figure's
upper-right quadrants.  Sums of low plus high spatial frequencies yield the
image reconstructions within the lower-left quadrants.  For comparison, the
progenitor 256x256 images representing ideal reconstructions are exhibited
in the lower-right quadrants.

![SuperRes]({{site.url}}/blog/images/superres/highpass_montage.jpg)
*Fig. 4.  Examples of 4X super-resolution results derived via the residual
loss function on validation samples.*

Looking at these super-resolution results, we observe that the CNN with
residual loss function recovers much, but not all, of the high spatial
frequency content missing from the input images.  In the car radio example
of figure 4, we can read the pink "96.80" station label in the
super-resolved output that is difficult to decipher in the low resolution
input.  But it is still nearly impossible to make out the smaller white
characters printed on the radio's buttons.  Restoration of narrow leaf
veins, flying fire embers and dancer costume sequins in figure 4 are also
noteworthy super-resolution successes.

It is instructive to compare CNN outputs resulting from different loss
functions while all other hyperparameters are held fixed.  In figure 5a, we
again see the 64x64 flower image following up-interpolation alongside its
256x256 progenitor.  Figure 5b exhibits 4X super-resolved CNN outputs based
upon the per-pixel, perceptual and high spatial frequency residual loss
functions.  At first glance, the three super-resolved pictures look nearly
identical.  But if we zoom into figure 5b, we observe that the perceptual
loss function result suffers from "checkerboard" pixelation which is not
present in the other two output pictures.  Johnson et al note that the
feature reconstruction loss gives rise to a slight cross-hatch pattern
visible under magnification [2].  Similar checkerboard patterning can be
seen after zooming into the perceptual loss result presented in figure 6.

![SuperRes]({{site.url}}/blog/images/superres/padded_lores_hires_flower.jpg)
*Fig. 5a.  Low resolution input and ideal high resolution output flower images.*

![SuperRes]({{site.url}}/blog/images/superres/montage_flower.jpg) *Fig. 5b.
Reconstructed flower images coming from pixel, perceptual and residual loss
functions.  Reconstruction differences can be seen by zooming into these
pictures.*

![SuperRes]({{site.url}}/blog/images/superres/padded_lores_hires_cars.jpg)
*Fig. 6a.  Low resolution input and ideal high resolution car images.*

![SuperRes]({{site.url}}/blog/images/superres/montage_cars.jpg) *Fig. 6b.
Reconstructed car images coming from pixel, perceptual and residual loss
functions.  Reconstruction differences can be seen by zooming into these
pictures.*

In order to quantitatively compare super-resolution outputs from the three
different CNN loss functions, we compute their peak signal-to-noise ratios
$($PSNR$)$.  This performance metric is often reported in the
super-resolution literature, and it correlates somewhat with image
restoration quality.  Following [1,2], we first evaluate Y channel PSNR
after converting progenitor and reconstructed images to YCbCr colorspace.
Working with 48 pairs of 4X super-resolved and progenitor validation
images, we find PSNR = 29.79 for the per-pixel loss function, 29.81 for the
perceptual loss function, and 30.21 for the residual loss function.  If we
instead evaluate PSNR for three color-channel RGB images, we obtain PSNR =
22.68 for the per-pixel loss function, 17.75 for the perceptual loss
function and 24.43 for the residual loss function.

Based upon these qualitative and quantiative comparisons, we conclude the
high spatial frequency residual loss function yields the best CNN
super-resolution performance.

## Super-resolution inference 

The neural network architecture in Table 1 is fully convolutional.  It
consequently may be applied to input images of any pixel size.  Figure 7
exhibits 4X super-resolved images derived from the Tensorflow model which
generated the validation results in figure 4.  The heights and widths of
the low-resolution input and reconstructed output images are indicated in
the figure.  Zooming into the figure renders apparent high spatial
frequency content recovered by the CNN.  It is particularly interesting to
observe restored crack details within the top stone picture.  Similarly,
super-resolution significantly sharpens blurry background text in the
bottom "American Heart Association" photo.

![SuperRes]({{site.url}}/blog/images/superres/crack_montage.jpg)

![SuperRes]({{site.url}}/blog/images/superres/red_car_montage.jpg)

![SuperRes]({{site.url}}/blog/images/superres/heart_assoc_montage.jpg)

*Fig. 7.  Three examples of full-image upsampling for pictures of various
pixel sizes.  $\small{(Left)}$ Low resolution input images upsampled 4X in
both horizontal and vertical directions via interpolation.
$\small{(Center)}$ Super-resolved CNN outputs based upon the residual loss
function.  $\small{(Right)}$ High resolution image progenitors.*

In theory, we can transform arbitrarily small input images into large-size
outputs by adding more up-convolutions to the CNN architecture in Table 1.
In practice, increasing super-resolution magnification yields diminishing
returns as fine detail is irretrievably lost.  Yet as figure 8
demonstrates, even 8X CNN super-resolution is meaningfully possible.

![SuperRes]({{site.url}}/blog/images/superres/montage_cifar.png)
*Fig. 8. Examples of 32x32 CIFAR images interpolated up to 256x256 and their
8X super-resolved counterparts.*

The figure illustrates natural and man-made objects from selected CIFAR-10
images [13].  The objects in the 32x32 pictures interpolated up to 256x256
are recognizable but very blurry.  In contrast, their 8X super-resolved
counterparts look surprisingly crisp.  We do not have access to the
original images from which CIFAR samples were created.  So this final
example represents a "real-world" application of single image
super-resolution.

## References

1.  [C. Dong, C.C. Loy, K. He and X. Tang, "Image Super-Resolution Using
Deep Convolutional Networks," IEEE Transactions on Pattern Analysis and
Machine Intelligence $\small{(2015)}$.](https://arxiv.org/pdf/1501.00092.pdf)

2.  [J. Johnson, A. Alahi and L. Fei-Fei, "Perceptual Losses for Real-Time
Style Transfer and Super-Resolution," ECCV $\small{(2016)}$.](https://arxiv.org/abs/1603.08155)

3.  [J. Johnson, A. Alahi and L. Fei-Fei, "Perceptual Losses for Real-Time
Style Transfer and Super-Resolution: Supplementary Material," $\small{(2016)}$.](https://cs.stanford.edu/people/jcjohns/papers/fast-style/fast-style-supp.pdf)

4.  [Y. Tai, J. Yang and X. Liu, "Image Super-Resolution via Deep Recursive
Residual Network," CVPR $\small{(2017)}$.](https://pdfs.semanticscholar.org/a559/70013b984f344dfbbbba677d89dce0ba5f81.pdf)

5.  [S. Ioffe and C. Szegedy, "Batch Normalization: Accelerating Deep
Network Training by Reducing Internal Covariate Shift," 32nd ICML 
$\small{(2015)}$.](https://arxiv.org/abs/1502.03167)

6.  [A.L. Maas, A.Y. Hannun and A.Y. Ng, "Rectifier Nonlinearities Improve
Neural Network Acoustic Models", 50th ICML $\small{(2013)}$.](https://web.stanford.edu/~awni/papers/relu_hybrid_icml2013_final.pdf)

7.  [G. Huang, Z. Liu, K.Q. Weinberger, L. van der Maaten, "Densely
Connected Convolutional Networks," CVPR $\small{(2017)}$.](https://arxiv.org/abs/https://arxiv.org/abs/1608.06993)

8.  [V. Dumoulin and F. Visin, "A Guide to Convolution Arithmetic for Deep
Learning," arXiv:1603.07285 v1 $\small{(2016)}$.](https://ai2-s2-pdfs.s3.amazonaws.com/7918/2aab186f0b68a8432540d8695e1646338479.pdf)

9.  [F.-F. Li, J. Johnson and S. Yeung, "CS-231N course slides, lecture 11,
Detection and Segmentation," slides 28 - 43
$\small{(2017)}$.](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf)

10.  [K. Simonyan and A. Zisserman, "Very Deep Convolutional Networks for
Large-Scale Image Recognition," ICLR $\small{(2015)}$.](https://arxiv.org/pdf/1409.1556.pdf)

11.  [See TensorFlow: An open-source software library for Machine Intelligence.](https://www.tensorflow.org/)

12.  [See Open Images Dataset V3.](https://github.com/openimages/dataset)

13.  [A. Krizhevsky, "Learning Multiple Layers of Features from Tiny
Images," Technical Report $\small{(2009)}$.](http://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf)
