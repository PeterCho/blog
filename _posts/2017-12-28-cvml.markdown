---
layout: post
title:  "Super-Resolving Individual Images via Fully Convolutional Neural Networks"
date:   2017-12-28 12:00:00
categories: Deep Learning
use_math: true
---

## Image reconstruction

Reconstructing degraded images to their original forms represents a classic
problem in computer vision.  Image deblurring, denoising, inpainting and
colorization all require recovering lost information.  Image restoration
problems represent ill-defined inverse problems, for downgraded input
generally maps onto many possible outputs.  We consequently should be
content to generate reconstructions that appear visually reasonable to the
human eye.

Before the widespread adoption of deep learning techniques within computer
vision, researchers attempted to recover higher resolution, RGB-colored
and/or defect-free pictures from low resolution, greyscale and/or defective
inputs via hand-crafted rules.  Today, Convolutional Neural Network (CNN)
approaches can automatically identify patterns present in real-world
imagery.  Moreover, the internet provides a nearly limitless source for CNN
training data which is inexpensive to harvest.  Starting with crisp, clean
and fully colored photos, one simply blurs, introduces artifacts and/or
removes color channels to obtain downgraded versions.  The resulting
downgraded pictures become CNN inputs, while their progenitors enter into
training loss functions.  Neural networks can consequently exploit vast
quantities of data to learn typical patterns that may be used to fill in
missing imagery information.

In this blog post, we investigate single image super-resolution via fully
convolutional neural networks.  In particular, we want to upsample small
images to larger versions which are as sharp as possible.  As figure 1
illustrates, pixel interpolation establishes a relatively poor baseline for
image upsampling.  In contrast, the original progenitor for the subsampled
image represents an ideal reconstruction.  We will compare various
super-resolution results to these two limiting cases in order to
qualitatively and quantitatively evaluate restoration performance.

![SuperRes]({{site.url}}/blog/images/superres/padded_flower_upsampled.jpg)
*Fig. 1.  Different upsamplings of an input 64x64 flower image to an output
picture with 256x256 pixel size.*

## CNN architecture and loss functions

A number of authors have recently explored CNN feed-forward models to
super-resolve input images.  For example, the Super-Resolution
Convolutional Neural Network $($SRCNN$)$ of Dong et al utilizes a 3
convolutional layer network with a per-pixel loss function that operates on
low resolution images upscaled via bicubic interpolation [1].  Johnson et
al worked with end-to-end ResNets containing 12 - 13 convolutional layers
[2,3].  Tai et al investigated even deeper ResNet architectures with up to
52 convolutional layers which were trained via recursive learning [4].
These investigators and others have reported impressive super-resolution
results using CNNs.

Drawing inspiration from earlier studies, we experimented with a variety of
fully convolutional networks.  We found the following "super-layer"
combinations of convolutions with 5x5 kernels, batch normalization [5],
leaky ReLU activation [6], DenseNets with 3x3 kernels [7], and transposed
convolutions with 3x3 kernels [8] to be useful CNN building blocks:

$\qquad$ CBL: Conv5, Batch norm, Leaky ReLU

$\qquad$ Dense-n: n-layer DenseNet with Batch norm and Leaky ReLU but no bottleneck layer

$\qquad$ UP-conv: Tranposed Conv3

We assembled these building blocks into the fully convolutional
architecture presented in table 1 which performs 4X super-resolution of RGB
images and contains 1.17M trainable parameters:

<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: left">Super layer</th>
      <th style="text-align: center">Activation volume [width x height x
channels] </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Input image</td>
      <td style="text-align: center">64 x 64 x 3</td>
    </tr>
    <tr>
      <td style="text-align: left">CBL</td>
      <td style="text-align: center">64 x 64 x 64</td>
    </tr>
    <tr>
      <td style="text-align: left">Dense-14</td>
      <td style="text-align: center">64 x 64 x 32</td>
    </tr>
    <tr>
      <td style="text-align: left">UP-conv, stride 1/2</td>
      <td style="text-align: center">128 x 128 x 64</td>
    </tr>
    <tr>
      <td style="text-align: left">UP-conv, stride 1/2</td>
      <td style="text-align: center">256 x 256 x 64</td>
    </tr>
    <tr>
      <td style="text-align: left">CBL</td>
      <td style="text-align: center">256 x 256 x 3</td>
    </tr>
</tbody>
</table>

*Table 1.  CNN architecture for 4X super-resolution of an input RGB image.*

In addition to trying various network architectures, we also explored
different loss functions.  The first is a per-pixel Euclidean loss which is
commonly used in image reconstruction problems.  It computes the L2 norm of
the difference between a ground-truth progenitor image and its
reconstructed counterpart.  The second is a perceptual loss which measures
the L1 norm of the difference between descriptors for progenitor and
reconstructed images computed via pretrained CNNs.  Following Johnson et al
[2], we use the relu2_2 layer from the VGG-16 model [9] to generate image
descriptors.

The final loss function we consider is based upon residual differences
between progenitor high-resolution images and their interpolated
low-resolution counterpats.  As figure 2 illustrates, such differences
contain high spatial frequency components present in the former and missing
in the latter.  We are consequently motivated to train a CNN to output high
frequency contents given low frequency inputs.  Our third loss function
computes the L2 norm of differences between high spatial frequency images
like that appearing in figure 3 and their predicted CNN analogues.

![SuperRes]({{site.url}}/blog/images/superres/hires_lores_residual2.jpg)
*Fig. 2.  The difference between the progenitor flower image and its
interpolated low-resolution counterpart contains high spatial frequency
content.*

## CNN training and validation

Programming in python, we build the CNN architecture and loss functions
within the TensorFlow deep learning framework [10].  We also download a
subset of the Open Images Dataset to serve as training, validation and
testing data [11].  CNN training depends upon a number of hyperparameters
which must be empirically set.  Table 2 summarizes the hyperparameter
values that we settled upon after running 30+ training experiments.

<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: left">Hyperparameter description</th>
      <th style="text-align: center">Value    </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Input image size</td>
      <td style="text-align: center">64 x 64</td>
    </tr>
    <tr>
      <td style="text-align: left">Output image size</td>
      <td style="text-align: center">256 x 256</td>
    </tr>
    <tr>
      <td style="text-align: left">Number of training images</td>
      <td style="text-align: center">108000</td>
    </tr>
    <tr>
      <td style="text-align: left">Number of validation images</td>
      <td style="text-align: center">4320</td>
    </tr>
    <tr>
      <td style="text-align: left">Minibatch size</td>
      <td style="text-align: center">25</td>
    </tr>
    <tr>
      <td style="text-align: left">Optimizer type</td>
      <td style="text-align: center">SGD with momentum</td>
    </tr>
    <tr>
      <td style="text-align: left">Momentum coefficient</td>
      <td style="text-align: center">0.9</td>
    </tr>
    <tr>
      <td style="text-align: left">Learning rate decay coefficient</td>
      <td style="text-align: center">0.333</td>
    </tr>
    <tr>
      <td style="text-align: left">Learning rate decay period</td>
      <td style="text-align: center">27000 training steps</td>
    </tr>
    <tr>
      <td style="text-align: left">Base learning rate</td>
      <td style="text-align: center">0.01</td>
    </tr>
    <tr>
      <td style="text-align: left">Leaky ReLU slope coefficient</td>
      <td style="text-align: center">0.1</td>
    </tr>
</tbody>
</table>

*Table 2.  List of hyperparameters employed for 4X super-resolution CNN
training and validation.*

Utilizing the hyperparameters in Table 2 and working with the residual
high-spatial frequency loss function, we first train a CNN to perform 4X
super-resolution.  Its training and validation learning curves generated
after running for 18 hours on a single GTX 1080 Ti GPU are illustrated in
figure 3.  The Tensorboard traces for both curves exhibit significant
fluctuations about their time-averaged means.  Yet both learning curves
asymptote to approximately the same mean regression errors.  So our CNN
does not appear to significantly overfit or underfit.  

![SuperRes]({{site.url}}/blog/images/superres/train_valid_error.png)
*Fig. 3.  Tensorboard display of mean regression errors between predicted
and ground truth high-spatial frequency contents for training $($orange$)$
and validation $($green$)$ image samples.*

Once training concludes, it is instructive to compare CNN inputs and
outputs.  Figure 4 illustrates four selected examples of 4X super-resolved
validation images.  In the figure's upper-left quadrants, 64x64 inputs to
the CNN are displayed after interpolated upsampling to 256x256.  High
spatial frequency contents predicted by the CNN appear in the figure's
upper-right quadrants.  Sums of low plus high spatial frequencies yield the
image reconstructions presented in the lower-left quadrants.  For
comparison, the progenitor 256x256 images representing ideal
reconstructions are exhibited in the lower-right quadrants.

![SuperRes]({{site.url}}/blog/images/superres/highpass_montage.jpg)
*Fig. 4.  Representative examples of high-pass residual super-resolution
results on validation samples.*

Looking at these super-resolution results, we observe that our CNN with the
high-pass residual loss function recovers much, but not all, of the high
spatial frequency content missing from the input images.  For instance in
the first car radio example in figure 4, we can readily read the red
"96.80" FM station characters in the super-resolved output but not within
the low resolution input.  But it is still nearly impossible to decipher
the smaller white characters printed on the radio's buttons.  Restoration
of leaf edges and veins, flying fire embers and dancer costume sequins in
figure 4 are also noteworthy qualitative successes of CNN super-resolution.

It is instructive to compare variations in super-resolution results coming
from different loss functions while holding all other CNN hyperparameters
fixed.  In figure 5a, we see again the 64x64 flower image following
up-interpolation alongside its 256x256 progenitor.  Figure 5b exhibits 4X
super-resolved outputs from our CNN using the per-pixel, perceptual and
high spatial frequency residual loss functions.  To first approximation,
the three individual super-resolution results look similar.  But if we zoom
into figure 5b, we observe that the perceptual loss function result
exhibits "checkerboard patterning" which is not present in the other two
loss function outputs.  Johnson et al note that the feature reconstruction
loss gives rise to a slight cross-hatch pattern visible under
mangnification [2].  A similar checkerboard pattern can be seen after
zooming within the perceptual loss result presented in figure 6.

![SuperRes]({{site.url}}/blog/images/superres/padded_lores_hires_flower.jpg)
*Fig. 5a.  Low resolution input and ideal high resolution output flower images.*

![SuperRes]({{site.url}}/blog/images/superres/montage_flower.jpg)
*Fig. 5b.  Reconstructed flower images coming from pixel, perceptual and
highpass loss functions.  Reconstruction differences can be seen by zooming
into these pictures.*

![SuperRes]({{site.url}}/blog/images/superres/padded_lores_hires_cars.jpg)
*Fig. 6a.  Low resolution input and ideal high resolution car images.*

![SuperRes]({{site.url}}/blog/images/superres/montage_cars.jpg)
*Fig. 6b.  Reconstructed car images coming from pixel, perceptual and
highpass loss functions.*

In order to quantitatively compare super-resolution results coming from the
three different CNN loss functions, we compute their PSNR values.




## Super-resolution inference results

![SuperRes]({{site.url}}/blog/images/superres/crack_montage.jpg)

![SuperRes]({{site.url}}/blog/images/superres/red_car_montage.jpg)

![SuperRes]({{site.url}}/blog/images/superres/heart_assoc_montage.jpg)

*Fig. 7.  Three examples of full-image upsampling for pictures of various
pixel sizes.  $\small{(Left)}$ Image inputs downsampled by 4X in both the
horizontal and vertical directions.  $\small{(Center)}$ Super-resolved
image outputs from the CNN.  $\small{(Right)}$ Original image progenitors.*


![SuperRes]({{site.url}}/blog/images/superres/montage_cifar.png)
*Fig. 8. Examples of 32x32 CIFAR images and their super-resolved 256x256
counterparts.*


## References

1.  [C. Dong, C.C. Loy, K. He and X. Tang, "Image Super-Resolution Using
Deep Convolutional Networks," $\small{(2015)}$.](https://arxiv.org/pdf/1501.00092.pdf)

2.  [J. Johnson, A. Alahi and L. Fei-Fei, "Perceptual Losses for Real-Time
Style Transfer and Super-Resolution," ECCV $\small{(2016)}$.](https://arxiv.org/abs/1603.08155)

3.  [J. Johnson, A. Alahi and L. Fei-Fei, "Perceptual Losses for Real-Time
Style Transfer and Super-Resolution: Supplementary Material," $\small{(2016)}$.](https://cs.stanford.edu/people/jcjohns/papers/fast-style/fast-style-supp.pdf)

4.  [Y. Tai, J. Yang and X. Liu, "Image Super-Resolution via Deep Recursive
Residual Network," CVPR $\small{(2017)}$.](https://pdfs.semanticscholar.org/a559/70013b984f344dfbbbba677d89dce0ba5f81.pdf)

5.  [S. Ioffe and C. Szegedy, "Batch Normalization: Accelerating Deep
Network Training by Reducing Internal Covariate Shift," 32nd ICML 
$\small{(2015)}$.](https://arxiv.org/abs/1502.03167)

6.  [A.L. Maas, A.Y. Hannun and A.Y. Ng, "Rectifier Nonlinearities Improve
Neural Network Acoustic Models", 50th ICML $\small{(2013)}$.](https://web.stanford.edu/~awni/papers/relu_hybrid_icml2013_final.pdf)

7.  [G. Huang, Z. Liu, K.Q. Weinberger, L. van der Maaten, "Densely
Connected Convolutional Networks," CVPR $\small{(2017)}$.](https://arxiv.org/abs/https://arxiv.org/abs/1608.06993)

8.  [V. Dumoulin and F. Visin, "A Guide to Convolution Arithmetic for Deep
Learning," arXiv:1603.07285 v1 $\small{(2016)}$.](https://ai2-s2-pdfs.s3.amazonaws.com/7918/2aab186f0b68a8432540d8695e1646338479.pdf)

9.  [K. Simonyan and A. Zisserman, "Very Deep Convolutional Networks for
Large-Scale Image Recognition," ICLR $\small{(2015)}$.](https://arxiv.org/pdf/1409.1556.pdf)

10.  [See TensorFlow: An open-source software library for Machine Intelligence.](https://www.tensorflow.org/)

11.  [See Open Images Dataset V3.](https://github.com/openimages/dataset)
