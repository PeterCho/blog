---
layout: post
title:  "Super-Resolving Individual Images via Fully Convolutional Neural Networks"
date:   2017-12-28 12:00:00
categories: Deep Learning
use_math: true
---

## Image reconstruction

Reconstructing degraded images to their original forms represents a classic
problem in computer vision.  Image deblurring, denoising, inpainting and
colorization all require recovering lost information.  Image restoration
problems represent ill-defined inverse problems, for downgraded input
generally maps onto many possible outputs.  We consequently should be
content to generate reconstructions that appear visually reasonable to the
human eye.

Before the widespread adoption of deep learning techniques within computer
vision, researchers attempted to recover higher resolution, RGB-colored
and/or defect-free pictures from low resolution, greyscale and/or defective
inputs via hand-crafted rules.  Today, Convolutional Neural Network (CNN)
approaches can automatically identify patterns present in real-world
imagery.  Moreover, the internet provides a nearly limitless source for CNN
training data which is inexpensive to harvest.  Starting with crisp, clean
and fully colored photos, one simply blurs, introduces artifacts and/or
removes color channels to obtain downgraded versions.  The resulting
downgraded pictures become CNN inputs, while their progenitors enter into
training loss functions.  Neural networks can consequently exploit vast
quantities of data to learn typical patterns that may be used to fill in
missing imagery information.

In this blog post, we investigate single image super-resolution via fully
convolutional neural networks.  In particular, we want to upsample small
images to larger versions which are as sharp as possible.  As figure 1
illustrates, pixel interpolation establishes a relatively poor baseline for
image upsampling.  In contrast, the original progenitor for the subsampled
image represents an ideal reconstruction.  We will compare various
super-resolution results to these two limiting cases in order to
qualitatively and quantitatively evaluate restoration performance.

![SuperRes]({{site.url}}/blog/images/superres/padded_flower_upsampled.jpg)
*Fig. 1.  Different upsamplings of an input 64x64 image to an output picture
with 256x256 pixel size.*

## CNN architecture, loss functions and training

A number of authors have recently explored CNN feed-forward models to
super-resolve input images.  For example, the Super-Resolution
Convolutional Neural Network (SRCNN) of Dong et al utilizes a 3
convolutional layer network with a per-pixel loss function that operates on
low resolution images upscaled via bicubic interpolation [1].  Johnson et
al worked with end-to-end ResNets containing 12 or 13 convolutional layers
and a perceptual loss function [2,3].  Tai et al investigated even deeper
ResNet architectures with up to 52 convolutional layers which were trained
via recursive learning [4].

Drawing inspiration from these previous studies, we experimented with a
variety of fully convolutional networks.  We found the following
"super-layer" combinations of convolutions with 5x5 kernels, batch
normalization [5], leaky ReLU activation [6], DenseNets with 3x3 kernels
[7], and transposed convolutions with 3x3 kernels [8] to be useful CNN
building blocks:

CBL: Conv5, Batch norm, Leaky ReLU

Dense-n: n-layer DenseNet with Batch norm and Leaky ReLU but no bottleneck layer

UP-conv: Tranposed Conv3

We assembled these building blocks into the CNN architecture presented in
table 1 which we employed for 4X image super-resolution.  This particular
CNN contains 1.17M trainable parameters.

<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: left">Super layer</th>
      <th style="text-align: center">Activation volume [width x height x
channels] </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Input image</td>
      <td style="text-align: center">64 x 64 x 3</td>
    </tr>
    <tr>
      <td style="text-align: left">CBL</td>
      <td style="text-align: center">64 x 64 x 64</td>
    </tr>
    <tr>
      <td style="text-align: left">Dense-14</td>
      <td style="text-align: center">64 x 64 x 32</td>
    </tr>
    <tr>
      <td style="text-align: left">UP-conv, stride 1/2</td>
      <td style="text-align: center">128 x 128 x 64</td>
    </tr>
    <tr>
      <td style="text-align: left">UP-conv, stride 1/2</td>
      <td style="text-align: center">256 x 256 x 64</td>
    </tr>
    <tr>
      <td style="text-align: left">CBL</td>
      <td style="text-align: center">256 x 256 x 3</td>
    </tr>
</tbody>
</table>

*Table 1.  CNN architecture for 4X super-resolution of an input RGB image.*


We also explored different loss functions.



- Per-pixel loss

- Perceptual loss

- High spatial frequency loss







<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: left">Hyperparameter description</th>
      <th style="text-align: center">Value    </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Input image size</td>
      <td style="text-align: center">64 x 64</td>
    </tr>
    <tr>
      <td style="text-align: left">Output image size</td>
      <td style="text-align: center">256 x 256</td>
    </tr>
    <tr>
      <td style="text-align: left">Number of training images</td>
      <td style="text-align: center">108000</td>
    </tr>
    <tr>
      <td style="text-align: left">Number of validation images</td>
      <td style="text-align: center">4320</td>
    </tr>
    <tr>
      <td style="text-align: left">Minibatch size</td>
      <td style="text-align: center">25</td>
    </tr>
    <tr>
      <td style="text-align: left">Optimizer type</td>
      <td style="text-align: center">SGD with momentum</td>
    </tr>
    <tr>
      <td style="text-align: left">Momentum coefficient</td>
      <td style="text-align: center">0.9</td>
    </tr>
    <tr>
      <td style="text-align: left">Learning rate decay coefficient</td>
      <td style="text-align: center">0.333</td>
    </tr>
    <tr>
      <td style="text-align: left">Learning rate decay period</td>
      <td style="text-align: center">27000 training steps</td>
    </tr>
    <tr>
      <td style="text-align: left">Base learning rate</td>
      <td style="text-align: center">0.01</td>
    </tr>
    <tr>
      <td style="text-align: left">Leaky ReLU slope coefficient</td>
      <td style="text-align: center">0.1</td>
    </tr>
</tbody>
</table>

*Table 2.  List of hyperparameters employed for 4X super-resolution CNN
training and validation.*



Training and validation learning curves are plotted in figure 2.

![SuperRes]({{site.url}}/blog/images/superres/train_valid_error.png)
*Fig. 2.  Tensorboard display of mean regression errors between predicted
and ground truth high-spatial frequency contents for training $($orange$)$
and validation $($green$)$ image samples.*

## Super-resolution results

Figure 3 illustrates four examples of super-resolved images.  In each
example, 64x64 inputs to the CNN are displayed after interpolation to
256x256 within the figure's upper-left quadrants.  High-resolution contents
predicted by the CNN appear in the figure's upper-right quadrants.  The
sums of low plus high spatial frequencies yield the image reconstructions
presented in lower-left quadrants.  For comparison, the original 256x256
images which represent ideal reconstructions are shown in the lower-right
quadrants.

![SuperRes]({{site.url}}/blog/images/superres/highpass_montage.jpg)
*Fig. 3.  Representative examples of high-pass residual super-resolution
results.*


![SuperRes]({{site.url}}/blog/images/superres/padded_lores_hires_flower.jpg)
*Fig. 4a.  Low and high resolution flower images.*

![SuperRes]({{site.url}}/blog/images/superres/montage_flower.jpg)
*Fig. 4b.  Reconstructed flower images coming from pixel, perceptual and
highpass loss functions.  Reconstruction differences can be seen by zooming
into these pictures.*

![SuperRes]({{site.url}}/blog/images/superres/padded_lores_hires_cars.jpg)
*Fig. 5a.  Low and high resolution car images.*

![SuperRes]({{site.url}}/blog/images/superres/montage_cars.jpg)
*Fig. 5b.  Reconstructed car images coming from pixel, perceptual and
highpass loss functions.*

![SuperRes]({{site.url}}/blog/images/superres/red_car_montage.jpg)
![SuperRes]({{site.url}}/blog/images/superres/crack_montage.jpg)
![SuperRes]({{site.url}}/blog/images/superres/heart_assoc_montage.jpg)

*Fig. 6.  Three examples of full-image upsampling for pictures of various
pixel sizes.  $\small{(Left)}$ Image inputs downsampled by 4X in both the
horizontal and vertical directions.  $\small{(Center)}$ Super-resolved
image outputs from the CNN.  $\small{(Right)}$ Original image progenitors.*


![SuperRes]({{site.url}}/blog/images/superres/montage_cifar.png)
*Fig. 7. Examples of 32x32 CIFAR images and their super-resolved 256x256
counterparts.*


## References

1.  [C. Dong, C.C. Loy, K. He and X. Tang, "Image Super-Resolution Using
Deep Convolutional Networks," $\small{(2015)}$.](https://arxiv.org/pdf/1501.00092.pdf)

2.  [J. Johnson, A. Alahi and L. Fei-Fei, "Perceptual Losses for Real-Time
Style Transfer and Super-Resolution," ECCV $\small{(2016)}$.](https://arxiv.org/abs/1603.08155)

3.  [J. Johnson, A. Alahi and L. Fei-Fei, "Perceptual Losses for Real-Time
Style Transfer and Super-Resolution: Supplementary Material," $\small{(2016)}$.](https://cs.stanford.edu/people/jcjohns/papers/fast-style/fast-style-supp.pdf)

4.  [Y. Tai, J. Yang and X. Liu, "Image Super-Resolution via Deep Recursive
Residual Network," CVPR $\small{(2017)}$.](https://pdfs.semanticscholar.org/a559/70013b984f344dfbbbba677d89dce0ba5f81.pdf)

5.  [S. Ioffe and C. Szegedy, "Batch Normalization: Accelerating Deep
Network Training by Reducing Internal Covariate Shift," 32nd ICML 
$\small{(2015)}$.](https://arxiv.org/abs/1502.03167)

6.  [A.L. Maas, A.Y. Hannun and A.Y. Ng, "Rectifier Nonlinearities Improve
Neural Network Acoustic Models", 50th ICML $\small{(2013)}$.](https://web.stanford.edu/~awni/papers/relu_hybrid_icml2013_final.pdf)

7.  [G. Huang, Z. Liu, K.Q. Weinberger, L. van der Maaten, "Densely
Connected Convolutional Networks," CVPR $\small{(2017)}$.](https://arxiv.org/abs/https://arxiv.org/abs/1608.06993)

8.  [V. Dumoulin and F. Visin, "A Guide to Convolution Arithmetic for Deep
Learning," arXiv:1603.07285 v1 $\small{(2016)}$.](https://ai2-s2-pdfs.s3.amazonaws.com/7918/2aab186f0b68a8432540d8695e1646338479.pdf)
