---
layout: post
title:  "Deep Reinforcement Learning of the Hangman Word Game"
date:   2017-02-25 12:00:00
categories: Deep Learning
use_math: true
---

## English word data

![Hangman]({{site.url}}/blog/images/hangman/word_chars_dist.jpg)
*Fig. *.  Distribution for numbers of letters in English words.  Words
containing fewer than four or more than twenty letters are not included in
this histogram.*

![Hangman]({{site.url}}/blog/images/hangman/monogram_freqs.jpg)
*Fig. *.   Relative frequency of individual letters appearing in English words.*

## Reinforcement learning via policy gradients

<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: left">Hyperparameter</th>
      <th style="text-align: center">Value    </th>
      <th style="text-align: center">Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Batch size</td>
      <td style="text-align: center">1E4</td>
      <td style="padding: 0 35px">  Number of training inputs per
gradient descent computation</td>
    </tr>
    <tr>
      <td style="text-align: left">Learning rate</td>
      <td style="text-align: center">3E-4</td>
      <td style="padding: 0 35px"> Learning rate for RMS propagation</td>
    </tr>
    <tr>
      <td style="text-align: left">RMSProp decay rate</td>
      <td style="text-align: center">0.9</td>
      <td style="padding: 0 35px">Decay rate for RMS propagation weights cache</td>
    </tr>
    <tr>
      <td style="text-align: left">RMSProp denom const</td>
      <td style="text-align: center">1E-5</td>
      <td style="padding: 0 35px">Constant preventing zero RMS propagation denominator</td>
    </tr>
    <tr>
      <td style="text-align: left">Leaky ReLU slope</td>
      <td style="text-align: center">1E-2</td>
      <td style="padding: 0 35px">Slope for negative domain of Leaky
ReLU nonlinearities</td>
    </tr>
    <tr>
      <td style="text-align: left"> $\lambda$ </td>
      <td style="text-align: center">1E-4</td>
      <td style="padding: 0 35px">L2 regularization coefficient</td>
    </tr>
    <tr>
      <td style="text-align: left"> $\gamma$ </td>
      <td style="text-align: center">0.9</td>
      <td style="padding: 0 35px">RL discount factor</td>
    </tr>

  </tbody>
</table>

*Table 1.  List of hyperparameter values used in the Hangman RL system.*


![Hangman]({{site.url}}/blog/images/hangman/avg_eventual_reward.jpg)
*Fig. *.  Plots of raw [red] and temporally smoothed [blue] RL agent
average discounted eventual rewards per episode plotted as functions of
training epoch.

![Hangman]({{site.url}}/blog/images/hangman/learning_curves.jpg)
*Fig. *.  Learning curves for training [red] and validation [blue] data sets.*


## Hangman game play

![Hangman]({{site.url}}/blog/images/hangman/RL_agent_vs_monogram_win_dist.jpg)
*Fig. *.  Fraction of hangman games won plotted as a function of number of
letters in words.  The red line in the left plot indicates a mean winning
fraction of 0.401 for an agent following the RL policy.  The red line in
the right plots indicates a mean winning fraction of 0.197 for an agent
following the single letter distribution in figure XX.*


## References

1.  [J. Schulman, "Deep Reinforcement Learning, Lecture 1: Introduction" UC
Berkeley course CS 294, $\small{(2015)}$](http://rll.berkeley.edu/deeprlcourse-fa15/docs/2015.08.26.Lecture01Intro.pdf)

9. [G. Hinton, "Neural Networks for Machine Learning", Slide 29 of Lecture
6 Coursera notes $\small{(2012)}$.](https://www.coursera.org/learn/neural-networks)
