---
layout: post
title:  "Deep Reinforcement Learning of the Hangman Word Game"
date:   2017-02-25 12:00:00
categories: Deep Learning
use_math: true
---

## Game words and rules

We start by downloading a text file containing 355K English words from the
github site in ref. [1].  We choose to ignore any words with fewer than 5
or more than 18 letters.  Words containing non-letter characters are also
discarded.  The resulting trimmed corpus is then split into training,
validation and testing data sets which respectively contain 312K, 10K and
26K words.

Many of the data sets' words such as "ladders", "raven" and "tacking" are
relatively simple.  But others like those listed in Table 1 are unfamiliar
$($at least to me$)$, and some originate from foreign languages that have
presumably been absorbed into English:

<table style="width:100%">
  <thead>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><b>apteryxes</b></td>
      <td style="text-align: center"><b>bijwoner</b></td>
      <td style="text-align: center"><b>chalcotrichite</b></td>
      <td style="text-align: center"><b>izvozchik</b></td>
    </tr>
    <tr>
      <td style="text-align: center"><b>komatiks</b></td>
      <td style="text-align: center"><b>muzjiks</b></td>
      <td style="text-align: center"><b>ootwith</b></td>
      <td style="text-align: center"><b>pirojki</b></td>
    </tr>
    <tr>
      <td style="text-align: center"><b>rajbansi</b></td>
      <td style="text-align: center"><b>sovkhoz</b></td>
      <td style="text-align: center"><b>sulphonamide</b></td>
      <td style="text-align: center"><b>sybow</b></td>
    </tr>
  </tbody>
</table>

*Table 1.  Examples of challenging entries within our game word corpus.*

The presence of foreign language words introduces interesting
counterexamples to conventional rules of English.  For example, the letter
"q" is always followed by "u" in English words.  But our data sets include
"qaimaqam, "seqed", "iraqis" and "qophs" which are transliterations of
arabic and hebrew terms.  Our game word vocabulary is thus challenging.

The basic rules of the hangman word game are simple.  A word is chosen at
random, and its *a priori* unknown letters are represented by dashes.  The
player guesses one letter at a time up to some maximum number of turns.  If
the word does not contain the letter, it is added to a rejected list and
the number of remaining turns is decremented.  But if the letter occurs in
the word, its location is revealed among the dashes, and the player is
allowed to choose another letter without losing a turn.  The player wins if
he correctly guesses the word before exhausting all his turns.

The one free parameter in this version of the hangman game is the maximum
number of allowed turns $t_{max}$.  If it is too small $($big$)$, the
player will always lose $($win$)$.  In order to set this parameter to a
reasonable value, it is useful to construct the letter-count distribution
for words in our corpus.  As figure 1 illustrates, the distribution peaks
at 9 letters per word.  We consequently set $t_{max} = 9$.

![Hangman]({{site.url}}/blog/images/hangman/word_chars_dist.jpg)
*Fig. 1.  Distribution for numbers of letters within game vocabulary words.*

It is also instructive to compute the frequency distribution for individual
letters within our word corpus.  According to figure 2, the five most
common letters are "e", "i", "a", "n" and "s".  One strategy for playing
hangman is to guess letters following the single-letter distribution.  This
simple approach consequently provides a baseline for comparison against
learned game playing behaviors.

![Hangman]({{site.url}}/blog/images/hangman/monogram_freqs.jpg)
*Fig. 2.   Relative frequency of individual letters appearing within 
game words.*

## Reinforcement learning via policy gradients

<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: left">Hyperparameter</th>
      <th style="text-align: center">Value    </th>
      <th style="text-align: center">Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Batch size</td>
      <td style="text-align: center">1E4</td>
      <td style="padding: 0 35px">  Number of training inputs per
gradient descent computation</td>
    </tr>
    <tr>
      <td style="text-align: left">Learning rate</td>
      <td style="text-align: center">3E-4</td>
      <td style="padding: 0 35px"> Learning rate for RMS propagation</td>
    </tr>
    <tr>
      <td style="text-align: left">RMSProp decay rate</td>
      <td style="text-align: center">0.9</td>
      <td style="padding: 0 35px">Decay rate for RMS propagation weights cache</td>
    </tr>
    <tr>
      <td style="text-align: left">RMSProp denom const</td>
      <td style="text-align: center">1E-5</td>
      <td style="padding: 0 35px">Constant preventing zero RMS propagation denominator</td>
    </tr>
    <tr>
      <td style="text-align: left">Leaky ReLU slope</td>
      <td style="text-align: center">1E-2</td>
      <td style="padding: 0 35px">Slope for negative domain of Leaky
ReLU nonlinearities</td>
    </tr>
    <tr>
      <td style="text-align: left"> $\lambda$ </td>
      <td style="text-align: center">1E-4</td>
      <td style="padding: 0 35px">L2 regularization coefficient</td>
    </tr>
    <tr>
      <td style="text-align: left"> $\gamma$ </td>
      <td style="text-align: center">0.9</td>
      <td style="padding: 0 35px">RL discount factor</td>
    </tr>

  </tbody>
</table>

*Table 2.  List of hyperparameter values used in the Hangman RL system.*


![Hangman]({{site.url}}/blog/images/hangman/avg_eventual_reward.jpg)
*Fig. *.  Plots of raw [red] and temporally smoothed [blue] RL agent
average discounted eventual rewards per episode plotted as functions of
training epoch.

![Hangman]({{site.url}}/blog/images/hangman/learning_curves.jpg)
*Fig. *.  Learning curves for training [red] and validation [blue] data sets.*


## Hangman game play

![Hangman]({{site.url}}/blog/images/hangman/RL_agent_vs_monogram_win_dist.jpg)
*Fig. *.  Fraction of hangman games won plotted as a function of number of
letters in words.  The red line in the left histogram indicates a mean
winning fraction of 0.401 for an agent following the RL policy.  The red
line on the right indicates a mean winning fraction of 0.197 for
an agent following the single letter distribution in figure 2.*


## References

1.  [355K English words available at https://github.com/felixdae/english-words.](https://github.com/felixdae/english-words)

2.  [J. Schulman, "Deep Reinforcement Learning, Lecture 1: Introduction" UC
Berkeley course CS 294, $\small{(2015)}$](http://rll.berkeley.edu/deeprlcourse-fa15/docs/2015.08.26.Lecture01Intro.pdf)

9. [G. Hinton, "Neural Networks for Machine Learning", Slide 29 of Lecture
6 Coursera notes $\small{(2012)}$.](https://www.coursera.org/learn/neural-networks)

*.  [P. Bachman, A. Sordoni and A. Trischler, "Towards Information-Seeking
Agents", arXiv:1612.02705v1 $\small{(2016)}$.](https://arxiv.org/abs/1612.02605)
