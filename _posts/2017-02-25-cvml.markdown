---
layout: post
title:  "Deep Reinforcement Learning of the Hangman Word Game"
date:   2017-02-25 12:00:00
categories: Deep Learning
use_math: true
---

## Game words and rules

In this blog post, we continue our study of Reinforcement Learning $($RL$)$
techniques using games as instructive and fun laboratories.  Here we focus
upon the classic word game "hangman" which involves guessing the letter
content of *a priori* unknown words.  Unlike the maze and Atari Pong
environments in our previous RL investigations, the hangman world state is
only partially observable.  So this word game introduces some new
complications which differ from those of the image-based games in our past
two postings.

We start by downloading a text file containing 355K English words from the
github site in ref. [1].  We choose to ignore any words in the file with
fewer than 5 or more than 18 letters.  Words containing non-letter
characters are also discarded.  The resulting trimmed corpus is then split
into training, validation and testing data sets which respectively contain
312K, 10K and 26K words.

Many of the data sets' words such as "ladders", "raven" and "tacking" are
relatively simple.  But others like the examples listed in Table 1 are
uncommon, and some words originate from foreign languages that have
presumably been absorbed into English:

<table style="width:100%">
  <thead>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><b>apteryxes</b></td>
      <td style="text-align: center"><b>bijwoner</b></td>
      <td style="text-align: center"><b>chalcotrichite</b></td>
      <td style="text-align: center"><b>izvozchik</b></td>
    </tr>
    <tr>
      <td style="text-align: center"><b>komatiks</b></td>
      <td style="text-align: center"><b>muzjiks</b></td>
      <td style="text-align: center"><b>ootwith</b></td>
      <td style="text-align: center"><b>pirojki</b></td>
    </tr>
    <tr>
      <td style="text-align: center"><b>rajbansi</b></td>
      <td style="text-align: center"><b>sovkhoz</b></td>
      <td style="text-align: center"><b>sulphonamide</b></td>
      <td style="text-align: center"><b>sybow</b></td>
    </tr>
  </tbody>
</table>

*Table 1.  Examples of difficult game vocabulary words.*

The presence of foreign language words introduces interesting
counterexamples to conventional rules of English.  For instance, the letter
'q' is always followed by 'u' in English words.  But our data set includes
"qaimaqam, "seqed", "iraqis" and "qophs" which are transliterations of
Arabic and Hebrew terms.  The game word vocabulary is thus challenging.

The basic rules of the hangman game are simple.  A word is chosen at
random, and its unknown letters are represented by dashes.  The player
guesses one letter at a time up to some maximum number of turns.  The
letter is added to a rejected list if the word does not contain it, and the
number of remaining turns is decremented.  But if the letter occurs in the
word, its location is revealed among the dashes, and the player is allowed
to choose another letter without losing a turn.  The player wins if he
correctly guesses the word before exhausting all his turns.

The one free parameter in this version of the hangman game is the maximum
number of allowed turns $t_{max}$.  If it is too small $($big$)$, the
player will always lose $($win$)$.  In order to set this parameter to a
reasonable value, it is helpful to construct the letter-count distribution
for corpus words.  As figure 1 illustrates, the distribution peaks at 9
letters per word.  We are thus motivated to set $t_{max} = 9$.

![Hangman]({{site.url}}/blog/images/hangman/word_chars_dist.jpg)
*Fig. 1.  Distribution for numbers of letters in game vocabulary words.*

It is also instructive to compute the frequency distribution for individual
letters within our word vocabulary.  According to figure 2, the ten most
common letters listed in descending order are 'e', 'i', 'a', 'n', 's', 'o',
'r', 't', 'l' and 'c'.  One strategy for playing hangman is to guess
letters following the single-letter distribution.  This simple approach
provides a useful baseline for comparison against learned game playing
behaviors.

![Hangman]({{site.url}}/blog/images/hangman/monogram_freqs.jpg) *Fig. 2.
Relative frequency of individual letters appearing in game vocabulary
words.*

## Game reinforcement learning

In order to teach a machine to play hangman via reinforcement learning, we
first need to embed its vocabulary words into some vector space.  We could
use "one-hot" encoding to convert letters 'a' through 'z' into a
26-dimensional basis.  All letters would then be evenly separated within a
sparse vector space.  But we prefer to instead employ a more compact
representation which captures English language structure.

We therefore work with the language model of Jozefowicz et al [2] which was
trained on the One Billion Word Benchmark [3].  These authors' python code
[4] generates 16-dimensional descriptors for each character corresponding
to ascii values 0 through 255 [5].  During a hangman game, a word with some
known and unknown characters is represented by concatenating lower-case
letter and underscore descriptors.  Since the maximum letter count in our
word corpus is 18, the word vector's dimension is 288 = 18$\times$16.
Words with fewer than 18 letters have space character descriptors
$($corresponding to ascii value 32$)$ filling their empty terminal slots.

As a hangman game proceeds, we also need to keep track of incorrectly
guessed letters in the world state.  We therefore append 16-dimensional
descriptors for each wrong letter onto the word vector.  As our game rules
permit a maximum of 9 wrong letters, the state vector's total length is 432
= $($18+9$)$$\times$16.  Initially, state vector slots 289 through 432 are
filled with 9 copies of the 16-dimensional ascii space descriptor.  But the
copies are replaced by letter descriptors as wrong guesses are made.

The 432-dimensional state vector becomes input to a policy neural network.
After experimenting with several different fully-connected network
architectures, we chose the configuration with two hidden layers
schematically pictured in figure 3.  The softmax action layer contains 26
nodes representing letters 'a' through 'z'.  The final action sampling node
resets the probabilities of any already-chosen letters to zero, and it
renormalizes the remaining unchosen letters' probabilities so that they sum
to one.  Selected output may then either be randomly sampled from the
renormalized distributions or taken as the most likely letter.

![Hangman]({{site.url}}/blog/images//hangman/hangman_network.jpg)
*Fig. 3.   Schematic diagram of policy network which maps input world
states onto output letter guesses.*

The neural network's 714 biases are initialized to 0, while its 75008
weights are initialized to $\sqrt{2 / n_{in}}$ per layer.  We choose to
work with uniform rather than random initial weight values to prevent large
fluctuations from swamping English letter structure.  Leaky ReLU rather
than conventional ReLU nonlinearities are employed in each hidden layer
node to avoid weight death during training [6].  50% dropout on the hidden
layers is performed during training to combat network overfitting [7].
L2-regularization is further employed to discourage weight values from
diverging.

At the start of each training episode, a word is randomly selected from the
training data set.  The input state vector is forward propagated through
the policy network, and the RL agent stochastically picks the next letter
to guess.  The episode terminates if all letters in the word are correctly
found or if the agent runs out of turns.  After a game finishes, the
environment emits the reward signal

$$ r = 1 - 2 \times n_{\rm unfound \> letters} / n_{\rm letters \> in \> word}.
$$

This reward ranges over the interval $[-1, 1]$, and it linearly decreases
with the fraction of unfound word letters.  Terminal rewards along with
state and action information are stored in tuples for 10K episodes batches.
Network biases and weights are then updated via gradient descent performed
with RMS propagation [8].

For pedagogical purposes, we implement the hangman game RL system from
scratch in C++ rather than employ an existing deep learning software
package.  As it is fairly simple, RL training finishes on a modern CPU in a
few hours.  We consequently ran several hundred experiments to search for
reasonable system hyperparameters.  Table 2 lists our set of
empirically-derived hyperparameter values.

<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: left">Hyperparameter</th>
      <th style="text-align: center">Value    </th>
      <th style="text-align: center">Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Batch size</td>
      <td style="text-align: center">1E4</td>
      <td style="padding: 0 35px">  Number of training inputs per
gradient descent computation</td>
    </tr>
    <tr>
      <td style="text-align: left">Learning rate</td>
      <td style="text-align: center">3E-4</td>
      <td style="padding: 0 35px"> Learning rate for RMS propagation</td>
    </tr>
    <tr>
      <td style="text-align: left">RMSProp decay rate</td>
      <td style="text-align: center">0.9</td>
      <td style="padding: 0 35px">Decay rate for RMS propagation weights cache</td>
    </tr>
    <tr>
      <td style="text-align: left">RMSProp denom const</td>
      <td style="text-align: center">1E-5</td>
      <td style="padding: 0 35px">Constant preventing zero RMS propagation denominator</td>
    </tr>
    <tr>
      <td style="text-align: left">Leaky ReLU slope</td>
      <td style="text-align: center">1E-2</td>
      <td style="padding: 0 35px">Slope for negative domain of Leaky
ReLU nonlinearities</td>
    </tr>
    <tr>
      <td style="text-align: left"> $\lambda$ </td>
      <td style="text-align: center">1E-4</td>
      <td style="padding: 0 35px">L2 regularization coefficient</td>
    </tr>
    <tr>
      <td style="text-align: left"> $\gamma$ </td>
      <td style="text-align: center">0.9</td>
      <td style="padding: 0 35px">RL discount factor</td>
    </tr>

  </tbody>
</table>

*Table 2.  Hyperparameter values used in the hangman game RL system.*

We experimented with different approaches to training the policy network.
These included training from scratch for entire games, training from
scratch for partial games, and iteratively training from previous
partial-game networks.  We also explored various metrics to monitor
training progress: loss function, fraction of word letters correctly
guessed, expected future discounted reward and fraction of games won.  In
the end, we found training the policy network from scratch for a maximum of
11 turns yielded the best results as measured by expected discounted reward
and fractions of games won.  

As figure 4 illustrates, expected discounted reward peaks around training
epoch 0.7 and then declines with further training.  The game winning
fraction values displayed in figure 5 rise and fall even more rapidly.  The
figure's training and validation curves also exhibit no signs of network
overfitting.  So we need to perform early stopping on policy network
training in order to maximize RL agent game playing performance.

![Hangman]({{site.url}}/blog/images/hangman/avg_eventual_reward.jpg)
*Fig. 4.  Raw [red] and temporally smoothed [blue] expected discounted
eventual reward per episode plotted as functions of training epoch.  These
results are based upon limiting the maximum number of game turns
to 11 during training.*


![Hangman]({{site.url}}/blog/images/hangman/learning_curves.jpg)
*Fig. 5. Fractions of games won by agent stochastically following learned
policy for training [red] and validation [blue] data sets.  These results
are based upon limiting the maximum number of game turns to 11 during
training.*

In addition to experimenting with network training and progress monitoring,
we also investigated different options for exploiting the learned policy
$\pi$.  The results in figure 5 are based upon stochastically sampling
$\pi$ to yield the RL agent's next guessed letter.  From the figure, we see
the validation win fraction peaks around 24% for games with a maximum of 11
turns.  But we empirically found that the validation win fraction increases
to 40% if we instead always select the most probable letter at each turn in
complete games.  So we adopt this quasi-deterministic letter selection rule
for the trained RL agent.

## Game play and performance

Once the RL agent finished learning, we evaluated its performance playing
full hangman games on the held-out set of 26K testing words.  It is
interesting to first examine the agent's decision making as a function of
game turn.  Figure 6 presents letter selection distributions for testing
game turns 1, 4, 7, 10, 13 and 16:

![Hangman]({{site.url}}/blog/images/hangman/montage_cropped_p_actions.jpg)
*Fig. 6.  RL agent letter selection distributions for different turns during 
hangman word games.*

The distributions for the first ten turns in each hangman game are
essentially delta functions.  In particular, the RL agent nearly always
guesses following the sequence 'e', 'o', 'n', 's', 'r', 'g', 'a', 'l', 't'
and 'i'.  This ordering of the top ten letters differs significantly from
that derived from the relative frequency of individual letters appearing in
figure 2.  After the first ten turns, the agent's letter selection
distributions start to exhibit multi-modal behavior.

The RL agent's pattern of letter choices can be seen in the YouTube video
in figure 7 as it plays five different episodes.  In the movie, we see the
unknown word initially represented by a set of underscore characters.  As
the game proceeds, correctly guessed letters colored green appear inside
the word.  Green letters are also added to the list of previously selected
characters along with incorrect guesses colored red.  Near the bottom of
the game window, the top five probabilities for the next letter coming from
policy $\pi$ are displayed.  The letter with the highest probability is
colored purple and represents the agent's next guess.

<a href="http://www.youtube.com/watch?feature=player_embedded&v=qCyQK7YPp1M"
target="_blank"><img src="http://img.youtube.com/vi/qCyQK7YPp1M/0.jpg"
alt="IMAGE ALT TEXT HERE" width="700" height="466" border="10" /></a>
*Fig. 7.  Movie of the RL agent playing 5 episodes of the hangman word
game.  Click on image above to start the YouTube video.*

By the end of the movie, the RL agent successfully guesses the letter
contents of three out of five random test words.  When the agent's
performance is measured on 26K testing words, its average winning game
fraction equals 0.401.  Words with more letters are generally easier for
the agent to solve, for they usually lead to more chances to guess.  The
agent's performance as a function of test word letter count is presented in
the histogram on the left side of figure 8.

![Hangman]({{site.url}}/blog/images/hangman/RL_agent_vs_monogram_win_dist.jpg)
*Fig. 8.  Fraction of hangman games won as a function of number of letters
in testing dataset words.  The red line on the left indicates a 0.401 mean
winning fraction for an agent following the RL policy.  The red line on the
right indicates a 0.197 mean winning fraction if the agent follows the
single letter distribution in figure 2.*

For comparison, the histogram on the right side of figure 8 illustrates the
fraction of games won if the agent simply follows the single letter
distribution in figure 2.  For all word sizes, the learned policy is
superior to the baseline strategy.  And the mean performance of the former
is two times better than that of the latter.  So the RL agent has clearly
learned something nontrivial about how to play hangman.

Nevertheless, there is much room for reinforcement learning improvement.
In particular, we would like to see more intelligent dependence of letter
selection upon input state.  Recent work by Bachman et al outlines a more
sophisticated approach to solving guessing games than the relatively simple
one we have presented here [9].  Among the applications considered by these
authors is the hangman word game.  Unfortunately, it is difficult to
quantitatively compare their system's performance with ours given their
qualitatively different word corpus and game parameters.

We leave exploration of more advanced deep RL techniques to word games like
hangman for future work.

## References

1.  [355K English words are available from https://github.com/felixdae/english-words.](https://github.com/felixdae/english-words)

2.  [R. Jozefowicz, O. Vinyals, M. Schuster, N. Shazeer and Y. Wu,
"Exploring the Limits of Language Modeling", arXiv:1602.02410v2 $\small{(2016)}$.](https://arxiv.org/abs/1602.02410)

3. [O. Vinyals and X. Pan, "Language Model on One Billion Word Benchmark" $\small{(2016)}$.](https://github.com/tensorflow/models/tree/master/lm_1b)

4.  [C. Chelba, T. Mikolov, M. Schuster, Q. Ge, T. Brants, P. Koehn and
T. Robinson, "One Billion Word Benchmark for Measuring Progress in
Statistical Language Modeling", arXiv:1312.3005v3
$\small{(2014)}$.](https://arxiv.org/pdf/1312.3005.pdf)

5.  [C. Morris, "Dissecting Google's Billion Word Language Model Part 1:
Character Embeddings", blog posting $\small{(2016)}$.](http://colinmorris.github.io/blog/1b-words-char-embeddings)

6. [A.L. Maas, A. Y. Hannun, A.Y. Ng, "Rectifier Nonlinearities Improve
Neural Network Acoustic Models", Proc. 30th ICML $\small{(2013)}$.](https://web.stanford.edu/~awni/papers/relu_hybrid_icml2013_final.pdf)

7.  [N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever and
R. Salakhutdinov, "Dropout: A Simple Way to Prevent Neural Networks from
Overfitting", J. Machine Learning Research $\small{(2014)}$
1929.](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)

8. [G. Hinton, "Neural Networks for Machine Learning", Slide 29 of Lecture
6 Coursera notes $\small{(2012)}$.](https://www.coursera.org/learn/neural-networks)

9.  [P. Bachman, A. Sordoni and A. Trischler, "Towards Information-Seeking
Agents", arXiv:1612.02705v1 $\small{(2016)}$.](https://arxiv.org/abs/1612.02605)

