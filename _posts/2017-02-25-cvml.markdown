---
layout: post
title:  "Deep Reinforcement Learning of the Hangman Word Game"
date:   2017-02-25 12:00:00
categories: Deep Learning
use_math: true
---

## Game words and rules

We start by downloading a text file containing 355K English words from the
github site in ref. [1].  We choose to ignore any words with fewer than 5
or more than 18 letters.  Words containing non-letter characters are also
discarded.  The resulting trimmed corpus is then split into training,
validation and testing data sets which respectively contain 312K, 10K and
26K words.

Many of the data sets' words such as "ladders", "raven" and "tacking" are
relatively simple.  But others like those listed in Table 1 are unfamiliar
$($at least to me$)$, and some originate from foreign languages that have
presumably been absorbed into English:

<table style="width:100%">
  <thead>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><b>apteryxes</b></td>
      <td style="text-align: center"><b>bijwoner</b></td>
      <td style="text-align: center"><b>chalcotrichite</b></td>
      <td style="text-align: center"><b>izvozchik</b></td>
    </tr>
    <tr>
      <td style="text-align: center"><b>komatiks</b></td>
      <td style="text-align: center"><b>muzjiks</b></td>
      <td style="text-align: center"><b>ootwith</b></td>
      <td style="text-align: center"><b>pirojki</b></td>
    </tr>
    <tr>
      <td style="text-align: center"><b>rajbansi</b></td>
      <td style="text-align: center"><b>sovkhoz</b></td>
      <td style="text-align: center"><b>sulphonamide</b></td>
      <td style="text-align: center"><b>sybow</b></td>
    </tr>
  </tbody>
</table>

*Table 1.  Examples of challenging game vocabulary words.*

The presence of foreign language words introduces interesting
counterexamples to conventional rules of English.  For example, the letter
"q" is always followed by "u" in English words.  But our data sets include
"qaimaqam, "seqed", "iraqis" and "qophs" which are transliterations of
arabic and hebrew terms.  Our game word vocabulary is thus challenging.

The basic rules of the hangman word game are simple.  A word is chosen at
random, and its *a priori* unknown letters are represented by dashes.  The
player guesses one letter at a time up to some maximum number of turns.  If
the word does not contain the letter, it is added to a rejected list and
the number of remaining turns is decremented.  But if the letter occurs in
the word, its location is revealed among the dashes, and the player is
allowed to choose another letter without losing a turn.  The player wins if
he correctly guesses the word before exhausting all his turns.

The one free parameter in this version of the hangman game is the maximum
number of allowed turns $t_{max}$.  If it is too small $($big$)$, the
player will always lose $($win$)$.  In order to set this parameter to a
reasonable value, it is useful to construct the letter-count distribution
for words in our corpus.  As figure 1 illustrates, the distribution peaks
at 9 letters per word.  We are thus motivated to set $t_{max} = 9$.

![Hangman]({{site.url}}/blog/images/hangman/word_chars_dist.jpg)
*Fig. 1.  Distribution for numbers of letters within game vocabulary words.*

It is also instructive to compute the frequency distribution for individual
letters within our word corpus.  According to figure 2, the five most
common letters are "e", "i", "a", "n" and "s".  One strategy for playing
hangman is to guess letters following the single-letter distribution.  This
simple approach consequently provides a baseline for comparison against
learned game playing behaviors.

![Hangman]({{site.url}}/blog/images/hangman/monogram_freqs.jpg)
*Fig. 2.   Relative frequency of individual letters appearing within 
game words.*

## Game reinforcement learning

In order to teach a machine to play hangman via reinforcement learning, we
first need to embed every vocabulary word into some vector space.  We could
use "one-hot" sparse encoding to convert letters "a" - "z" into a
26-dimensional basis.  But we prefer to instead employ a more compact
representation which captures English language structure.  We therefore
work with the language model of Jozefowicz et al [ref] which was trained on
the One Billion Word Benchmark [ref].  In particular, we use these authors'
python code to generate 16-dimensional descriptors for each character
corresponding to ascii values 0 through 255 [Colin post].

During a hangman game, a word with some known and unknown characters is
represented by concatenating lower-case letter and underscore descriptors.
Since the maximum letter count in our word corpus is 18, the word vector's
dimension is $18 \times 16 = 288$.  Words with fewer than 18 letters have
space character $($corresponding to ascii value 32$)$ descriptors filling
their empty slots.

As the game proceeds, we need to keep track of incorrectly guessed letters.
We consequently append 16-dimensional descriptors for each wrong letter
after the word vector representation.  As our game rules permit a maximum
of 9 wrong letters, the state vector's total length is $(18+9) \times 16 =
432$.  Initially, state vector slots 289 through 432 are filled with 9
copies of the ascii space descriptor.  But the copies are replaced by
genuine letter descriptors as wrong guesses are made.

The 432-dimensional state vector becomes input to a policy neural network.
After experimenting with several different fully-connected network
architectures, we chose the configuration with two hidden layers pictured
in figure XX.  The softmax action layer contains 26 nodes corresponding to
letters 'a' through 'z'.  The final action sampling node resets the
probabilities of any already-chosen letters to zero, and it renormalizes
the remaining unchosen letters' probabilities so that they sum to one.

![Hangman]({{site.url}}/blog/images//hangman/hangman_network.jpg)
*Fig. 3.   Schematic diagram of policy network which maps input word
states into output letter guesses.*

The neural network's 714 biases are initialized to 0, while its 75008
weights are initialized to $\sqrt{2 / n_{in}}$ per layer.  We choose to
work with uniform rather than randomly sampled initial weight values to
avoid English language structure among letters from being swamped by large
fluctuations.  Leaky ReLU rather than conventional ReLU nonlinearities are
employed in each hidden layer node to prevent weight death during training
[ref].  50% dropout on the hidden layers is also performed during training
to combat network overfitting [ref].  L2-regularization is further employed
to discourage weight values from diverging.

At the start of each episode, a word is randomly selected from the training
data set.  The input state vector is forward propagated through the policy
network, and the RL agent stochastically picks its next letter to guess.
The episode terminates if all letters in the word are correctly found or if
the agent runs out of turns.  After a game finishes, the environment emits
the reward signal 

$$ r = 1 - 2 \times n_{\rm unfound \> letters} / n_{\rm letters \> in \> word}.
$$

This reward ranges over the interval $[-1, 1]$, and it linearly decreases
with the fraction of unfound word letters.  Terminal rewards along with
state and action information are stored in tuples for 10K episodes batches.
Network biases and weights are then updated via gradient descent performed
with RMS propagation [ref].

For pedagogical purposes, we implement the hangman game RL system from
scratch in C++ rather than employ an existing deep learning software
package.  As it is fairly simple, RL training finishes on a modern CPU in a
few hours.  We consequently ran several hundred experiments to search for
reasonable system hyperparameter values.  Table 2 lists our final set of
empirically-derived hyperparameters.

<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: left">Hyperparameter</th>
      <th style="text-align: center">Value    </th>
      <th style="text-align: center">Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Batch size</td>
      <td style="text-align: center">1E4</td>
      <td style="padding: 0 35px">  Number of training inputs per
gradient descent computation</td>
    </tr>
    <tr>
      <td style="text-align: left">Learning rate</td>
      <td style="text-align: center">3E-4</td>
      <td style="padding: 0 35px"> Learning rate for RMS propagation</td>
    </tr>
    <tr>
      <td style="text-align: left">RMSProp decay rate</td>
      <td style="text-align: center">0.9</td>
      <td style="padding: 0 35px">Decay rate for RMS propagation weights cache</td>
    </tr>
    <tr>
      <td style="text-align: left">RMSProp denom const</td>
      <td style="text-align: center">1E-5</td>
      <td style="padding: 0 35px">Constant preventing zero RMS propagation denominator</td>
    </tr>
    <tr>
      <td style="text-align: left">Leaky ReLU slope</td>
      <td style="text-align: center">1E-2</td>
      <td style="padding: 0 35px">Slope for negative domain of Leaky
ReLU nonlinearities</td>
    </tr>
    <tr>
      <td style="text-align: left"> $\lambda$ </td>
      <td style="text-align: center">1E-4</td>
      <td style="padding: 0 35px">L2 regularization coefficient</td>
    </tr>
    <tr>
      <td style="text-align: left"> $\gamma$ </td>
      <td style="text-align: center">0.9</td>
      <td style="padding: 0 35px">RL discount factor</td>
    </tr>

  </tbody>
</table>

*Table 2.  Hyperparameter values used in the Hangman RL system.*

We experimented with different approaches to training the policy network.
These included training from scratch for entire games, training from
scratch for partial games, and training from previous partial-game
networks.  We also explored several metrics to monitor learning progress
during training.  They included the loss function, the fraction of word
letters correctly guessed, the fraction of games won, and the expected
discounted reward

$$ \eta(\pi) = E_{s_0, a_0, \cdots} \Bigl[ \sum_{t = 0}^{\infty} \gamma^t
r(s_t) \Bigr] $$

where 

$$a_t  \, \tilde \, \pi(a_t | s_t)  \quad {\rm and} \quad 
s_{t+1} \, \tilde \, P(s_{t+1} | s_t, a_t ). $$

In the end, we found training the policy network from scratch for a maximum
of 11 turns yielded the best results as measured by fractions of games won
and expected discounted reward.  As figure XX illustrates, the game winning
fraction rapidly rises and then declines as training continues.  The
training and validation curves in this figure also exhibit no signs of
network overfitting.  Expected discounted reward values displayed in figure
XX similarly peak before one training epoch is completed and then fall with
further training.  So we perform early training stopping in order to
maximize the policy network's game playing performance.

![Hangman]({{site.url}}/blog/images/hangman/learning_curves.jpg) *Fig. *.
Fractions of games won for training [red] and validation [blue] data sets
plotted as functions of training epoch.  These results are based upon
limiting the maxinum number of game turns to 11 during training.*

![Hangman]({{site.url}}/blog/images/hangman/avg_eventual_reward.jpg)
*Fig. *.  Raw [red] and temporally smoothed [blue] expected discounted
eventual reward per episode plotted as functions of training epoch.  These
results are based upon limiting the maximum number of game turns
to 11 during training.*

## Game play and results


![Hangman]({{site.url}}/blog/images/hangman/montage_cropped_pi_probs.jpg)
*Fig. *.  Learned letter policy distributions at different turns during a 
hangman word game.  These plots correspond to training epoch 0.492. *


<a href="http://www.youtube.com/watch?feature=player_embedded&v=qCyQK7YPp1M"
target="_blank"><img src="http://img.youtube.com/vi/qCyQK7YPp1M/0.jpg"
alt="IMAGE ALT TEXT HERE" width="700" height="466" border="10" /></a>
*Fig. *.  Movie of deep RL agent playing 5 episodes of the hangman word
game.  Click on image above to start the YouTube video.*





![Hangman]({{site.url}}/blog/images/hangman/RL_agent_vs_monogram_win_dist.jpg)
*Fig. *.  Fraction of hangman games won plotted as a function of number of
letters in test dataset words.  The red line in the left histogram
indicates a mean winning fraction of 0.401 for an agent following the RL
policy.  The red line on the right indicates a 0.197 mean winning fraction
if the agent follows the single letter distribution in figure 2.*


## References

1.  [355K English words available at https://github.com/felixdae/english-words.](https://github.com/felixdae/english-words)

*.  [R. Jozefowicz, O. Vinyals, M. Schuster, N. Shazeer and Y. Wu,
"Exploring the Limits of Language Modeling", arXiv:1602.02410v2 $\small{(2016)}$.](https://arxiv.org/abs/1602.02410)

*. [O. Vinyals and X. Pan, "Language Model on One Billion Word Benchmark" $\small{(2016)}$.](https://github.com/tensorflow/models/tree/master/lm_1b)

*.  [C. Chelba, T. Mikolov, M. Schuster, Q. Ge, T. Brants, P. Koehn and
T. Robinson, "One Billion Word Benchmark for Measuring Progress in
Statistical Language Modeling", arXiv:1312.3005v3
$\small{(2014)}$.](https://arxiv.org/pdf/1312.3005.pdf)

*.  [C. Moris, "Dissecting Google's Billion Word Language Model Part 1:
Character Embeddings", blog posting $\small{(2016)}$.](http://colinmorris.github.io/blog/1b-words-char-embeddings)

*. [A.L. Maas, A. Y. Hannun, A.Y. Ng, "Rectifier Nonlinearities Improve
Neural Network Acoustic Models", Proc. 30th ICML $\small{(2013)}$.](https://web.stanford.edu/~awni/papers/relu_hybrid_icml2013_final.pdf)

*.  [N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever and
R. Salakhutdinov, "Dropout: A Simple Way to Prevent Neural Networks from
Overfitting" J. Machine Learning Research $\small{(2014)}$
1929.](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)


2.  [J. Schulman, "Deep Reinforcement Learning, Lecture 1: Introduction" UC
Berkeley course CS 294, $\small{(2015)}$](http://rll.berkeley.edu/deeprlcourse-fa15/docs/2015.08.26.Lecture01Intro.pdf)

9. [G. Hinton, "Neural Networks for Machine Learning", Slide 29 of Lecture
6 Coursera notes $\small{(2012)}$.](https://www.coursera.org/learn/neural-networks)

*.  [P. Bachman, A. Sordoni and A. Trischler, "Towards Information-Seeking
Agents", arXiv:1612.02705v1 $\small{(2016)}$.](https://arxiv.org/abs/1612.02605)

