---
layout: post
title:  "Inpainting and Colorizing Flower Pictures via Conditional Image
GANS"
date:   2018-06-27 12:00:00
categories: Deep Learning
use_math: true
published:  true
---

## Image-to-image translation

In November 2016, Isola, Zhu, Zhou and Efros posted an arXiv paper entitled
"Image-to-Image Translation with Conditional Adversarial Networks" which
demonstrated that conditional generative adversarial networks $($CGANs$)$
can be used to transform pictures in one domain into corresponding
counterparts in another [1].  These authors argued that just as a single
verbal concept can be expressed in multiple languages, so can a single
visual concept be rendered in multiple views.  Isola *et al.* developed a
common framework to translate between image pairs.  They subsequently
applied it to several problems in computer vision and computer graphics
such as day-night, photo-map and edge-object image conversions.  Their
results were stunning.

In this blog posting, we build upon the work of ref. [1] and investigate
image inpainting and colorizing applications.  In particular, we follow the
basic strategy of Isola *et al.* but modify several of their implementation
details.  For instance, we dispense with dropout and employ pixelwise
rather than batch normalization following most convolution operations.  We
also introduce time-dependence into CGAN training via initial image
blurring and loss-term coefficient modulation.  We find such temporal
evolution improves inpainting result quality.  Furthermore, we incorporate
random noise descriptor inputs into the CGAN generator.  This source of
non-determinism can lead to synthesized RGB outputs from greyscale inputs
which look more realistic than the colored image progenitors.

We work with a corpus of O$($15K$)$ 256 $\times$ 256 flower images
aggregated from the Oxford 17 and 102 category datasets [2], the Jena 30
dataset [3] and other pictures we manually harvested from the internet [4].
For inpainting experiments, we superpose boxes of various sizes and colors
at random locations in each flower image.  This variable masking procedure
is performed three times for every input picture.  For colorizing
experiments, we convert each RGB image into a unique greyscale counterpart
via a luminosity transformation.  Representative examples of progenitor,
masked and greyscale versions of flower images are displayed in figure 1.

![INPAINT]({{site.url}}/blog/images/inpainting_colorization/originals_bboxes_greyscales_montage.png)
*Fig. 1.  LEFT: Example 256 $\times$ 256 flower images.  CENTER: Randomly
sized and colored boxes superposed at various locations inside each
progenitor picture.  RIGHT: Greyscale versions of progenitor images.*

The downgraded images become conditional inputs to a generator "U-net"
which is schematically illustrated in figure 2.  This network's
encoder-decoder architecture incorporates skip connections between
corresponding layers before and after the middle bottleneck [5].
Isola *et al.* found such skip connections yield much higher quality
synthesized images.  Ideally, the generator should produce output pictures
which look as realistic as the deformed inputs' progenitors.



![INPAINT]({{site.url}}/blog/images/inpainting_colorization/Unet.jpg)
*Fig. 2.  Schematic diagram for CGAN generator's U-net architecture.  This
diagram is a minor variant of figure 3 from [1].*

The "superlayers" in the U-net diagram represent combinations of
convolutions with 5 $\times$ 5 kernels, pixelwise normalizations [6],
leaky ReLU activations [7], linear projections, stride-2 downsamplings
and nearest-neighbor upscalings [8].  The particular tensor combinations
and shapes we employ inside the CGAN generator for inpainting applications
are listed in Table 1a:

<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: left">CGAN generator superlayer</th>
      <th style="text-align: center">Output shape [width $\times$ height $\times$ channels] </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Input conditional image plane</td>
      <td style="text-align: center">256 $\times$ 256 $\times$ 3 </td>
    </tr>

    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu </td>
      <td style="text-align: center">256 $\times$ 256 $\times$ 16 </td>
    </tr>
    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu, stride 2</td>
      <td style="text-align: center">128 $\times$ 128 $\times$ 16 </td>
    </tr>

    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu </td>
      <td style="text-align: center">128 $\times$ 128 $\times$ 24 </td>
    </tr>
    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu, stride 2</td>
      <td style="text-align: center">64 $\times$ 64 $\times$ 24 </td>
    </tr>

    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu </td>
      <td style="text-align: center">64 $\times$ 64 $\times$ 32 </td>
    </tr>
    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu, stride 2</td>
      <td style="text-align: center">32 $\times$ 32 $\times$ 32 </td>
    </tr>

    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu </td>
      <td style="text-align: center">32 $\times$ 32 $\times$ 40 </td>
    </tr>
    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu, stride 2</td>
      <td style="text-align: center">16 $\times$ 16 $\times$ 40 </td>
    </tr>

    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu </td>
      <td style="text-align: center">16 $\times$ 16 $\times$ 48 </td>
    </tr>
    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu, stride 2</td>
      <td style="text-align: center">8 $\times$ 8 $\times$ 48 </td>
    </tr>

    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu </td>
      <td style="text-align: center">8 $\times$ 8 $\times$ 56 </td>
    </tr>

    <tr>
      <td style="text-align: left">Linear projection</td>
      <td style="text-align: center">1 $\times$ 1 $\times$ $Z_{dim}$ </td>
    </tr>

    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu </td>
      <td style="text-align: center">8 $\times$ 8 $\times$ 56 </td>
    </tr>
    <tr>
      <td style="text-align: left">Upscale, Conv5, Pixelwise norm, Leaky Relu</td>
      <td style="text-align: center">16 $\times$ 16 $\times$ $($56 + 48 [skip]$)$ </td>
    </tr>

    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu </td>
      <td style="text-align: center">16 $\times$ 16 $\times$ 48 </td>
    </tr>
    <tr>
      <td style="text-align: left">Upscale, Conv5, Pixelwise norm, Leaky Relu</td>
      <td style="text-align: center">32 $\times$ 32 $\times$ $($48 + 40 [skip]$)$ </td>
    </tr>

    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu </td>
      <td style="text-align: center">32 $\times$ 32 $\times$ 40 </td>
    </tr>
    <tr>
      <td style="text-align: left">Upscale, Conv5, Pixelwise norm, Leaky Relu</td>
      <td style="text-align: center">64 $\times$ 64 $\times$ $($40 + 32 [skip]$)$ </td>
    </tr>

    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu </td>
      <td style="text-align: center">64 $\times$ 64 $\times$ 32 </td>
    </tr>
    <tr>
      <td style="text-align: left">Upscale, Conv5, Pixelwise norm, Leaky Relu</td>
      <td style="text-align: center">128 $\times$ 128 $\times$ $($32 + 24 [skip]$)$ </td>
    </tr>

    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu </td>
      <td style="text-align: center">128 $\times$ 128 $\times$ 24 </td>
    </tr>
    <tr>
      <td style="text-align: left">Upscale, Conv5, Pixelwise norm, Leaky Relu</td>
      <td style="text-align: center">256 $\times$ 256 $\times$ $($24 + 16 [skip]$)$ </td>
    </tr>

    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu </td>
      <td style="text-align: center">256 $\times$ 256 $\times$ 16 </td>
    </tr>
    <tr>
      <td style="text-align: left">Conv5, No norm, Tanh</td>
      <td style="text-align: center">256 $\times$ 256 $\times$ 3</td>
    </tr>

    <!---  Add empty line in final table row -->

    <tr>  
      <td style="text-align: left">$ $</td>
      <td style="text-align: center">$ $</td>
    </tr>
</tbody>
</table>

*Table 1a.  Generator network used to conditionally synthesize 256 $\times$
256 images for inpainting applications.*

The CGAN's discriminator network has a simpler architecture which converts
pairs of progenitor and conditional images into probabilities that the
progenitor is a genuine rather than synthetic picture:

<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: left">CGAN discriminator superlayer</th>
      <th style="text-align: center">Output shape [width $\times$ height $\times$ channels] </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Input image planes + conditional image planes</td>
      <td style="text-align: center">256 $\times$ 256 $\times$ $($3 + 3$)$</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv5, Leaky Relu, stride 2</td>
      <td style="text-align: center">128 $\times$ 128 $\times$ 16</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu, stride 2</td>
      <td style="text-align: center">64 $\times$ 64 $\times$ 32</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu, stride 2</td>
      <td style="text-align: center">32 $\times$ 32 $\times$ 64</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu, stride 2</td>
      <td style="text-align: center">16 $\times$ 16 $\times$ 128</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu, stride 2</td>
      <td style="text-align: center">8 $\times$ 8 $\times$ 256</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu, stride 2</td>
      <td style="text-align: center">4 $\times$ 4 $\times$ 256</td>
    </tr>
    <tr>
      <td style="text-align: left">Linear proj, Sigmoid</td>
      <td style="text-align: center">1 $\times$ 1 $\times$ 1</td>
    </tr>

    <!---  Add empty line in final table row -->

    <tr>
      <td style="text-align: left">$ $</td>
      <td style="text-align: center">$ $</td>
    </tr>
</tbody>
</table>

*Table 1b.  Discriminator network used to analyze $($progenitor,
conditional$)$ image pairs for inpainting applications.*

Several points about this Conditional GAN are worth noting:

i.  The generator [discriminator] network contains 1.9M [3.8M] trainable
variables.  

ii.  Pixelwise normalizations in the generator and discriminator
superlayers significantly stabilize CGAN training [6].  In particular, we
never observe discriminator or generator learning curves suffer from
discontinuous spikes when pixelwise normalizations are utilized.

iii. Skip connections between generator superlayers are implemented by
concatenating encoder tensors onto similarly-sized decoder tensors.

iv.  For colorizing applications, input conditional image planes have 1
rather than 3 color channels.

v.  Optional noise descriptors may be introduced into the CGAN generator
via concatenation with the bottleneck layer.  For colorizing experiments,
we choose to combine the bottleneck's $Z_{dim}$ tensor and a second random
tensor with the same dimensions.  In this case, channel numbers following
the bottleneck layer in Table 1a are doubled as needed.

We split the progenitor and deformed flower pictures into 90% training and
10% testing subsets.  We then train the CGAN network's parameters from
scratch using the TensorFlow deep learning framework [9].  To facilitate
generation of relatively large 256 $\times$ 256 images, we employ the
gradual resolution enhancement approach developed in a previous blog
posting [10].  Image inputs to the discriminator and generator are blurred
with a gaussian kernel whose radius initially equals 10% of their pixel
size.  As the blurring schedule in figure 3 illustrates, the kernel radius
ramps down to zero during the first 15% of training.  No low-pass blurring
is subsequenty performed on input images.

![INPAINT_COLOR]({{site.url}}/blog/images/inpainting_colorization/blur_schedule.jpg)
*Fig. 3.  Blurring kernel radius for discriminator and generator image
inputs linearly decreases to zero during the first 15% of CGAN training.*

Network training attempts to minimize a loss function containing distinct
"photo-reconstruction" and "photo-realism" terms [1]:

$$ {\cal{L}}_{total} = \lambda_{L1} {\cal L}_{L1} + \lambda_{CGAN} {\cal L}_{CGAN}. $$

Minimization of the L1 loss

$$ {\cal{L}}_{L1} = {\bf{E}}_{x, y, z} \bigl{|} x - G(y,z) \bigr{|} $$

encourages the generator to capture low spatial frequency content within
output imagery.  Minimization of the CGAN loss

$$ {\cal{L}}_{CGAN} = {\bf{E}}_{x, y} \bigl[ \log D(x,y) \bigr] +
{\bf{E}}_{y, z} \bigl[ \log(1 - D(G(y,z), y) \bigr]$$

encourages the generator to synthesize high spatial frequencies.  In these
equations, $x$, $y$ and $z$ respectively represent the progenitor image,
deformed image and random noise vector.  While these equations follow
ref. [1], our nomenclature conventions intentionally differ from those of
Isola *et al.*

It is useful to encourage the generator to first concentrate on recovering
gross progenitor image content and to later fill in fine details.  So we
take coefficients $\lambda_{L1}$ and $\lambda_{CGAN}$ to be functions of
training fraction in which the L1 term initially dominates while the CGAN
term grows over time.  After conducting several experiments, we empirically
found the coefficient schedules displayed in figure 4 to work well for
inpainting and colorizing applications.  The starting and stopping values
for $\lambda_{L1}$ and $\lambda_{CGAN}$ are listed in Table 2 along with
several other CGAN training hyperparameters.

![INPAINT_COLOR]({{site.url}}/blog/images/inpainting_colorization/loss_coeffs_schedule.jpg)
*Fig. 4.  The L1 and CGAN loss term coefficients remain fixed during the
first 30% and last 20% of training.  $\lambda_{L1}$ diminishes while
$\lambda_{CGAN}$ grows during the other 50% of training.*


<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: left">Hyperparameter description</th>
      <th style="text-align: center">Value    </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Number of training images</td>
      <td style="text-align: center"> 41544 [inpainting]; 13848 [colorizing] </td>
    </tr>
    <tr>
      <td style="text-align: left">Training image augmentation</td>
      <td style="text-align: center"> Random horizontal flipping </td>
    </tr>
    <tr>
      <td style="text-align: left">Number of training batches</td>
      <td style="text-align: center"> 75K [inpainting]; 50K [colorizing] </td>
    </tr>
    <tr>
      <td style="text-align: left">Minibatch size</td>
      <td style="text-align: center">36</td>
    </tr>
    <tr>
      <td style="text-align: left">Image pixel size</td>
      <td style="text-align: center">256 $\times$ 256</td>
    </tr>
    <tr>
      <td style="text-align: left">Initial blur kernel radius in pixels</td>
      <td style="text-align: center">25.6</td>
    </tr>
    <tr>
      <td style="text-align: left">Final blur kernel radius in pixels</td>
      <td style="text-align: center">0</td>
    </tr>

    <tr>
      <td style="text-align: left">Initial L1 loss term coefficient</td>
      <td style="text-align: center">100.0</td>
    </tr>
    <tr>
      <td style="text-align: left">Final L1 loss term coefficient</td>
      <td style="text-align: center">85.0</td>
    </tr>

    <tr>
      <td style="text-align: left">Initial CGAN loss term coefficient</td>
      <td style="text-align: center">0.0</td>
    </tr>
    <tr>
      <td style="text-align: left">Final CGAN loss term coefficient</td>
      <td style="text-align: center">0.15</td>
    </tr>
    <tr>
      <td style="text-align: left">Adam optimizer $\beta_1$</td>
      <td style="text-align: center">0.0 </td>
    </tr>
    <tr>
      <td style="text-align: left">Adam optimizer $\beta_2$</td>
      <td style="text-align: center">0.99</td>
    </tr>
    <tr>
      <td style="text-align: left">Base learning rate</td>
      <td style="text-align: center">0.0001 </td>
    </tr>
    <tr>
      <td style="text-align: left">Descriptor dimension $Z_{dim}$</td>
      <td style="text-align: center">128 </td>
    </tr>
    <tr>
      <td style="text-align: left">Standard deviation for weight-initializing gaussian</td>
      <td style="text-align: center">0.02 </td>
    </tr>
    <tr>
      <td style="text-align: left">Leaky Relu slope coefficient</td>
      <td style="text-align: center">0.2</td>
    </tr>

    <!---  Add empty line in final table row -->

    <tr>  
      <td style="text-align: left">$ $</td>
      <td style="text-align: center">$ $</td>
    </tr>
  </tbody> 
</table>

*Table 2.  CGAN hyperparameters and their empirically-determined values.*

It is instructive to compare the impacts of the separate L1 and CGAN terms
in the loss function as well as their time-dependent combination upon
imagery reconstruction.  We do so in the next section where we apply
image-to-image translations to flower inpainting and colorizing problems.

## Conditional image GAN results

Our approach to CGAN training forces the discriminator and generator to
first learn about gross object shapes, colors and textures.  While training
proceeds, object details are refined as low-pass filtering is relaxed.
Towards the end of training, the networks incorporate high spatial
frequency content into their inputs and outputs.  Figures 5a and 5b
illustrate these basic trends for both inpainting and colorizing CGANs.

![INPAINT_COLOR]({{site.url}}/blog/images/inpainting_colorization/training_evolution_inpainting.jpg)
*Fig. 5a.  Generator inputs and outputs at equally spaced intervals
during training of the inpainting CGAN.*

![INPAINT_COLOR]({{site.url}}/blog/images/inpainting_colorization/training_evolution_colorizing.png)
*Fig. 5b.  Generator inputs and outputs at equally spaced intervals
during training of the colorizing CGAN.*

Once a CGAN is trained, its generator may be used to perform image
reconstruction on new examples which it never previously saw.  CGAN
reconstructions then cannot represent network memorizations of specific
input-output pairs.  So from here on, we work with inference images which
were separated off from their training counterparts and genuinely test deep
learning generalization.

Figure 6 illustrates inpainting reconstructions based upon the L1 and CGAN
loss terms by themselves as well as their combination that followed the
learning schedule in figure 3.  The L1 reconstructions recover gross shapes
and colors inside the masked box regions but completely lack high spatial
frequency details.  The pure CGAN term reconstructions appear badly
distorted.  In contrast, the combined L1 + CGAN results look remarkably
plausible.  While these reconstructions do not precisely match the
progenitor flower views within the rightmost column of figure 6, we believe
their filling-in of missing information is compelling.

![INPAINT_COLOR]({{site.url}}/blog/images/inpainting_colorization/inpainting_five_frames_0328_1261.png)
*Fig. 6.  COL 1: Randomly sized and colored boxes superposed on inference
flower images.  COL 2: Images reconstructed via a pure L1 loss function.
COL 3: Images reconstructed via a pure CGAN loss function. COL 4: Images
reconstructed via the total loss function whose L1 and CGAN coefficients
follow the training schedule of figure 3.  COL 5: Progenitor flower images.*

Figure 7 presents more examples of generally successful flower image
inpainting.  The figure's first column contains partially masked input
images.  The second column presents L1 + CGAN reconstructions while the
third displays progenitor pictures.  These examples demonstrate that the
inpainting networks have developed a nontrivial understanding of flower
morphology, texture and color patterns.

![INPAINT_COLOR]({{site.url}}/blog/images/inpainting_colorization/decent_inpaintings_montage.png)
*Fig. 7.  LEFT: Randomly sized and colored boxes superposed on various
inference flower images.  CENTER: Images inpainted by the CGAN generator.
RIGHT: Progenitor images.*

The basic goal of image inpainting is to restore lost pixel content as
faithfully as possible.  It consequently tries to map each degraded picture
onto a single reconstruction.  We consequently did not incorporate any
random noise input within the inpainting CGAN's generator.  In contrast,
image colorizing can potentially map every greyscale picture onto many RGB
outputs.  So for the colorizing CGAN generator, we concatenate the
bottleneck layer inside the U-net architecture with a random noise
descriptor to encourage synthetic image variety.  As figure 8a illustrates,
some colorized outputs look qualitatively similar to their progenitor RGB
pictures.  But in other cases like those in figure 8b, the trained CGAN
converts greyscale inputs to colored outputs which nontrivially differ from
the originals.  In a few instances, the synthetically colored views look
more "realistic" to our eye than the actual progenitor pictures!

![INPAINT_COLOR]({{site.url}}/blog/images/inpainting_colorization/qualitatively_similar_colorizations_montage.png)
*Fig. 8a.  LEFT COL: Greyscale versions of various inference flower
images.  CENTER COL: Images colorized by the CGAN generator.  RIGHT COL:
Progenitor images' colorings are qualititatively similar to the
reconstructed images'.*

![INPAINT_COLOR]({{site.url}}/blog/images/inpainting_colorization/nontrivially_different_colorizations_montage.png)
*Fig. 8b.  LEFT COL: Greyscale versions of various inference flower images.
CENTER COL: Images colorized by the trained conditional image GAN
generator.  RIGHT COL: Progenitor images' colorings differ nontrivially
from the reconstructed images'.*

The conditional image GAN results presented so far are encouraging.  But we
have also observed several obvious reconstruction failures.  Figures 9 and
10 display inpainting and colorizing examples that look highly unrealistic.
Some of these poor inference results might lie so far away from training
data samples that the CGAN was unable to reasonably extrapolate.  In other
cases, the networks clearly struggle with complex scenes containing many
different object types.  As we have seen in our previous GAN studies,
adversarial networks can unfortunately fail to produce sensible outputs for
a sizable fraction of their inference inputs.

![INPAINT_COLOR]({{site.url}}/blog/images/inpainting_colorization/bad_inpaintings_montage.png)
*Fig. 9. Poor CGAN inpainting results.  Columns in this figure follow those
in figure 7.*

![INPAINT_COLOR]({{site.url}}/blog/images/inpainting_colorization/bad_colorizations_montage.png)
*Fig. 10. Poor CGAN colorizing results.  Columns in this figure follow
those in figures 8a and 8b.*

After subjectively assessing 180 inpainting and 180 colorizing inference
results, we estimate 84% of the former and 75% of the latter look
reasonably realistic.  In more stringent and controlled "real vs fake"
Amazon Turk perceptual studies, Isola *et al.* found that 22.5% of their
colorization results fooled human subjects [1].  Clearly more work remains
to be done in the future to improve image-to-image translation robustness.


## References

1.  [P. Isola, J.-Y. Zhu, T. Zhou and A. A. Efros, "Image-to-Image
Translation with Conditional Adversarial Networks," arXiv:1611.07004v2 $\small{(2017)}$.](https://arxiv.org/pdf/1611.07004.pdf)

2. [M.-E. Nilsback and A. Zisserman, "17 and 102 Category Flower
Datasets," $\small{(2008)}$.](http://www.robots.ox.ac.uk/~vgg/data/flowers/)

3. [M. Seeland, M. Rzanny, N. Alaqraa, J. Wäldchen and P. Mäder, "Plant Species
Classification Using Flower Images -- A Comparative Study of Local Feature
Representations", PLOS ONE 12 $\small{(2017)}$.](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0170629)

4. [See "Improving Flower Image Classification via Conditional GAN Data
Augmentation" post in this blog series $\small{(2018)}.](https://petercho.github.io.blog//deep/learning/2018/05/18/cvml.html)

5.  [O. Ronneberger, P. Fischer and T. Brox, "U-Net: Convolutional Networks
for Biomedical Image Segmentation," arXiv:1505.04597v1 $\small{(2015)}$.](https://arxiv.org/pdf/1505.04597.pdf)

6. [T. Karras, T. Aila, S. Laine and J. Lehtinen, "Progressive Growing of
GANs for Improved Quality, Stability and Variation", ICLR $\small{(2018)}$.](https://arxiv.org/pdf/1710.10196.pdf)

7.  [A.L. Maas, A.Y. Hannun and A.Y. Ng, "Rectifier Nonlinearities Improve
Neural Network Acoustic Models", 30th ICML $\small{(2013)}$.](https://web.stanford.edu/~awni/papers/relu_hybrid_icml2013_final.pdf)

8. [A. Odena, V. Dumoulin and C. Olah, "Deconvolution and Checkerboard
Artifacts," Distill
$\small{(2016)}$.](https://distill.pub/2016/deconv-checkerboard/)

9.  [TensorFlow: An open-source software library for Machine Intelligence.](https://www.tensorflow.org/)

10.  [See "Synthesizing Bird Images via Gradual Resolution Enhancement
during GAN Training" post in this blog series
$\small{(2018)}$.](https://petercho.github.io/blog//deep/learning/2018/04/21/cvml.html)









