---
layout: post
title:  "Inpainting and Colorizing Flower Pictures via Conditional Image
GANS"
date:   2018-06-27 12:00:00
categories: Deep Learning
use_math: true
published: false
---

## Image-to-image translation

In November 2016, Isola, Zhu, Zhou and Efros posted an arXiv paper entitled
"Image-to-Image Translation with Conditional Adversarial Networks" which
demonstrated that conditional GANs can be used to transform pictures in one
domain into corresponding counterparts in another [1].  These authors
argued that just as a single verbal concept can be expressed in multiple
languages, so can a single visual concept be rendered in multiple views.
Isola *et al.* developed a common framework to translate between image
pairs.  They subsequently applied it to several problems in computer vision
and computer graphics such as day-night, photo-map and edge-object image
conversions.  Their results were stunning.

In this blog posting, we build upon the work of ref. [1] and focus upon
image inpainting and colorizing applications.  In particular, we follow the
basic strategy of Isola *et al.* but modify several of their implementation
details.  For instance, we dispense with dropout and employ pixelwise
rather than batch normalization following most convolution operations.  We
also introduce time-dependence into CGAN training via initial image
blurring and loss-term coefficient modulation.  We find such temporal
evolution improves inpainting results' quality.  Furthermore, we
incorporate random noise descriptor inputs into the CGAN generator.  This
source of non-determinism can lead to synthesized RGB outputs from
greyscale inputs which may look more realistic than the colored image
progenitors.

We work with a corpus of O$($15K$)$ 256 $\times$ 256 flower images
aggregated from the Oxford 17 and Oxford 102 datasets [refs], the Jena 30
dataset [refs] and other pictures we manually harvested from the internet
[ref].  For inpainting experiments, we superpose boxes of various sizes and
colors at random locations in each flower image.  This variable masking
procedure is performed three times for every input picture.  For colorizing
experiments, we convert each RGB image into a unique greyscale counterpart
via a luminosity transformation.  Representative examples of progenitor,
partly masked and greyscale versions of flower images are displayed in
figure 1.

![INPAINT]({{site.url}}/blog/images/inpainting_colorization/originals_bboxes_greyscales_montage.png)
*Fig. 1.  LEFT: Example 256 $\times$ 256 flower images.  CENTER: Randomly
sized and colored boxes superposed at various locations inside each
progenitor picture.  RIGHT: Greyscale versions of progenitor images.*


The downgraded images become conditional inputs to a generator "U-net"
which is schematically illustrated in figure 2.  This network's
encoder-decoder architecture incorporates skip connections between
corresponding layers before and after the middle bottleneck [Unet ref].
Isola *et al.* found such skip connections yield much higher quality
synthesized images.  Ideally, the generator should produce output pictures
which look as realistic as the deformed inputs' progenitors.



![INPAINT]({{site.url}}/blog/images/inpainting_colorization/Unet.jpg)
*Fig. 2.  Schematic diagram for CGAN generator's U-net architecture.  This
diagram is a minor variant of figure 3 from [1].*

The "superlayers" in the U-net diagram represent combinations of
convolutions with 5 $\times$ 5 kernels, pixelwise normalizations [ref],
leaky ReLU activations [ref], linear projections, stride 2 downsamplings
and nearest-neighbor upscalings [ref].  The particular tensor combinations
and shapes we employ inside the CGAN generator for inpainting applications
are listed in Table 1a:

<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: left">CGAN generator superlayer</th>
      <th style="text-align: center">Output shape [width $\times$ height $\times$ channels] </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Input conditional image plane</td>
      <td style="text-align: center">256 $\times$ 256 $\times$ 3 </td>
    </tr>

    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu </td>
      <td style="text-align: center">256 $\times$ 256 $\times$ 16 </td>
    </tr>
    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu, stride 2</td>
      <td style="text-align: center">128 $\times$ 128 $\times$ 16 </td>
    </tr>

    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu </td>
      <td style="text-align: center">128 $\times$ 128 $\times$ 24 </td>
    </tr>
    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu, stride 2</td>
      <td style="text-align: center">64 $\times$ 64 $\times$ 24 </td>
    </tr>

    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu </td>
      <td style="text-align: center">64 $\times$ 64 $\times$ 32 </td>
    </tr>
    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu, stride 2</td>
      <td style="text-align: center">32 $\times$ 32 $\times$ 32 </td>
    </tr>

    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu </td>
      <td style="text-align: center">32 $\times$ 32 $\times$ 40 </td>
    </tr>
    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu, stride 2</td>
      <td style="text-align: center">16 $\times$ 16 $\times$ 40 </td>
    </tr>

    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu </td>
      <td style="text-align: center">16 $\times$ 16 $\times$ 48 </td>
    </tr>
    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu, stride 2</td>
      <td style="text-align: center">8 $\times$ 8 $\times$ 48 </td>
    </tr>

    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu </td>
      <td style="text-align: center">8 $\times$ 8 $\times$ 56 </td>
    </tr>

    <tr>
      <td style="text-align: left">Linear projection</td>
      <td style="text-align: center">1 $\times$ 1 $\times$ $Z_{dim}$ </td>
    </tr>

    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu </td>
      <td style="text-align: center">8 $\times$ 8 $\times$ 56 </td>
    </tr>
    <tr>
      <td style="text-align: left">Upscale, Conv5, Pixelwise norm, Leaky Relu</td>
      <td style="text-align: center">16 $\times$ 16 $\times$ $($56 + 48 [skip]$)$ </td>
    </tr>

    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu </td>
      <td style="text-align: center">16 $\times$ 16 $\times$ 48 </td>
    </tr>
    <tr>
      <td style="text-align: left">Upscale, Conv5, Pixelwise norm, Leaky Relu</td>
      <td style="text-align: center">32 $\times$ 32 $\times$ $($48 + 40 [skip]$)$ </td>
    </tr>

    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu </td>
      <td style="text-align: center">32 $\times$ 32 $\times$ 40 </td>
    </tr>
    <tr>
      <td style="text-align: left">Upscale, Conv5, Pixelwise norm, Leaky Relu</td>
      <td style="text-align: center">64 $\times$ 64 $\times$ $($40 + 32 [skip]$)$ </td>
    </tr>

    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu </td>
      <td style="text-align: center">64 $\times$ 64 $\times$ 32 </td>
    </tr>
    <tr>
      <td style="text-align: left">Upscale, Conv5, Pixelwise norm, Leaky Relu</td>
      <td style="text-align: center">128 $\times$ 128 $\times$ $($32 + 24 [skip]$)$ </td>
    </tr>

    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu </td>
      <td style="text-align: center">128 $\times$ 128 $\times$ 24 </td>
    </tr>
    <tr>
      <td style="text-align: left">Upscale, Conv5, Pixelwise norm, Leaky Relu</td>
      <td style="text-align: center">256 $\times$ 256 $\times$ $($24 + 16 [skip]$)$ </td>
    </tr>

    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu </td>
      <td style="text-align: center">256 $\times$ 256 $\times$ 16 </td>
    </tr>
    <tr>
      <td style="text-align: left">Conv5, No norm, Tanh</td>
      <td style="text-align: center">256 $\times$ 256 $\times$ 3</td>
    </tr>

    <!---  Add empty line in final table row -->

    <tr>  
      <td style="text-align: left">$ $</td>
      <td style="text-align: center">$ $</td>
    </tr>
</tbody>
</table>

*Table 1a.  Generator network used to conditionally synthesize 256 $\times$
256 images for inpainting applications.*

The CGAN's discriminator network has a simpler architecture which converts
pairs of progenitor and conditional images into probabilities that the
progenitor is a genuine rather than synthetic picture:

<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: left">CGAN discriminator superlayer</th>
      <th style="text-align: center">Output shape [width $\times$ height $\times$ channels] </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Input image planes + conditional image planes</td>
      <td style="text-align: center">256 $\times$ 256 $\times$ $($3 + 3$)$</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv5, Leaky Relu, stride 2</td>
      <td style="text-align: center">128 $\times$ 128 $\times$ 16</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu, stride 2</td>
      <td style="text-align: center">64 $\times$ 64 $\times$ 32</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu, stride 2</td>
      <td style="text-align: center">32 $\times$ 32 $\times$ 64</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu, stride 2</td>
      <td style="text-align: center">16 $\times$ 16 $\times$ 128</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu, stride 2</td>
      <td style="text-align: center">8 $\times$ 8 $\times$ 256</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu, stride 2</td>
      <td style="text-align: center">4 $\times$ 4 $\times$ 256</td>
    </tr>
    <tr>
      <td style="text-align: left">Linear proj, Sigmoid</td>
      <td style="text-align: center">1 $\times$ 1 $\times$ 1</td>
    </tr>

    <!---  Add empty line in final table row -->

    <tr>
      <td style="text-align: left">$ $</td>
      <td style="text-align: center">$ $</td>
    </tr>
</tbody>
</table>

*Table 1b.  Discriminator network used to analyze $($progenitor,
conditional$)$ image pairs for inpainting applications.*

Several points about this Conditional GAN are worth noting:

i.  The generator [discriminator] network contains 1.9M [3.8M] trainable
variables.  

ii.  Pixelwise normalizations in the generator and discriminator
superlayers significantly stabilize CGAN training [ref].  In particular, we
never observed discriminator or generator learning curves exhibit
discontinuous spikes when pixelwise normalizations are utilized.  

iii. Skip connections between generator superlayers are implemented by
concatenating encoder tensors onto similarly-sized decoder tensors.

iv.  For colorizing applications, input conditional image planes have 1
rather than 3 color channels.

v.  Optional noise descriptors may be introduced into the CGAN generator
via concatenation with the bottleneck layer.  For colorizing experiments,
we choose to combine the bottleneck's $Z_{dim}$ tensor and a second random
tensor with the same dimensions.  In this case, channel numbers following
the bottleneck layer in Table 1a are doubled as needed.

We split the progenitor and deformed flower pictures into 90% training and
10% testing subsets.  The CGAN network's parameters are then trained from
scratch via the TensorFlow deep learning framework [ref].  To facilitate
generation of relatively large 256 $\times$ 256 images, we employ the
gradual resolution enhancement approach developed in a previous blog
posting [ref].  Image inputs to the discriminator and generator are blurred
with a gaussian kernel whose radius initially equals 10% of their pixel
size.  As the blurring schedule in figure XX illustrates, the kernel radius
ramps down to zero during the first 15% of training.  No low-pass blurring
is subsequenty performed on input images.

![INPAINT_COLOR]({{site.url}}/blog/images/inpainting_colorization/blur_schedule.jpg)
*Fig. XX.  Blurring kernel radius for discriminator and generator image
inputs linearly decreases to zero during the first 15% of CGAN training.*

Network training attempts to minimize a loss function containing distinct
"photo-reconstruction" and "photo-realism" terms [1]:

$$ {\cal{L}}_{total} = \lambda_{L1} {\cal L}_{L1} + \lambda_{CGAN} {\cal L}_{CGAN}. $$

Minimization of the L1 loss

$$ {\cal{L}}_{L1} = {\bf{E}}_{x, y, z} \bigl{|} x - G(y,z) \bigr{|} $$

encourages the generator to capture low spatial frequency content within
output imagery.  Minimization of the CGAN function

$$ {\cal{L}}_{CGAN} = {\bf{E}}_{x, y} \bigl[ \log D(x,y) \bigr] +
{\bf{E}}_{y, z} \bigl[ \log(1 - D(G(y,z), y) \bigr]$$

encourages the generator to synthesize high spatial frequencies.  In these
equations, $x$, $y$ and $z$ respectively represent the progenitor image,
deformed image and random noise vector.  Our equations follow ref. [1].
But our nomenclature conventions intentionally differ from those of Isola
*et al.*

It's useful to encourage the generator to first concentrate on recovering
gross progenitor image content and to later fill in fine details.  So we
take coefficients $\lambda_{L1}$ and $\lambda_{CGAN}$ to be functions of
training fraction in which the L1 term initially dominates while the CGAN
term grows over time.  After conducting several experiments, we empirically
found the coefficient schedules displayed in fig XX to work well for
inpainting and colorizing applications.  The starting and stopping values
for $\lambda_{L1}$ and $\lambda_{CGAN}$ are listed in Table 2 along with
several other CGAN training hyperparameters.

![INPAINT_COLOR]({{site.url}}/blog/images/inpainting_colorization/loss_coeffs_schedule.jpg)
*Fig. XX.  The L1 and CGAN loss term coefficients remain fixed during the
first 30% and last 20% of training.  $\lambda_{L1}$ diminishes while
$\lambda_{CGAN}$ grows during the other 50% of training.*

<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: left">Hyperparameter description</th>
      <th style="text-align: center">Value    </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Number of training images</td>
      <td style="text-align: center"> 41544 [inpainting]; 13848 [colorizing] </td>
    </tr>
    <tr>
      <td style="text-align: left">Training image augmentation</td>
      <td style="text-align: center"> Random horizontal flipping </td>
    </tr>
    <tr>
      <td style="text-align: left">Number of training batches</td>
      <td style="text-align: center"> 75K [inpainting]; 50K [colorizing] </td>
    </tr>
    <tr>
      <td style="text-align: left">Minibatch size</td>
      <td style="text-align: center">36</td>
    </tr>
    <tr>
      <td style="text-align: left">Image pixel size</td>
      <td style="text-align: center">256 $\times$ 256</td>
    </tr>
    <tr>
      <td style="text-align: left">Initial blur kernel radius in pixels</td>
      <td style="text-align: center">25.6</td>
    </tr>
    <tr>
      <td style="text-align: left">Final blur kernel radius in pixels</td>
      <td style="text-align: center">0</td>
    </tr>

    <tr>
      <td style="text-align: left">Initial L1 loss term coefficient</td>
      <td style="text-align: center">100.0</td>
    </tr>
    <tr>
      <td style="text-align: left">Final L1 loss term coefficient</td>
      <td style="text-align: center">85.0</td>
    </tr>

    <tr>
      <td style="text-align: left">Initial CGAN loss term coefficient</td>
      <td style="text-align: center">0.0</td>
    </tr>
    <tr>
      <td style="text-align: left">Final CGAN loss term coefficient</td>
      <td style="text-align: center">0.15</td>
    </tr>
    <tr>
      <td style="text-align: left">Adam optimizer $\beta_1$</td>
      <td style="text-align: center">0.0 </td>
    </tr>
    <tr>
      <td style="text-align: left">Adam optimizer $\beta_2$</td>
      <td style="text-align: center">0.99</td>
    </tr>
    <tr>
      <td style="text-align: left">Base learning rate</td>
      <td style="text-align: center">0.0001 </td>
    </tr>
    <tr>
      <td style="text-align: left">Descriptor dimension $Z_{dim}$</td>
      <td style="text-align: center">128 </td>
    </tr>
    <tr>
      <td style="text-align: left">Standard deviation for weight-initializing gaussian</td>
      <td style="text-align: center">0.02 </td>
    </tr>
    <tr>
      <td style="text-align: left">Leaky Relu slope coefficient</td>
      <td style="text-align: center">0.2</td>
    </tr>

    <!---  Add empty line in final table row -->

    <tr>  
      <td style="text-align: left">$ $</td>
      <td style="text-align: center">$ $</td>
    </tr>
  </tbody> 
</table>

*Table 2.  CGAN hyperparameters and their empirically-determined values.*

## Image inpainting results



![INPAINT_COLOR]({{site.url}}/blog/images/inpainting_colorization/training_evolution.jpg)
*Fig. XX.  Sample image inputs to discriminator and generator and
corresponding generator outputs at equally spaced intervals during training.*


![INPAINT_COLOR]({{site.url}}/blog/images/inpainting_colorization/inpainting_five_frames_0328_1261.png)
*Fig. AA.  COL 1: Randomly sized and colored boxes superposed on inference
flower images.  COL 2: Images reconstructed via a pure autoencoder loss
function.  COL 3: Images reconstructed via a pure GAN loss function. COL 4:
Images reconstructed via a loss function with autoencoder and GAN terms
whose coefficients follow the schedule in figure XX.  COL 5: Progenitor
flower images.*



![INPAINT_COLOR]({{site.url}}/blog/images/inpainting_colorization/decent_inpaintings_montage.png)
*Fig. BB.  LEFT: Randomly sized and colored boxes superposed on various
inference flower images.  CENTER: Images inpainted by the trained
conditional image GAN generator.  RIGHT: Progenitor images.*

![INPAINT_COLOR]({{site.url}}/blog/images/inpainting_colorization/bad_inpaintings_montage.png)
*Fig. CC. Poor GAN inpainting results.  Columns in this figure follow the
pattern of figure BB.*



## Image colorizing results

In closing, we note that "real-world" training sets are often imbalanced
among different classes.  Practical problems can render some classifier
data inputs much more difficult to gather than others.  It would be
interesting to see how well Conditional GAN data augmentation could help
compensate for *a priori* unbalanced training sets.  In particular,
training a CGAN from a highly skewed corpus and then using it to rebalance
the object classes might improve classifier performance.  We leave studying
such class balancing issues to future work.

![INPAINT_COLOR]({{site.url}}/blog/images/inpainting_colorization/qualitatively_similar_colorizations_montage.png)
*Fig. DD.  LEFT COLS: Greyscale versions of various inference flower images.
CENTER COLS: Images colorized by the trained conditional image GAN
generator.  RIGHT COLS: Progenitor images.*

![INPAINT_COLOR]({{site.url}}/blog/images/inpainting_colorization/nontrivially_different_colorizations_montage.png)
*Fig. EE.  LEFT COLS: Greyscale versions of various inference flower images.
CENTER COLS: Images colorized by the trained conditional image GAN
generator.  RIGHT COLS: Progenitor images.*




![INPAINT_COLOR]({{site.url}}/blog/images/inpainting_colorization/bad_colorizations_montage.png)
*Fig. FF. Poor GAN colorizing results.  Columns in this figure follow the
pattern of figures DD and EE.*



## References


1.  [P. Isola, J.-Y. Zhu, T. Zhou and A. A. Efros, "Image-to-Image
Translation with Conditional Adversarial Networks," arXiv:1611.07004v2 $\small{(2017)}$.](https://arxiv.org/pdf/1611.07004.pdf)

2.  [O. Ronneberger, P. Fischer and T. Brox, "U-Net: Convolutional Networks
for Biomedical Image Segmentation," arXiv:1505.04597v1 $\small{(2015)}$.](https://arxiv.org/pdf/1505.04597.pdf)


1.  [A. Krizhevsky, I. Sutskever and G.E. Hinton, "Imagenet Classification
with Deep Convolutional Neural Networks," NIPS
$\small{(2012)}$.](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)

2. [See TensorFlow tutorial "How to Retrain an Image Classifier for New
Categories."](https://www.tensorflow.org/tutorials/image_retraining)

3.  [S. Ioffe and C. Szegedy, "Batch Normalization: Accelerating Deep
Network Training by Reducing Internal Covariate Shift," 32nd ICML 
$\small{(2015)}$.](https://arxiv.org/abs/1502.03167)

4.  [A.L. Maas, A.Y. Hannun and A.Y. Ng, "Rectifier Nonlinearities Improve
Neural Network Acoustic Models", 30th ICML $\small{(2013)}$.](https://web.stanford.edu/~awni/papers/relu_hybrid_icml2013_final.pdf)

5.  [N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever and
R. Salakhutdinov, "Dropout: A Simple Way to Prevent Neural Networks from
Overfitting," Journal of Machine Learning Research $\bf {15}$
$\small{(2014)}$.](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)

6.  [D.H.T Hien, "A guide to receptive field arithmetic for Convolutional
Neural Networks," $\small{(2017)}$.](https://medium.com/@nikasa1889/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807)

7.  [TensorFlow: An open-source software library for Machine Intelligence.](https://www.tensorflow.org/)

8.  [A. Antoniou, A. Storkey and H. Edwards, "Data Augmentation Generative
Adversarial Networks," 
arXiv:1711.043040v3 $\small{(2018)}$.](https://arxiv.org/pdf/1711.04340.pdf)

9.  [G. Mariani, F. Scheidegger, R. Istrate, C. Bekas and C. Malossi,
"BAGAN: Data Augmentation with Balancing GAN,"
arXiv:1803.09655v1 $\small{(2018)}$.](https://arxiv.org/pdf/1803.09655.pdf)

10.  [M. Frid-Adar, E. Klang, M. Amitai, J. Goldberger and H. Greenspan,
"Synthetic Data Augmentation using GAN for Improved Liver Lesion
Classification," arXiv:1801.02385v1
$\small{(2018)}$.](https://arxiv.org/pdf/1801.02385.pdf)

11.  [M. Mirza and S. Osindero, "Conditional Generative Adversarial Nets,"
arXiv:1411.1784v1 $\small{(2014)}$.](https://arxiv.org/pdf/1411.1784.pdf)

12. [A. Odena, V. Dumoulin and C. Olah, "Deconvolution and Checkerboard
Artifacts," Distill
$\small{(2016)}$.](https://distill.pub/2016/deconv-checkerboard/)

13. [T. Karras, T. Aila, S. Laine and J. Lehtinen, "Progressive Growing of
GANs for Improved Quality, Stability and Variation", ICLR $\small{(2018)}$.](https://arxiv.org/pdf/1710.10196.pdf)

14. [H. Zhang, T. Xu, H. Li, S. Zhang, X. Wang, X. Huang and D. Metaxas,
"StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative
Adversarial Networks," arXiv:1612.03242v2
$\small{(2017)}$.](https://arxiv.org/pdf/1612.03242.pdf)

15.  [See "Synthesizing Bird Images via Gradual Resolution Enhancement
during GAN Training" post in this blog series
$\small{(2018)}$.](https://petercho.github.io/blog//deep/learning/2018/04/21/cvml.html)

16.  [I. Goodfellow, "NIPS 2016 Tutorial: Generative Adversarial Networks,"
arXiv:1701.001604v4 $\small{(2017)}$.](https://arxiv.org/pdf/1701.00160.pdf)

17.  [A. Odena, C. Olah and J. Shlens, "Conditional Image Synthesis with
Auxiliary Classifier GANs,"
arXiv:1610.09585v4 $\small{(2017)}$.](https://arxiv.org/pdf/1610.09585.pdf)


