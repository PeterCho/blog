---
layout: post
title:  "Localizing Faces via Image Scaling, Data Augmentation and Instance Segmentation"
date:   2016-06-27 12:00:0
categories: Deep Learning
---

## Training data for faces and hands

In an earlier blog post, we presented a first-attempt at face detection via
deep semantic segmentation [1].  The initial results based upon 3K+
training images were qualitatively encouraging.  But we noted a number of
false negative and false positive results generated by our trained network.
Moreover, two or more faces appearing close together in an image were
sometimes merged together into a single segmentation region.  We also did
not quantify the performance of our first-attempt system.  In this blog
post, we return to the face detection problem and focus upon rectifying
these earlier shortcomings.

We start by increasing the imagery dataset size.  This time, we work with
11491 internet images which were intentionally chosen to exhibit variety
along multiple dimensions.  2815 of the images originated from the WIDER
collection [2], while the remaining 8676 were downloaded primarily from
Google Images.  A random subset of 700 images was reserved for validation,
and another random subset of 700 images was held out for testing.  The
remaining 10K+ images were used for training.

Each input image was manually inspected for human face content.  Any faces
found were annotated with bounding boxes using dlib's image labeling tool
[3].  A total of 36196 boxes were wrapped around faces in 8671 images.  As
figure 1 illustrates, the annotated faces belong to people with varying
attributes (e.g. gender, age, ethnicity, nationality) performing different
activities (e.g. eating, running, protesting, sleeping) in disparate
settings (e.g. restaurants, farms, business meetings, war zones).  Faces
occur in different poses (e.g. straight towards camera, profile,
downwards), and several are partially occluded by objects (e.g. eyeglasses,
masks, cigarettes).  Moreover, some of our face data intentionally come
from "2D" sources such as photographs, paintings and drawings.  As Table 1
indicates, the face bounding boxes span a wide range of pixel widths.


![FaceLocalization]({{site.url}}/blog/images/face_localization/training_images/training_montage.png)
*Fig. 1. Representative examples of annotated training images.  Human faces
and hands are marked by red and cyan bounding boxes.*


<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: center">Bounding box</th>
      <th style="text-align: center">Lower bounding box</th>
      <th style="text-align: center">Upper bounding box</th>
    </tr>
    <tr>
      <th style="text-align: center">percentage</th>
      <th style="text-align: center">pixel width</th>
      <th style="text-align: center">pixel width</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">0-10</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">7.7</td>
    </tr>
    <tr>
      <td style="text-align: center">10-20</td>
      <td style="text-align: center">7.7</td>
      <td style="text-align: center">12.9</td>
    </tr>
    <tr>
      <td style="text-align: center">20-30</td>
      <td style="text-align: center">12.9</td>
      <td style="text-align: center">18.7</td>
    </tr>
    <tr>
      <td style="text-align: center">30-40</td>
      <td style="text-align: center">18.7</td>
      <td style="text-align: center">24.9</td>
    </tr>
    <tr>
      <td style="text-align: center">40-50</td>
      <td style="text-align: center">24.9</td>
      <td style="text-align: center">31.2</td>
    </tr>
    <tr>
      <td style="text-align: center">50-60</td>
      <td style="text-align: center">31.2</td>
      <td style="text-align: center">39.2</td>
    </tr>
    <tr>
      <td style="text-align: center">60-70</td>
      <td style="text-align: center">39.2</td>
      <td style="text-align: center">50.8</td>
    </tr>
    <tr>
      <td style="text-align: center">70-80</td>
      <td style="text-align: center">50.8</td>
      <td style="text-align: center">67.9</td>
    </tr>
    <tr>
      <td style="text-align: center">80-90</td>
      <td style="text-align: center">67.9</td>
      <td style="text-align: center">100.3</td>
    </tr>
    <tr>
      <td style="text-align: center">90-100</td>
      <td style="text-align: center">100.3</td>
      <td style="text-align: center">Infinity</td>
    </tr>
  </tbody>
</table>

<!---

|  Bbox   	| Lower bbox    | Upper bbox    |
|  percentage  	| pixel width   | pixel width   | 
|:-------------:|:-------------:|:-------------:|
|   0-10	|  0 	        |  7.7  	|
|   10-20	|  7.7	        | 12.9  	|
|   20-30 	|  12.9	        | 18.7  	| 
|   30-40 	|  18.7 	| 24.9  	|
|   40-50	|  24.9 	| 31.2  	| 
|   50-60	|  31.2 	| 39.2  	| 
|   60-70	|  39.2 	| 50.8	        | 
|   70-80	|  50.8 	| 67.9  	| 
|   80-90	|  67.9	        | 100.3  	| 
|   90-100	|  100.3	| Infinity      | 

-->

*Table 1.  Pixel width ranges for 36K+ face bounding boxes listed as
functions of cumulative bounding box percentages.*

As we generated face bounding boxes, we observed that people frequently put
their hands on faces (see figure 2).  Indeed, 2363 of our 36196 face
bounding boxes are overlapped by bare hand content.  Semantic segmentation
networks trained with such "contaminated" face data often mistakenly
classify hand pixels as belonging to faces.  So we decided to label hands
as well as faces in our imagery corpus.  But we only annotated bare hands
which share similar skin tones to faces and ignored hands covered by opaque
gloves.  7259 of our 11491 images contain bare hand content.

Since hands are inherently more deformable than faces, their appearance
within images is significantly more varied.  As dlib's image labeling tool
only enables objects to be annotated by rectangles rather than arbitrary
polygons, we sometimes used more than one bounding box to trace different
parts of a hand with higher fidelity than a single rectangle could achieve.
In contrast, we always labeled a single face with a single bounding box.

![FaceLocalization]({{site.url}}/blog/images/face_localization/training_images/hands_on_faces.png)
*Fig. 2. Examples of annotated images with overlapping face and hand
bounding boxes.  A single hand may be enclosed by more than one box.*

Among our training, validation and testing images, 2146 have neither human
faces nor hands.  Figure 3 illustrates a few of these negative examples.
Some contain no human content.  Others display body parts such as backs of
heads, torsos and legs whose shapes or colorings might be confused with
faces or hands by a machine.  We also intentionally included several
pictures of animal faces into our imagery corpus which a classifier should
learn to distinguish from human faces.  Moreover, we made sure to
incorporate photos, paintings and line drawings of non-human faces so that
a classifier would not always fire on such 2D representations.

![FaceLocalization]({{site.url}}/blog/images/face_localization/training_images/negative_examples.png)
*Fig. 3. Negative examples containing neither human face nor hand content.*

As in our earlier face detection post, we work with the DeepLab framework
[4 - 6] built on top of Caffe [7] in order to perform semantic image
segmentation.  For Deep Neural Network (DNN) training, we decompose the
annotated training images into square tiles with 321x321 pixel dimensions.
For each tile, a mask is written to an 8-bit greyscale PNG file whose value
= 0, 1 or 2 for background, face or hand pixels.  Exported image tiles
become inputs to Deeplab's DNN, while the masks become inputs to Deeplab's
loss function.

In our first-attempt at face detection via semantic segmentation, we
generated DNN training data using images and bounding box annotations at
their original pixel sizes.  But human faces appear in internet imagery at
a wide variety of scales.  So this time, we consider an input image along
with its half-sized and double-sized versions.  The choice of scale for a
training image depends upon the median pixel width of all its bounding
boxes.  If the median width is less than 27 pixels, we replace the original
image by its double-sized version and multiply all its bounding box
coordinates by 2.  On the other hand, we work with the half-sized version
and divide all bounding box coordinates by 2 if the median pixel width is
greater than 54.  Face content used for training is thereby rendered closer
in scale to the DNN's O(100 pixel) receptive field size.

In this investigation, we also oversample every training image in order to
augment the total volume of input data.  Each annotated bounding box
becomes a seed for a square tile.  The face [hand] bounding box is allowed
to seed up to 8 [3] tiles, and a maximum of 20 tiles are randomly generated
per image.  If the bounding box has dimensions less than 321x321, the tile
is randomly positioned so as to include the entire rectangle.  Otherwise,
the tile contains some random part of the bounding box contents.

We perform further data augmentation by modulating the image tiles' color
contents.  Each pixel in a fixed percentage of tiles has its hue,
saturation and value color coordinates perturbed by small, constant
offsets.  Furthermore, gaussian noise is added to a random subset of tiles.
50% of all tiles are also horizontally flipped.  Figures 4a and 4b display
one representative training image and 18 of its modulated tiles.  By seeing
many such variations, the DNN hopefully grows insensitive to precise image
positioning, coloring and noise levels when learning to classify faces and
hands.

![FaceLocalization]({{site.url}}/blog/images/face_localization/data_augmentation/image_00692.jpg)
*Fig. 4a. A representative training image containing multiple face and hand
bounding boxes.*

![FaceLocalization]({{site.url}}/blog/images/face_localization/data_augmentation/output_6x3_tile_montage.jpg)
*Fig. 4b. Examples of training tiles extracted around face and hand boxes
in figure 4a.  Note variations in the image chips' translations, colorings
and horizontal parities.*

A total of 147984 image tile-plus-mask pairs were generated for DNN
training.  Approximately 84% of the 321x321 tiles contained face and/or
hand content, while the remaining 16% corresponded to negative training
data.  All of the tile-mask pairs were used to finetune a DeepLab variant
of the VGG-16 network [8].  We set the training batch size = 30, base
learning rate = 0.000125, momentum = 0.9 and weight decay = 0.0005.
DeepLab's loss function and pixel-level validation curves then appeared to
asymptote after the network was trained via a conventional SGD solver for
5+ epochs.

## Instance segmentation of faces

Once the DNN is trained, we can apply it to the images in our validation
and testing sets.  Each inference image is decomposed into 1500x1500 tiles,
and adjacent tiles overlap their neighbors by 100 pixels.  After the DNN
segments every tile pixel, the tiles are recombined to form a single mask
whose width and height dimensions precisely match those of the original
image.  Since label assignments located within 100 pixels of an interior
tile border can be ignored, deleterious tile edge effects upon semantic
segmentation are minimized.

Just as taking scale variation of human faces into account is important for
training, so too is it necessary for inference.  We consequently pass each
new image along with its half-sized and double-sized versions through the
trained network.  For example, figure 5 displays one representative image
from the 1990's "Friends" sitcom.  Face and hand pixels segmented by the
trained DNN are tinted red and yellow in figure 6a at the three different
image scales.  For each pixel labeled as face or hand, the classifier
returns a confidence value between 0 to 1.  Confidence scores for the three
scaled versions of the "Friends" image are illustrated as heat maps in
figure 6b.

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/friends/image_04084.jpg)
*Fig. 5. A "Friends" validation image.*

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/friends/double_full_half_ccs.png)
*Fig. 6a.  Pixel-level semantic segmentation of faces and hands in
the test image (upper right) and its double-sized (left) and
half-sized (lower right) versions.  Colored bounding boxes surround
connected components within these segmentation masks.*

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/friends/double_full_half_scores.png)
*Fig. 6b.  Pixel-level scores for semantic segmentation results in figure
6a.  Warmer colors indicate higher confidences than cooler colors.*

We next form connected components from segmented face and hand pixels at
each image scale.  Bounding boxes enclosing the connected components appear
as colored rectangles in figure 6a.  If a connected component corresponds
to genuine signal and if it is relatively small compared to the DNN's O(100
pixel) receptive field in the original-sized or half-sized images, we
expect to find a larger counterpart in the double-sized or original-sized
images.  Similarly if a connected component is relatively large in the
original-sized or double-sized images, a smaller counterpart should exist
in the half-sized or original-sized images.  We consequently ignore
connected components which are too small or too large at particular image
scales.

Surviving connected components are assigned confidence values corresponding
to the "hottest" 20th-percentile among its individual pixel scores.  Since
we are primarily interested in face detection, we set a higher threshold
score for hand components than for face components.  Moreover, we expect
object detection to be easiest to perform within half-sized images and
hardest for double-sized images.  So our thresholds are more stringent
[lenient] for the former [latter].  Connected components whose confidence
values do not meet these score thresholds are rejected.  Bounding boxes
for components rejected either by size or score considerations are colored
black in figure 6a.

We next consolidate all the connected components across the three image
scales.  Components occuring in double-sized [half-sized] images are first
downsampled [upsampled] to original-sized image dimensions.  We then search
for connected components originating from different scales which overlap in
area by more than 25%.  The component with the higher confidence value
relative to its score threshold is retained, while the other component is
rejected.

The connected components surviving after image scale consolidation for our
"Friends" example are presented in figure 7.  Recalculated component
bounding boxes for faces and hands are respectively colored orange and
light green in the figure.  Since the pixel-level segmentations for
Monica's and Chandler's faces merged together, we observe a one bounding
box rather than two enclosing their faces.

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/friends/flattened_segs_04084.png)
*Fig. 7.  Semantic segmentation results consolidated across three image scales.*

Up to this point, we have worked with segmentation results generated by a
DNN trained to classify object categories rather than object instances.
But images containing two or more human faces often exhibit little or no
separation between them.  So some way is needed to distinguish individual
faces.  We adopt a variation of the approach we developed for localizing
individual words within text-in-the-wild described in our previous post
[9].  In particular, we train a new DNN to semantically segment facial
quadrants.  We start with our O(36K) facial bounding boxes and decompose
them into top-left, top-right, bottom-left and bottom-right quarters.
Ignoring hand content, we generate a new set of augmented 321 x 321 image
tiles that take face bounding box scale into account.  Each pixel in the
tiles' accompanying masks is labeled by one of four classification
categories or background.  After DeepLab ingests these new training data
and finetunes a new model, facial quadrants may be segmented out from
validation and testing images.  Figure 8 displays representative results
from the secondary DNN.

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/quadrant_masks/montage_image_01712___segmented_image_01712.jpg)

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/quadrant_masks/montage_image_00936___segmented_image_00936.jpg)

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/quadrant_masks/montage_image_01750___segmented_image_01750.jpg)

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/quadrant_masks/montage_image_00336___doublesized_segmented_image_00336.jpg)

*Fig. 8.  Facial quadrant segmentation for four representative images.  The
first three images are original-sized, while the fourth is double-sized.*

Face quadrant segmentation is intrinsically more difficult than full face
segmentation.  So it is not surprising to see some classification mistakes
in figure 8.  Nevertheless, quadrant segmentation generally works well
enough to isolate individual faces.  This is true even for faces viewed in
profile, faces nearby by hands, and faces partly cropped by image borders.
Returning to our "Friends" example, we observe in figure 9 that quadrant
segmentation successfully distinguishes the faces of Chandler, Monica,
Rachel and Ross at the original-sized, double-sized and half-sized image
scales.

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/friends/double_full_half_quads.png)
*Fig. 9.  Face quadrant segmentation results at original-sized (upper
right), double-sized (left) and half-sized (lower right) image scales.*

Looping over pixels inside the three candidate facial connected components
of figure 7, we search for face centers inside the quadrant segmentation
masks at the components' image scales.  A small bounding box is dragged
around each component's orange bounding box.  The location of the small box
is not allowed to overlap any previously-found small box marking some face
center.  At each allowed position, the numbers n_green [n_blue] (n_red)
{n_yellow} of pixels colored green [blue] (red) {yellow} within the upper
left [upper right] (lower left) {lower right} of the small box are counted.
Small box locations where n_green, n_blue, n_red and n_yellow are all
nonzero and where n_green + n_blue + n_red + n_yellow is locally maximal
mark individual face centers.  Four such face centers are found for the
"Friends" image (see figure 10).

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/friends/quad_centers_04084_orange.png)
*Fig. 10.  Midpoints of white rectangles mark centers for individual faces.
Orange bounding boxes enclose the original facial connected components as
in figure 7.*

Finally, every facial pixel inside the original connected component's
orange bounding box is reassigned to the closest small box center in a
manner similar to K-means clustering.  Large face connected components
extracted via category segmentation may thus be split into two or more
smaller components by instance segmentation.  We subsequently compute
probability densities for the new components' horizontal and vertical pixel
coordinates.  The components' borders are set equal to these probability
distriubtions 5% and 95% cumulative values.  The final bounding boxes
wrapped around the new face components represent our processing pipeline's
output results.  And the four, full "Friends" faces appearing in figure 11
are successfully localized by this procedure.

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/friends/segmented_image_04084.png)
*Fig. 11.  Final set of localized faces and hands are indicated by orange
and light-green bounding boxes*.

## Face localization results

700 validation images were used to optimize multiple hyperparameters in our
total pipeline including data augmentation threshold fractions, DNN
learning rates, face connected component bounding box size and score
thresholds.  After freezing the hyperparameter values, we then ran our
entire system on an independent set of 700 test images.  Qualitatively
successful examples of facial localization for groups of people are
displayed in figure 12.  Note that faces viewed in profile, partly occluded
by hands and significantly covered by surgical masks are extracted.

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/montage_image_00014___segmented_image_00014.jpg)

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/montage_image_02124___segmented_image_02124.jpg)

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/montage_image_09626___segmented_image_09626.jpg)

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/montage_image_05893___segmented_image_05893.jpg)

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/montage_image_02192___segmented_image_02192.jpg)

*Fig. 12.  Faces and hands localized in images containing groups of people.
Red and yellow tinted pixels are enclosed by orange and light-green
bounding boxes.*

Localizing faces within crowd images such as those in figure 13 is more
challenging.  In these examples, many are obscured by shadows, oriented at
various angles relative to the camera or densely packed in the depth
dimension.  Nevertheless, our system successfully extracts most
individuals' faces in these crowd scenes.

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/montage_image_08754___segmented_image_08754.jpg)

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/montage_image_09561___segmented_image_09561.jpg)

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/montage_image_02269___segmented_image_02269.jpg)

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/montage_image_03836___segmented_image_03836.jpg)

*Fig. 13.  Faces and hands localized in images containing crowds of people.
Red and yellow tinted pixels are enclosed by orange and light-green
bounding boxes.*

Recall that we intentionally incorporated several "2D" sources of faces in
our training set such as images of photos, paintings and line drawings.  We
would like our system to work on such abstract representations of human
faces in addition to conventional images of real people in the real world.
So it is interesting to see face localization results for three "2D" test
images presented in figures 14, 15 and 16.

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/montage_image_10208___segmented_image_10208.jpg)

*Fig. 14.   Face localized in an image of a black-and-white photograph.*

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/montage_image_05206___segmented_image_05206.jpg)

*Fig. 15.   Face localized in an oil painting image.*

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/montage_image_05236___segmented_image_05236.jpg)

*Fig. 16.   Face localized in a line drawing image.*

Of course, our system also makes mistakes.  A number of false positive
detections slip through.  They include human hands, non-human face drawings
and animal visages being incorrectly classified as human faces (see figure
17).  The system also suffers from false negatives wherein faces are partly
or entirely missed (see figure 18).

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/bad_results/false_positives.png)

*Fig. 17.   Human face false alarms.*

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/bad_results/false_negatives.png)

*Fig. 18.   Examples of partially or completely missed faces.*

Our system's ability to find faces is a strong function of their pixel
sizes within an input image.  In turn, the distribution of pixel sizes
among our entire set of 36K+ face bounding boxes is highly non-uniform and
skews heavily towards small rectangles.  Working with the bounding box
percentiles in Table 1, we report recall fractions for the 700 images in
our test set in Table 2.  Recall is given by the fractional coverage of
ground truth bounding boxes by pixels classified as face within every test
image.  As Table 2 clearly illustrates, semantic segmentation of faces
fails if their sizes are significantly smaller than the DNN's O(100 pixel)
receptive field.  Performing inference on double-sized versions of each
test image certainly improves system recall.  But even after image
doubling, our system cannot find a human face if its width is less than 18
pixels.  Averaging over all bounding box sizes, we find total recall =
0.686 for the test set of 700 images.

<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: center">Lower bounding box</th>
      <th style="text-align: center">Upper bounding box</th>
      <th style="text-align: center">Number of</th>
      <th style="text-align: center">Recall</th>
    </tr>
    <tr>
      <th style="text-align: center">pixel width</th>
      <th style="text-align: center">pixel width</th>
      <th style="text-align: center">bounding boxes</th>
      <th style="text-align: center">fraction</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">0</td>
      <td style="text-align: center">7.7</td>
      <td style="text-align: center">140</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center">7.7</td>
      <td style="text-align: center">12.9</td>
      <td style="text-align: center">213</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center">12.9</td>
      <td style="text-align: center">18.7</td>
      <td style="text-align: center">204</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center">18.7</td>
      <td style="text-align: center">24.9</td>
      <td style="text-align: center">227</td>
      <td style="text-align: center">0.602</td>
    </tr>
    <tr>
      <td style="text-align: center">24.9</td>
      <td style="text-align: center">31.2</td>
      <td style="text-align: center">204</td>
      <td style="text-align: center">0.688</td>
    </tr>
    <tr>
      <td style="text-align: center">31.2</td>
      <td style="text-align: center">39.2</td>
      <td style="text-align: center">207</td>
      <td style="text-align: center">0.786</td>
    </tr>
    <tr>
      <td style="text-align: center">39.2</td>
      <td style="text-align: center">50.8</td>
      <td style="text-align: center">214</td>
      <td style="text-align: center">0.804</td>
    </tr>
    <tr>
      <td style="text-align: center">50.8</td>
      <td style="text-align: center">67.9</td>
      <td style="text-align: center">215</td>
      <td style="text-align: center">0.832</td>
    </tr>
    <tr>
      <td style="text-align: center">67.9</td>
      <td style="text-align: center">100.3</td>
      <td style="text-align: center">191</td>
      <td style="text-align: center">0.809</td>
    </tr>
    <tr>
      <td style="text-align: center">100.3</td>
      <td style="text-align: center">Infinity</td>
      <td style="text-align: center">336</td>
      <td style="text-align: center">0.807</td>
    </tr>
  </tbody>
</table>


<!---

| Lower bbox    | Upper bbox    | Number of     | Recall    |
| pixel width   | pixel width   | bboxes	| fraction  |
|:-------------:|:-------------:|:-------------:|:---------:|
|  0 	        |  7.7  	|  140		| 0         |
|  7.7	        | 12.9  	|  213		| 0	    |
|  12.9	        | 18.7  	|  204		| 0         |
|  18.7 	| 24.9  	|  227		| 0.602     |
|  24.9 	| 31.2  	|  204		| 0.688     |
|  31.2 	| 39.2  	|  207		| 0.786     |
|  39.2 	| 50.8	        |  214		| 0.804     |
|  50.8 	| 67.9  	|  215		| 0.832     |
|  67.9	        | 100.3  	|  191		| 0.809     |
|  100.3	| Infinity      |  336		| 0.807     |

-->

*Table 2.  Human face recall for 700 test images as a function of bounding
box pixel width.*

We also compute a precision value for each test image from the ratio of
pixels classified as face which lie inside ground truth bounding boxes to
the total number of pixels classified as face.  When averaged over 700 test
images, we obtain pixel precision = 0.835.  Finally, we combine the
pixel-level recall and precision measures by taking their harmonic mean to
obtain an overall balanced F-score = 0.753.

It is instructive to compare these recall and precision values derived from
an initial DNN which segmented both face and hand pixels with corresponding
metrics derived from a DNN that was trained only using face bounding boxes
and which classifies only face pixels.  Although localizing hands within
arbitrary internet imagery is interesting in its own right, our primary
motivation for annotating hands within the training imagery was to minimize
the DNN's tendency to erroneously classify them as faces.  When hand
content is ignored, we find recall = 0.723, precision = 0.774 and F-score =
0.747.  So face localization precision is indeed improved by including hand
labels within the training set, though recall is diminished.

Pixel-level precision and recall are useful metrics for quantifying a
classification system's performance.  But they can hide deficiencies in
object localization such as those depicted in figure 19.  For example, one
bounding box may overlap more than one face.  Alternatively, one face may
be overlapped by more than one bounding box.  So it is useful to compare
manually labeled ground-truth bounding boxes with automatically detected
bounding boxes in our testing set in order to assess our pipeline face
localization capabilities at an object level.

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/bad_results/wrong_bboxes.png)

*Fig. 19.  (LHS) One bounding box incorrectly encloses two faces. (RHS) Three
bounding boxes incorrectly overlap one face.*

For every ground-truth bounding box within each test image, we search for a
detection box counterpart whose intersection-over-union ratio is maximal.
Similarly, we identify a ground-truth counterpart for every detection
bounding box with maximal intersection-over-union value.  Frequency
histograms for these two distributions are plotted in figure 20.  The large
spikes in these plots' zero-value bins indicates that our system generates
a sizable number of false negatives and false positives.  But the
distributions' median 0.710 and 0.711 values for ground-truth and detected
face bounding box IoU quantifies our system's face localization performance
on a challenging set of internet imagery.

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/IoUs.png)

*Fig. 20.  (LHS) Intersection-over-union distribution for 2156 ground-truth
face bounding boxes manually labeled in 700 testing images. (RHS)
Intersection-over-union distribution for 1632 face bounding boxes
automatically detected in 700 testing images.*

## References

1.  See "Detecting Faces via Deep Semantic Segmentation of Images" post
dated Jan 20, 2016.

2.  S. Yang, P. Luo, C.C. Loy and X. Tang, "WIDER FACE: A Face Detection
Benchmark", arXiv: 1511.06523v1 (2015).

3.  D. King, dlib C++ library, http://dlib.net/ml.html.

4.  L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy and A.L. Yuille,
*Semantic Image Segmentation with Deep Convolutional Nets and Fully
Connected CRFs*, arXiv:1412.7062 (2015).

5.  G. Papandreou, L.-C. Chen, K. Murphy and A. L. Yuille, Weakly and
semi-supervised learning of a DCNN for semantic image segmentation,
http://arxiv.org/1502.02734, (2015).

6.  See https://bitbucket.org/deeplab/deeplab-public/

7.  See http://caffe.berkeleyvision.orig.

8.  M. Jaderberg, K. Simonyan, A. Vedaldi and A. Zisserman, *Synthetic Data
and Artifical Neural Networks for Natural Scene Text Recognition*, arXiv:
1406.2227v4 (2014).

9.  See "Localizing Text-in-the-Wild via Synthetic Phrase Generation and
Semantic Segmentation" post dated Apr 25, 2016.
