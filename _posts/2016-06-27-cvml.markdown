---
layout: post
title:  "Localizing Faces via Image Scaling, Data Augmentation and Instance Segmentation"
date:   2016-06-27 12:00:0
categories: Deep Learning
---

## Faces and hands training data

In an earlier blog post, we presented a first attempt at face detection via
deep semantic segmentation [1].  The initial results based upon 3K+
training images were qualitatively encouraging.  But we noted a number of
false negative and false positive results generated by our trained network.
Moreover, two or more faces appearing close together in an image were
sometimes merged together within a single segmentation region.  And we did
not previously quantify the classifier's performance.  In this blog post,
we return to the face detection problem and focus upon rectifying these
earlier shortcomings.

We start by increasing our training set size.  This time we work with 11491
internet images which were intentionally chosen to exhibit variety along
multiple dimensions.  A random subset of 700 images was reserved for
validation, and another random subset of 700 images was held out for
testing.  The remaining 10K+ images were used for training.

8671 images contain a total of 36196 bounding boxes wrapped around human
faces.  As figure X illustrates, the annotated faces belong to people with
varying attributes (e.g. gender, age, ethnicity) performing different
activities (e.g. eating, running, protesting) in disparate settings
(e.g. restaurants, farms, business meetings).  Faces occur in different
poses (e.g. straight towards camera, profile, downwards), and several are
partially occluded by objects (e.g. eyeglasses, masks, cigarettes).  As
figure X tabulates, the face bounding boxes span a range of pixel widths.
Moreover, some of our face data intentionally come from "2D" sources such
as photographs, paintings and drawings.

![FaceLocalization]({{site.url}}/blog/images/face_localization/training_images/training_montage.png)
*Fig. X. Representative examples of annotated training images.  Human faces
and hands are marked by red and cyan bounding boxes.*


<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: center">Bounding box</th>
      <th style="text-align: center">Lower bounding box</th>
      <th style="text-align: center">Upper bounding box</th>
    </tr>
    <tr>
      <th style="text-align: center">percentage</th>
      <th style="text-align: center">pixel width</th>
      <th style="text-align: center">pixel width</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">0-10</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">7.7</td>
    </tr>
    <tr>
      <td style="text-align: center">10-20</td>
      <td style="text-align: center">7.7</td>
      <td style="text-align: center">12.9</td>
    </tr>
    <tr>
      <td style="text-align: center">20-30</td>
      <td style="text-align: center">12.9</td>
      <td style="text-align: center">18.7</td>
    </tr>
    <tr>
      <td style="text-align: center">30-40</td>
      <td style="text-align: center">18.7</td>
      <td style="text-align: center">24.9</td>
    </tr>
    <tr>
      <td style="text-align: center">40-50</td>
      <td style="text-align: center">24.9</td>
      <td style="text-align: center">31.2</td>
    </tr>
    <tr>
      <td style="text-align: center">50-60</td>
      <td style="text-align: center">31.2</td>
      <td style="text-align: center">39.2</td>
    </tr>
    <tr>
      <td style="text-align: center">60-70</td>
      <td style="text-align: center">39.2</td>
      <td style="text-align: center">50.8</td>
    </tr>
    <tr>
      <td style="text-align: center">70-80</td>
      <td style="text-align: center">50.8</td>
      <td style="text-align: center">67.9</td>
    </tr>
    <tr>
      <td style="text-align: center">80-90</td>
      <td style="text-align: center">67.9</td>
      <td style="text-align: center">100.3</td>
    </tr>
    <tr>
      <td style="text-align: center">90-100</td>
      <td style="text-align: center">100.3</td>
      <td style="text-align: center">Infinity</td>
    </tr>
  </tbody>
</table>

<!---

|  Bbox   	| Lower bbox    | Upper bbox    |
|  percentage  	| pixel width   | pixel width   | 
|:-------------:|:-------------:|:-------------:|
|   0-10	|  0 	        |  7.7  	|
|   10-20	|  7.7	        | 12.9  	|
|   20-30 	|  12.9	        | 18.7  	| 
|   30-40 	|  18.7 	| 24.9  	|
|   40-50	|  24.9 	| 31.2  	| 
|   50-60	|  31.2 	| 39.2  	| 
|   60-70	|  39.2 	| 50.8	        | 
|   70-80	|  50.8 	| 67.9  	| 
|   80-90	|  67.9	        | 100.3  	| 
|   90-100	|  100.3	| Infinity      | 

-->


*Fig. X.  Pixel width ranges for 36K+ face bounding boxes listed as
functions of cumulative bounding box percentages.*

As we generated facial bounding boxes using Davis King's image labeling
tool [ref], we observed that humans frequently put their hands on their
faces (see figure XX).  Indeed, XX percent of our 36K+ face bounding boxes
are overlapped by bare hand content.  Semantic segmentation networks
trained with such "contaminated" face data often mistakenly classify hands
as faces.  So we decided to label hands as well as faces in our imagery
corpus.  But we only annotated bare hands which share similar skin tones to
faces and ignored hands covered by opaque gloves.  7259 of our 11491 images
contain bare hand content.

Since hands are inherently more deformable than faces, their appearance
within images is significantly more varied.  As the image labeling tool
only enables objects to be annotated by rectangles rather than arbitrary
polygons, we sometimes used more than one bounding box to trace different
parts of a hand with higher fidelity than a single rectangle could achieve.
In contrast, we always labeled a single face with a single bounding box.

![FaceLocalization]({{site.url}}/blog/images/face_localization/training_images/hands_on_faces.png)
*Fig. X. Examples of annotated images with overlapping face and hand
bounding boxes.  A single hand may be enclosed by more than one box.*

Among our training, validation and testing images, O(2K) have neither human
faces nor hands.  Figure XX illustrates a few of these negative examples.
Some contain no human content.  Others display body parts such as backs of
heads, torsos and legs whose shapes or colorings might be confused with
faces or hands by a machine.  We also intentionally included several
pictures of animal faces into our imagery corpus which a classifier should
learn to distinguish from human faces.  Moreover, we made sure to
incorporate photos, paintings and line drawings of non-human faces so that
a classifier would not always fire on such 2D representations.

![FaceLocalization]({{site.url}}/blog/images/face_localization/training_images/negative_examples.png)
*Fig. X. Negative examples containing neither human face nor hand content.*

As in our earlier face detection post, we work with the DeepLab framework
built on top of Caffe [7 - 9] in order to perform semantic image
segmentation.  For Deep Neural Network (DNN) training, we decompose the
annotated training images into square tiles with 321x321 pixel dimensions.
For each tile, a mask written to an 8-bit greyscale PNG file is created
whose value = 0, 1 or 2 for background, face and hand pixels.  Exported
image tiles become inputs to Deeplab's DNN, while the masks become inputs
to Deeplab's loss function.

In our first attempt at face detection via semantic segmentation, we
generated DNN training data using images and bounding box annotations at
their original pixel sizes.  But human faces occur within internet imagery
at a wide variety of scales.  So this time, we consider an input image
along with its half-sized and double-sized versions.  The choice of scale
for a training image depends upon the median pixel width of all its
bounding boxes.  If the median width is less than 27 pixels, we replace the
original image by its double-sized version and multiply all its bounding
box coordinates by 2.  On the other hand, we work with the half-sized image
and divide all bounding box coordinates by 2 if the median pixel width is
greater than 54.  Face content used to train the DNN is thereby rendered
closer in scale to the classifier's O(100 pixel) receptive field size.

In this second investigation of face segmentation, we also oversample every
training image in order to augment the total volume of input data.  Each
annotated bounding box becomes a seed for a square tile.  If theA face
[hand] bounding box is allowed to seed up to 8 [3] tiles, and a maximum of
20 tiles are randomly generated per image.  If the bounding box has
dimensions less than 321x321, the tile is randomly positioned so as to
include the entire rectangle.  Otherwise, the tile contains some random
part of the bounding box contents.

We perform further data augmentation by modulating the image tiles' color
contents.  Each pixel within some percentage of tiles has its hue,
saturation and value color coordinates perturbed by small, constant
offsets.  Furthermore, gaussian noise is added to a random subset of tiles.
50% of all tiles are also horizontally flipped.  Figures XX and XX present
one representative training image and 18 of its modulated tiles.  By seeing
many such variations, the DNN hopefully grows insensitive to precise image
positioning, coloring and noise levels when learning to classify faces and
hands.

![FaceLocalization]({{site.url}}/blog/images/face_localization/data_augmentation/image_00692.jpg)
*Fig. X. A representative training image containing multiple face and hand
bounding boxes.*

![FaceLocalization]({{site.url}}/blog/images/face_localization/data_augmentation/output_6x3_tile_montage.jpg)
*Fig. X. Examples of training tiles extracted around face and hand boxes in
figure X.  Note variations in the image chips' translations, colorings and
horizontal parities.*

A total of 147984 image tile plus mask pairs were generated for DNN
training.  Approximately 84% of the 321x321 tiles contained face and/or
hand content, while the remaining 16% corresponded to negative training
data.  All of the tile-mask pairs were used to finetune a DeepLab variant
of the VGG-16 network [refs].  We set the batch size = 30, base learning
rate = 0.000125, momentum = 0.9 and weight decay = 0.0005.  DeepLab's loss
function and pixel-level validation curves appeared to asymptote after the
network was trained via a conventional SGD solver for 5+ epochs.

## Facial instance segmentation

Once the DNN is trained, we can apply it to the 700+700 images in our
validation and testing sets.  Each new image is decomposed into 1500x1500
tiles, and adjacent tiles overlap their neighbors by 100 pixels.  After the
DNN classifies every pixel within each tile, the tiles are recombined to
form a single mask whose width and height dimensions precisely match those
of the original image.  Since segmentation labels located 100 pixels from a
tile border can be ignored, deleterious tile edge effects upon segmentation
are minimized.

Just as taking scale variation of human faces into account is important for
training, so too is it necessary for inference.  We consequently pass each
new image along with its half-sized and double-sized versions through the
trained network.  For example, figure XX displays one representative image
from the 1990's "Friends" sitcom.  Face and hand pixels segmented by the
trained DNN are tinted red and yellow in figure XX at the 3 different image
scales.  For each pixel labeled as face or hand, the classifier returns a
confidence score between 0 to 1 which are illustrated in figure XX as a
heat map.

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/friends/image_04084.jpg)
*Fig. X. A "Friends" image in the validation set.*

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/friends/double_full_half_ccs.png)
*Fig. X.  Pixel-level semantic segmentation of faces and hands in
the test image (upper right) and its double-sized (left) and
half-sized (lower right) versions.  Colored bounding boxes surround
connected components within these segmentation masks.*

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/friends/double_full_half_scores.png)
*Fig. X.  Pixel-level scores for semantic segmentation results of figure X.
Warmer colors indicate higher confidences than cooler colors.*

We next form connected components from segmented face and hand pixels at
each image scale.  Bounding boxes enclosing the connected components appear
as colored rectangles in figure XX.  If a connected component is relatively
small compared to the DNN's receptive field in the original-sized or
half-sized images and if it corresponds to genuine signal, we expect to
find a larger counterpart within the double-sized or original-sized images.
Similarly if a connected component is relatively large in the
original-sized or double-sized images, a smaller counterpart should exist
in the half-sized or original-sized images.  We consequently ignore
connected components at particular image scale which are too small or too
large.

Surviving connected components are assigned confidence values corresponding
to the "hottest" 20th-percentile among its individual pixel scores.  Since
we are primarily interested in face detection, we set a higher threshold
score for hand components than for face components.  Moreover, we expect
object detection to be easiest to perform within half-sized images and
hardest for double-sized images.  So our thresholds are more stringent
[lenient] for the former [latter].  Connected components whose confidence
values do not meet these score thresholds are rejected.  Bounding boxes
for components rejected either by size or score considerations are colored
black in figure XX.

We next consolidate all the connected components across the 3 image pyramid
scales.  Components found in double-sized [half-sized] images are
downsampled [upsampled] to original-sized image dimensions.  After such
resampling, we search for connected components originating from different
scales which overlap in area by more than 25%.  The component with the
higher confidence value relative to its score threshold is retained, while
the other component is rejected.  

The surviving connected components after image scale consolidation for our
"Friends" example are presented in figure XX.  Recalculated component
bounding boxes for faces and hands appear as orange and light green
rectangles in the figure.  Since the pixel-level segmentations for Monica's
and Chandler's faces merged together, we observe a one bounding box
enclosing their two faces rather than two boxes around two faces.

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/friends/flattened_segs_04084.png)
*Fig. X.  Semantic segmentation results consolidated across 3 image scales.*

Up to this point, we have worked with segmentation results generated by a
DNN trained to classify object categories rather than object instances.
But images containing two or more human faces often exhibit little or no
separation between them.  So some way is needed to distinguish individual
faces.  We adopt a variation of the approach we developed for localizing
individual words within text-in-the-wild described in our previous post
[ref].  In particular, we train a new DNN to semantically segment facial
quadrants.  We start with our O(36K) facial bounding boxes and decompose
them into top-left, top-right, bottom-left and bottom-right quarters.
Ignoring hand content, we generate a new set of augmented 321 x 321 image
tiles that take face bounding box scale into account.  Each pixel in the
tiles' accompanying masks is labeled by one of four classification
categories or background.  After DeepLab ingests these new training data
and finetunes a new model, facial quadrants may be segmented out from
validation and testing images.  Figure XX displays representative results
from the secondary DNN.

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/quadrant_masks/montage_image_01712___segmented_image_01712.jpg)

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/quadrant_masks/montage_image_00936___segmented_image_00936.jpg)

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/quadrant_masks/montage_image_01750___segmented_image_01750.jpg)

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/quadrant_masks/montage_image_00336___doublesized_segmented_image_00336.jpg)

*Fig. X.  Facial quadrant segmentation for four representative images.  The
first three images are original-sized, while the fourth is double-sized.*

Face quadrant segmentation is intrinsically more difficult than full face
segmentation.  So it is not surprising to see some classification mistakes
in figure XX.  Nevertheless, quadrant segmentation generally works well
enough to isolate individual faces.  This is true even for faces viewed in
profile, faces nearby by hands, and faces partly cropped by image borders.
Returning to our "Friends" example, we observe in figure XX that quadrant
segmentation successfully distinguishes the faces of Chandler, Monica,
Rachel and Ross at the original-sized, double-sized and half-sized image
scales.

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/friends/double_full_half_quads.png)
*Fig. X.  Face quadrant segmentation results at original-sized (upper
right), double-sized (left) and half-sized (lower right) image scales.*

Looping over pixels inside the three candidate facial connected components
of figure XX, we search for face centers inside the quadrant segmentation
masks at the components' image scales.  A small bounding box is dragged
around each component's orange bounding box.  The location of the small box
is not allowed to overlap any previously-found small box marking some face
center.  At each allowed position, the numbers n_green [n_blue] (n_red)
{n_yellow} of pixels colored green [blue] (red) {yellow} within the upper
left [upper right] (lower left) {lower right} of the small box are counted.
Small box locations where n_green, n_blue, n_red and n_yellow are all
nonzero and where n_green + n_blue + n_red + n_yellow is locally maximal
mark individual face centers.  Four such face centers are found for the
"Friends" image (see figure XX).

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/friends/quad_centers_04084_orange.png)
*Fig. X.  Midpoints of white rectangles mark centers for individual faces.
Orange bounding boxes enclose the original facial connected components as in figure XX.*

Finally, every facial pixel inside the original connected component's
orange bounding box is reassigned to the closest small box center in a
manner similar to K-means clustering.  Large face connected components
extracted via category segmentation may thus be split into two or more
smaller components by instance segmentation.  We subsequently compute
probability densities for the new components' horizontal and vertical pixel
coordinates.  The components' borders are set equal to these probability
distriubtions 5% and 95% cumulative values.  The final bounding boxes
wrapped around the new face components represent our processing pipeline's
output results.  And the four, full "Friends" faces appearing in figure XX
are successfully localized by this procedure.

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/friends/segmented_image_04084.png)
*Fig. X.  Final set of localized faces and hands are indicated by orange
and green bounding boxes*.

## Face localization performance results


![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/montage_image_00014___segmented_image_00014.jpg)

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/montage_image_02124___segmented_image_02124.jpg)

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/montage_image_09626___segmented_image_09626.jpg)

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/montage_image_05893___segmented_image_05893.jpg)

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/montage_image_02192___segmented_image_02192.jpg)


*Fig. X.   Faces and hands localized in images containing groups of people.*

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/montage_image_08754___segmented_image_08754.jpg)

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/montage_image_09561___segmented_image_09561.jpg)

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/montage_image_02269___segmented_image_02269.jpg)

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/montage_image_03836___segmented_image_03836.jpg)

*Fig. X.   Faces and hands localized in images containing crowds of people.*

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/montage_image_10208___segmented_image_10208.jpg)

*Fig. X.   Face localized in an image of a black-and-white photograph.*

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/montage_image_05206___segmented_image_05206.jpg)

*Fig. X.   Face localized in an oil painting image.*

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/montage_image_05236___segmented_image_05236.jpg)

*Fig. X.   Face localized in a line drawing image.*


![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/bad_results/false_negatives.png)

*Fig. X.   Examples of partially or completely missed faces.*

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/bad_results/false_positives.png)

*Fig. X.   Human face false alarms.*

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/bad_results/wrong_bboxes.png)

*Fig. X.  LHS: One bounding box incorrectly encloses two faces. (RHS) Three
bounding boxes incorrectly overlap one face.*






## References

1.  See "Detecting Faces via Deep Semantic Segmentation of Images" post
dated Jan 20, 2016.

1.  M. Jaderberg, K. Simonyan, A. Vedaldi and A. Zisserman, *Synthetic Data
and Artifical Neural Networks for Natural Scene Text Recognition*, arXiv:
1406.2227v4 (2014).

2.  See www.imagemagick.org/Usage/text/ .

3.  L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy and A.L. Yuille,
*Semantic Image Segmentation with Deep Convolutional Nets and Fully
Connected CRFs*, arXiv:1412.7062 (2015).


