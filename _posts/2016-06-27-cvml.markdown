---
layout: post
title:  "Localizing Faces via Image Scaling, Data Augmentation and Instance Segmentation"
date:   2016-06-27 12:00:0
categories: Deep Learning
---

## Faces and hands training data

In an earlier blog post, we presented a first attempt at face detection via
deep semantic segmentation [1].  The initial results based upon 3K+
training images were qualitatively encouraging.  But we noted a number of
false negative and false positive results generated by our trained network.
Moreover, two or more faces appearing close together in an image were
sometimes merged together within a single segmentation region.  And we did
not previously quantify the classifier's performance.  In this blog post,
we return to the face detection problem and focus upon rectifying these
earlier shortcomings.

We start by increasing our training set size.  This time we work with 11491
internet images which were intentionally chosen to exhibit variety along
multiple dimensions.  A random subset of 700 images was reserved for
validation, and another random subset of 700 images was held out for
testing.  The remaining 10K+ images were used for training.

8671 images contain a total of 36196 bounding boxes wrapped around human
faces.  As figure X illustrates, the annotated faces belong to people with
varying attributes (e.g. gender, age, ethnicity) performing different
activities (e.g. eating, running, protesting) in disparate settings
(e.g. restaurants, farms, business meetings).  Faces occur in different
poses (e.g. straight towards camera, profile, downwards), and several are
partially occluded by objects (e.g. eyeglasses, masks, cigarettes).  As
figure X tabulates, the face bounding boxes span a range of pixel widths.
Moreover, some of our face data intentionally come from "2D" sources such
as photographs, paintings and drawings.

![FaceLocalization]({{site.url}}/blog/images/face_localization/training_images/training_montage.png)
*Fig. X. Representative examples of annotated training images.  Human faces
and hands are marked by red and cyan bounding boxes.*


|  Bbox   	| Lower bbox   	| Upper bbox   	|
|  percentage  	| pixel width   | pixel width  	| 
|:-------------:|:-------------:|:-------------:|
|   0-10	|  0 		| 7.7  		|
|   10-20	|  7.7	 	| 12.9  	|
|   20-30 	|  12.9		| 18.7  	| 
|   30-40 	|  18.7 	| 24.9  	|
|   40-50	|  24.9 	| 31.2  	| 
|   50-60	|  31.2 	| 39.2  	| 
|   60-70	|  39.2 	| 50.8	  	| 
|   70-80	|  50.8 	| 67.9  	| 
|   80-90	|  67.9		| 100.3  	| 
|   90-100	|  100.3	| Infinity  	| 

*Fig. X.  Pixel width ranges for 36K+ face bounding boxes listed as functions of
cumulative bounding box percentages.*

As we generated facial bounding boxes using Davis King's image labeling
tool [ref], we observed that humans frequently put their hands on their
faces (see figure XX).  Indeed, XX percent of our 36K+ face bounding boxes
are overlapped by bare hand content.  Semantic segmentation networks
trained with such "contaminated" face data often mistakenly classify hands
as faces.  So we decided to label hands as well as faces in our imagery
corpus.  But we only annotated bare hands which share similar skin tones to
faces and ignored hands covered by opaque gloves.  7259 of our 11491 images
contain bare hand content.

Since hands are inherently more deformable than faces, their appearance
within images is significantly more varied.  As the image labeling tool
only enables objects to be annotated by rectangles rather than arbitrary
polygons, we sometimes used more than one bounding box to trace different
parts of a hand with higher fidelity than a single rectangle could achieve.
In contrast, we always labeled a single face with a single bounding box.

![FaceLocalization]({{site.url}}/blog/images/face_localization/training_images/hands_on_faces.png)
*Fig. X. Examples of annotated images with overlapping face and hand
bounding boxes.*

Among our training, validation and testing images, O(2K) have neither human
faces nor hands.  Figure XX illustrates a few of these negative examples.
Some contain no human content.  Others display body parts such as backs of
heads, torsos and legs whose shapes or colorings might be confused with
faces or hands by a machine.  We also intentionally included several
pictures of animal faces into our imagery corpus which a classifier should
learn to distinguish from human faces.  Moreover, we made sure to
incorporate photos, paintings and line drawings of non-human faces so that
a classifier would not always fire on such 2D representations.

![FaceLocalization]({{site.url}}/blog/images/face_localization/training_images/negative_examples.png)
*Fig. X. Negative examples containing neither human face nor hand content.*

As in our earlier face detection post, we work with the DeepLab framework
built on top of Caffe [7 - 9] in order to perform semantic image
segmentation.  For Deep Neural Network (DNN) training, we decompose the
annotated training images into square tiles with 321x321 pixel dimensions.
For each tile, a mask written to an 8-bit greyscale PNG file is created
whose value = 0, 1 or 2 for background, face and hand pixels.  Exported
image tiles become inputs to Deeplab's DNN, while the masks become inputs
to Deeplab's loss function.

In our first attempt at face detection via semantic segmentation, we
generated DNN training data using images and bounding box annotations at
their original pixel sizes.  But human faces occur within internet imagery
at a wide variety of scales.  So this time, we consider an input image
along with its half-sized and double-sized versions.  The choice of scale
for a training image depends upon the median pixel width of all its
bounding boxes.  If the median width is less than 27 pixels, we replace the
original image by its double-sized version and multiply all its bounding
box coordinates by 2.  On the other hand, we work with the half-sized image
and divide all bounding box coordinates by 2 if the median pixel width is
greater than 54.  Face content used to train the DNN is thereby rendered
closer in scale to the classifier's O(100 pixel) receptive field size.

In this second investigation of face segmentation, we also oversample every
training image in order to augment the total volume of input data.  Each
annotated bounding box becomes a seed for a square tile.  If theA face
[hand] bounding box is allowed to seed up to 8 [3] tiles, and a maximum of
20 tiles are randomly generated per image.  If the bounding box has
dimensions less than 321x321, the tile is randomly positioned so as to
include the entire rectangle.  Otherwise, the tile contains some random
part of the bounding box contents.

We perform further data augmentation by modulating the image tiles' color
contents.  Each pixel within some percentage of tiles has its hue,
saturation and value color coordinates perturbed by small, constant
offsets.  Furthermore, gaussian noise is added to a random subset of tiles.
50% of all tiles are also horizontally flipped.  Figures XX and XX present
one representative training image and 18 of its modulated tiles.  By seeing
many such variations, the DNN hopefully grows insensitive to precise image
positioning, coloring and noise levels when learning to classify faces and
hands.

![FaceLocalization]({{site.url}}/blog/images/face_localization/data_augmentation/image_00692.jpg)
*Fig. X. A representative training image containing multiple face and hand
bounding boxes.*

![FaceLocalization]({{site.url}}/blog/images/face_localization/data_augmentation/output_6x3_tile_montage.jpg)
*Fig. X. Examples of training tiles extracted around face and hand boxes in
figure X.  Note variations in the image chips' translations, colorings and
horizontal parities.*

A total of 147984 image tile plus mask pairs were generated for DNN
training.  Approximately 84% of the 321x321 tiles contained face and/or
hand content, while the remaining 16% corresponded to negative training
data.  All of the tile-mask pairs were used to finetune a DeepLab variant
of the VGG-16 network [refs].  We set the batch size = 30, base learning
rate = 0.000125, momentum = 0.9 and weight decay = 0.0005.  DeepLab's loss
function and pixel-level validation curves appeared to asymptote after the
network was trained via a conventional SGD solver for 5+ epochs.

## Face instance segmentation

Once the DNN is trained, we can apply it to the 700+700 images in our
validation and testing sets.  Each new image is decomposed into 1500x1500
tiles, and adjacent tiles overlap their neighbors by 100 pixels.  After the
DNN classifies every pixel within each tile, the tiles are recombined to
form a single mask whose width and height dimensions precisely match those
of the original image.  Since segmentation labels located 100 pixels from a
tile border can be ignored, deleterious tile edge effects upon segmentation
are minimized.

Just as taking scale variation of human faces into account is important for
training, so too is it important for inference.  We consequently pass each
new image along with its half-sized and double-sized versions through the
trained network.  For example, figure XX displays one representative image
from the 1990's "Friends" sitcom.  Segmented face and hand pixels are
tinted red and yellow in figure XX at the 3 different image scales.
Connected components representing candidate face and hand regions in the
original, half and double sized images are subsequently formed.  Colored
rectangles in the figure depict bounding boxes enclosing the connected
components at each image pyramid scale.

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/friends/image_04084.jpg)
*Fig. X. A "Friends" image in the validation set.*

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/friends/double_full_half_ccs.png)
*Fig. X.  Pixel-level semantic segmentation of faces and hands in
the test image (upper right) and its double-sized (left) and
half-sized (lower right) versions.  Colored bounding boxes surround
connected components within these segmentation masks.*

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/friends/double_full_half_scores.png)
*Fig. X.  Pixel-level scores for semantic segmentation results of figure X.
Warmer colors indicate higher confidences than cooler colors.*

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/friends/flattened_segs_04084.png)
*Fig. X.  Semantic segmentation results consolidated across 3 image scales.*


![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/friends/double_full_half_quads.png)
*Fig. X.  Face quadrant segmentation results at original-sized (upper
right), double-sized (left) and half-sized (lower right) image scales.*

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/friends/quad_centers_04084_orange.png)
*Fig. X.  Midpoints of white rectangles mark centers for individual faces.
Orange bounding boxes enclose the original facial connected components as in figure XX.*


![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/friends/segmented_image_04084.png)
*Fig. X.  Final set of localized faces and hands are indicated by orange
and green bounding boxes*.

## Face localization performance results

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/quadrant_masks/montage_image_01712___segmented_image_01712.jpg)

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/quadrant_masks/montage_image_00936___segmented_image_00936.jpg)

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/quadrant_masks/montage_image_01750___segmented_image_01750.jpg)

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/quadrant_masks/montage_image_00336___doublesized_segmented_image_00336.jpg)

*Fig. X.  Face quadrant segmentations for 4 representative images*.



![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/montage_image_00014___segmented_image_00014.jpg)

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/montage_image_02124___segmented_image_02124.jpg)

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/montage_image_09626___segmented_image_09626.jpg)

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/montage_image_05893___segmented_image_05893.jpg)

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/montage_image_02192___segmented_image_02192.jpg)


*Fig. X.   Faces and hands localized in images containing groups of people.*

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/montage_image_08754___segmented_image_08754.jpg)

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/montage_image_09561___segmented_image_09561.jpg)

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/montage_image_02269___segmented_image_02269.jpg)

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/montage_image_03836___segmented_image_03836.jpg)

*Fig. X.   Faces and hands localized in images containing crowds of people.*

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/montage_image_10208___segmented_image_10208.jpg)

*Fig. X.   Face localized in an image of a black-and-white photograph.*

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/montage_image_05206___segmented_image_05206.jpg)

*Fig. X.   Face localized in an oil painting image.*

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/good_results/montage_image_05236___segmented_image_05236.jpg)

*Fig. X.   Face localized in a line drawing image.*


![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/bad_results/false_negatives.png)

*Fig. X.   Examples of partially or completely missed faces.*

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/bad_results/false_positives.png)

*Fig. X.   Human face false alarms.*

![FaceLocalization]({{site.url}}/blog/images/face_localization/testing_images/bad_results/wrong_bboxes.png)

*Fig. X.  LHS: One bounding box incorrectly encloses two faces. (RHS) Three
bounding boxes incorrectly overlap one face.*






## References

1.  See "Detecting Faces via Deep Semantic Segmentation of Images" post
dated Jan 20, 2016.

1.  M. Jaderberg, K. Simonyan, A. Vedaldi and A. Zisserman, *Synthetic Data
and Artifical Neural Networks for Natural Scene Text Recognition*, arXiv:
1406.2227v4 (2014).

2.  See www.imagemagick.org/Usage/text/ .

3.  L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy and A.L. Yuille,
*Semantic Image Segmentation with Deep Convolutional Nets and Fully
Connected CRFs*, arXiv:1412.7062 (2015).


