---
layout: post
title:  "Classifying Gender via a Convolutional Neural Network for Faces"
date:   2016-08-09 12:00:00
categories: Deep Learning
---

## Gender-labeled facial imagery

In our previous blog post, we localized faces within internet photos using
instance segmentation [1].  In this entry, we continue to accumulate and
investigate a sizable corpus of "faces in the wild".  But our primary focus
is now upon classifying rather than detecting faces.

Humans around the world may be described in terms of various
characteristics which change on time scales ranging from seconds
(e.g. mood) to decades (e.g. age).  But for most people, gender labels are
constant throughout their lifetimes.  Moreover, physical differences exist
between male and female faces.  So gender arguably represents one of the
most basic human attributes.  We seek to build a classifer which can
identify gender based upon facial imagery input.

We begin by extending our dataset of internet images from our earlier face
localization studies.  To date, we have collected and annotated 13740
images which intentionally exhibit a high degree of variety.  2815 of the
images originated from the WIDER collection [2], while the remaining
pictures were downloaded primarily from Google Images.  Within this corpus,
10492 photos contain 43752 human faces, and 8671 pictures contain 34936
hands.  Both faces and hands were annotated with bounding boxes via dlib's
IMGLAB tool [3].  The labeled faces belong to people with differing
attributes (e.g. gender, age, ethnicity) performing activities
(e.g. socializing, swimming, singing) within disparate settings
(e.g. hospital rooms, memorial services, sport stadiums).  Moreover,
people's visages are oriented in variable poses (e.g. facing camera,
looking upwards, profile view), and many are partially occluded by objects
(e.g. eyeglasses, drinking cups, other people's heads).  We also
intentionally increased the number of internet images containing faces from
paintings and drawings relative to our previous face investigation.

While dlib's IMGLAB is convenient for drawing bounding boxes around
objects, it does not currently support hierarchical assignment of multiple
labels per box.  So we developed our own gender attribution tool based upon
the 3D OpenSceneGraph toolkit [5].  Our program imports and displays face
bounding boxes generated via IMGLAB. A user may press the "1", "2" or "0"
keys on the number pad to set gender attributes for all bounding boxes in
an image to male, female or unknown.  After a particular box is selected
via the mouse, its gender attribute may be altered to male, female or
unknown by pressing the "1", "2" or "~" keys near the upper left of a
keyboard.  We used this attribution tool to assign gender labels to 40K+
facial bounding boxes.

One example of an image containing multiple gender assignments is displayed
in figure 1.  Purple [cyan] bounding boxes enclose female [male] faces.
Yellow bounding boxes mark persons for which we were significantly
uncertain about their sex.  As we attempted to accurately attribute
thousands of different people appearing in internet images, we were
surprised by how often it is difficult to determine gender from facial and
other appearance cues.  In particular, we found distinguishing baby boys
from baby girls to be nearly impossible if their clothing provided no clear
cultural hints.  In other cases, one can deduce that blurry patches in
images must correspond to human faces.  But such patches may contain too
little information for a gender label to be confidently assigned.  All such
faces marked with "unknown" genders are ignored by our classification
pipeline.

![FaceGender]({{site.url}}/blog/images/face_gender/training/image_03815_crowd_genders_01.png)
*Fig. 1. An internet image attributed with female, male and unknown gender labels.*

The bounding boxes in figure 1 are snuggly wrapped around individual faces.
But information useful for gender determination is contained in hair style,
neckware and upper-body clothing appearance.  We consequently double all
facial bounding box widths and heights.  The resulting human head regions
come in a wide variety of pixel sizes.  But a convolutional neural network
requires uniformly-sized imagery input.  In particular, the CNN that we
develop below takes in 96x96 pixel-sized image chips.  So inputs larger
than 96x96 were subsampled, while inputs smaller than 96x96 were superposed
onto black backgrounds.  Examples of head chips extracted from figure 1 and
spatially resized to 96x96 are presented in figure 2.

![FaceGender]({{site.url}}/blog/images/face_gender/training/training_chips_106x106.png)
*Fig. 2. Three female and three male head chips extracted from figure 1.
The pixel sizes of the chips' non-black regions are doubled compared to
their tightly-cropped facial bounding box progenitors.*

In order to expand the volume of our training and testing data, we searched
online for other sets of gender-labeled face chips.  While many facial
databases can be found on the internet [5], relatively few come with gender
metadata.  One relatively recent dataset may be obtained from the
OUI-Adience Face Image Project from which we extracted 17457 photos [6,7].
The sources for these data are flickr albums.  Adience images are
consequently "in the wild".  We note, however, that flickr users are not
likely to be representative of the global human population.  We also
included 369 head shots of Iranian women available from the Iranian Face
Database [8,9].  After combining all these head shots, we have a total of
54760 image chips labeled as male or female.

We randomly reserve 2.5% of the 54K+ chips for validation and another 7.5%
for testing.  Image augmentation is performed upon the remaining 90% of
chips in order to increase the variety of training samples.  The enlarged
bounding boxes surrounding people's heads are translated and rotated by
small amounts.  Half of all the augmented chips are also horizontally
flipped.  Random constant offsets are added to each pixel's hue, saturation
and value color coordinates.  Gaussian noise is also intentionally injected
into augmented images.  Figure 3 displays facial chips before and after
image augmentation.

![FaceGender]({{site.url}}/blog/images/face_gender/training/augmented_female_face.png)

![FaceGender]({{site.url}}/blog/images/face_gender/training/augmented_male_face.png)

*Fig. 3. Examples of imagery augmentation.  Input female and male training
chips appear in the leftmost column of this figure.  Their output variants
are randomly translated, rotated, color-modulated and flipped relative to
the original images.*

For each of the training chips which originated from our highly-variable
set of faces, we add five perturbed counterparts.  But since training chips
from the Adience and Iran Faces databases exhibit less intrinsic variety,
we only add one augmented variant per chip from these data sources.  After
augmentation, we end up with a total of 260788 training chips which carry
male or female labels.

Our final set of input data comes from images containing no facial content.
Working with our original set of 13.7K+ full-sized internet photos, we
randomly choose the number, sizes and locations of bounding boxes in each
picture.  A random box which overlaps any part of a human head is rejected.
A random box is also heavily biased towards enclosing an image region with
sizable entropy.  We thus generate 99204 non-face image chips, and we
reserve 0.5% [1.5%] for validation [testing].  Figure 4 displays
representative examples of non-face chips.

![FaceGender]({{site.url}}/blog/images/face_gender/training/non_face_training_samples.png)
*Fig. 4.  Image chips extracted from internet photos which contain no human
face content.*

## CNN architecture, hyperparameters and training

We seek to classify input image chips into one of 3 categories: non-face,
male face or female face.  Since the number of output categories is small,
we adopt the hypothesis that a simple convolutional neural network should
be sufficient for this task.  We want to keep the network's overall size
small in order to minimize its overfitting potential and required training
time.  Yet the network must also have sufficient capacity to handle our
data set's heterogeneity.  After experimenting with a number of different
network configurations, we settled upon the one sketched in figure 5.

![FaceGender]({{site.url}}/blog/images/face_gender/training/padded_cropped_net.png)
*Fig. 5. Training architecture for a CNN used to classify facial genders.
This neural net graph was generated by the web-based Netscope visualization
tool [10].*

The input to the network is an RGB image chip of pixel size 96x96.
Following the highly successful VGG-16 model [11], we set the pixel
dimensions of all convolutional filters in figure 5 to 3x3.  The network's
first, second, third and fourth convolutional layers contain 96, 192, 224
and 256 filters.  The fully-connected fifth and sixth layers both contain
256 nodes.  The seventh layer holds 3 nodes corresponding to the non-face,
male and female output categories.  Filtering in layers 1-6 of the network
is followed by a nonlinear Rectified Linear Unit (ReLU) operation.  Max
pooling with stride 2 ??? is performed after the first three convolutions
which reduces image chip pixel width and height each by half.  In the final
layer, softmax probabilities are computed for the 3 classes.  The one with
the highest score yields the classification label output corresponding to
the image chip input.

The network pictured in figure 5 contains a total of 38,886,688 weights
whose values we train from scratch.  In order to combat overfitting this
large number of variables, we intentionally perform dropout after each ReLU
step [12].  Dropout probabilities grow in magnitude with increasing layer
depth as follows: 0.1, 0.25, 0.25, 0.5, 0.5, 0.5, 0.5.  We also employ
L2-regularization to minimize overfitting.

We implement this CNN using the caffe framework [13].  Its weights and
biases are all initialized with random values drawn from a Gaussian
distribution with mean = 0 and standard deviation = 0.01.  We train the
network using stochastic gradient descent with momentum = 0.9 and minibatch
size = 100.  We experimented with adjusting the initial learning rate and
settled upon 0.01.  It decreases by 0.33 after every 30K iterations.  We
also varied the weight decay hyperparameter and eventually chose lambda =
0.001.

Learning curves for our network are plotted as functions of training epoch
in figure 6.  We instruct caffe to hold out 10% of the input training data
in order to monitor the CNN's accuracy.  The cyan curve in the figure
represents a temporally-smoothed accuracy metric for the 90% of image chips
actually used to train the network.  The red curve illustrating the same
accuracy metric for the held-out data asymptotes to a value only slightly
less than the cyan curve.  So dropout plus L2-regularization clearly
prevent the neural network from overfitting.

![FaceGender]({{site.url}}/blog/images/face_gender/training/padded_accuracy_curves.png)
*Fig. 6. CNN model accuracy plotted as a function of training epoch.  The
cyan curve is a temporally-smoothed version of the dark blue accuracy
values for the training data.  The red curve illustrates the CNN's accuracy
on a held-out subset of the training data.*

## Facial gender inference

We settled upon our CNN's architecture and hyperparameters after performing
multiple runs and analyzing its output on a validation set of 1617 image
chips.  Somewhat surprisingly, we found that the CNN had greater difficulty
classifying females than males.  Perhaps the frequent presence of facial
hair makes male faces intrinsicially easier to identify than female faces.
We also discovered that softmax probabilities for correctly labeled
validation chips were generally higher than those for their incorrectly
classified counterparts.

It is unreasonable to expect any automated system to perform perfectly all
the time.  But machines should return confidence values along with their
predictions.  We may then ignore any automated result whose confidence lies
below some threshold.  Using our validation data, we empirically determined
softmax probability thresholds of 0.52 and 0.58 for male and female
classification labels.  If the CNN outputs a confidence score for a gender
label that lies below these thresholds, the result is declared too
uncertain to be trusted.

After fixing the CNN's architecture and hyperparameters, we perform
inference upon a reserved test set of 4536 image chips to assess the
network's performance.  Representative examples of test chips whose facial
genders are correctly classified are presented in figures 7a and 7b.  They
include several challenging instances where faces are significantly
cropped, blurred or occluded.

![FaceGender]({{site.url}}/blog/images/face_gender/results/padded_correct_female_classifications.png)
*Fig. 7a. Examples of test image chips correctly classified by the CNN as
female.*

![FaceGender]({{site.url}}/blog/images/face_gender/results/padded_correct_male_classifications.png)
*Fig. 7b.  Examples of test image chips correctly classified by the CNN as
male.*

It is interesting to note that a number of test chips contain faces
originating from "2D" sources such as oil paintings, pencil drawings and
poster billboards.  In our previous blog entry, we found that instance
segmentation could successfully localize such "flat" faces in internet
images [1].  Figures 8a and 8b demonstrate that a machine can also
successfully classify genders for 2D faces.

![FaceGender]({{site.url}}/blog/images/face_gender/results/female_2D_correct.png)
*Fig. 8a.  Examples of test faces coming from paintings, line drawings and
billboard placards correctly classified as female.*

![FaceGender]({{site.url}}/blog/images/face_gender/results/male_2D_correct.png)
*Fig. 8b.  Examples of test faces coming from paintings, line drawings and
physical photos correctly classified as male.*

If the incoming image quality is sufficiently poor, our system cannot
confidently return any classification result.  For instance, the machine
marks as unknown the facial genders for all chips in figure 9.  Our CNN
also definitely makes classification mistakes.  Representative examples of
incorrectly classified female and male image chips are shown in figures 10a
and 10b.  For several of the chips in these figures, it is difficult for
even a human to determine gender without seeing the context of the full
images from which they were excised.  Moreover, it is often hard to tell
gender for very young and old human subjects as well as for people whose
faces are obscured by masks or markings.  Small pixel sizes and croppings
of head shots further render sex determination difficult.  But in other
failure cases, most humans would have no trouble determining correct gender
assignments.


![FaceGender]({{site.url}}/blog/images/face_gender/results/padded_unsure_classifications.png)
*Fig. 9. Examples of test faces for which the CNN was significantly
uncertain about gender.  The top [bottom] row contains female [male] image
chips.*


![FaceGender]({{site.url}}/blog/images/face_gender/results/incorrect_female_2_male_classifications.png)
*Fig. 10a. Examples of female test image chips which the CNN incorrectly
classified as male.*

![FaceGender]({{site.url}}/blog/images/face_gender/results/incorrect_male_2_female_classifications.png)
*Fig. 10b. Examples of male test image chips which the CNN incorrectly
classified as female.*

The confusion matrix in Table 1 quantifies our system's overall
classification performance.  The CNN has little difficulty separating
non-face from human-face inputs.  It consequently could be used to improve
precision for detectors which localize faces within internet imagery.  The
86% correct classification rate for male faces is also respectable.  It
compares favorably with recent CNN gender results reported for smaller and
less challenging data sets than ours [6].  On the other hand, our CNN
struggles to correctly identify female faces, and it gives up on over 12%
of female test inputs.  By lowering the female softmax threshold below
0.58, we could increase the correctly-classified female fraction to around
80%.  But such a threshold relaxation would come at the unacceptable cost
of also increasing the incorrectly-classified female fraction.

<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: center"> non- </th>
      <th style="text-align: center"> male </th>
      <th style="text-align: center"> female </th>
      <th style="text-align: center"> uncertain </th>
      <th style="text-align: center"> %  </th>
      <th style="text-align: center"> %  </th>
      <th style="text-align: center"> %  </th>
    </tr>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: center"> face </th>
      <th style="text-align: center"> face </th>
      <th style="text-align: center"> face </th>
      <th style="text-align: center"> gender </th>
      <th style="text-align: center"> correct </th>
      <th style="text-align: center"> uncertain </th>
      <th style="text-align: center"> incorrect </th>
    </tr>

  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>non-face</strong></td>
      <td style="text-align: center">1026</td>
      <td style="text-align: center">3</td>
      <td style="text-align: center">8</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">98.94</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1.06</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>male face</strong></td>
      <td style="text-align: center">49</td>
      <td style="text-align: center">1559</td>
      <td style="text-align: center">148</td>
      <td style="text-align: center">56</td>
      <td style="text-align: center">86.04</td>
      <td style="text-align: center">3.09</td>
      <td style="text-align: center">10.87</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>female face</strong></td>
      <td style="text-align: center">23</td>
      <td style="text-align: center">210</td>
      <td style="text-align: center">1246</td>
      <td style="text-align: center">208</td>
      <td style="text-align: center">73.85</td>
      <td style="text-align: center">12.32</td>
      <td style="text-align: center">13.81</td>
    </tr>
  </tbody>
</table>

*Table 1. Classification results for a held-out truth set of 4536 image
chips.*

## Detecting and classifying faces in internet imagery

It is interesting to combine the face localization detector developed in
our previous blog posting with the face gender classifier described here.
We then pass a brand new set of "images in the wild" not previously seen by
either the detector or classifier through the end-to-end system.  Instance
segmentation attempts to wrap bounding boxes around faces of genuine
humans, and the classifier attempts to deduce the detected faces' genders.
Several faces which were both successfully localized and classified are
presented in figure 11.  They include relatively simple examples where
human subjects look straight into the camera.  Moreover, no face is
occluded by another person's head in the figure.

![FaceGender]({{site.url}}/blog/images/face_gender/results/good_all_faces.png)
*Fig. 11. Example test images in which all faces were automatically
localized and assigned correct gender labels.  Red [blue] bounding boxes
enclose female [male] faces.*

Figures 12a and 12b illustrate results for more challenging photos
involving crowd scenes, head coverings that reduce facial visibility,
people located far down-range, and faces significantly oriented away from
the camera direction.  We observe a few localized faces for which the
classifier was uncertain about a person's sex.  But generally for the
pictures in figure 12, the detector has more trouble finding faces than
does the classifier determining their genders.

![FaceGender]({{site.url}}/blog/images/face_gender/results/good_most_faces.png)
*Fig. 12a. Example test images for which no localized face was assigned an
incorrect gender label.  Red [blue] bounding boxes enclose female [male]
faces, while the CNN was uncertain about gender for faces marked by green
bounding boxes.  A few faces in these images were not localized.*

![FaceGender]({{site.url}}/blog/images/face_gender/results/OK_faces.png)
*Fig. 12b. Example test images for which no localized face was assigned an
incorrect gender label.  Several faces in these images were not localized.*

Finally, the examples in figure 13 illustrate failures of both the detector
and classifier.  They include cases where faces were not localized as well
as incorrect gender labels for detected faces.  We leave improving both
facial localization and classification for future work.

![FaceGender]({{site.url}}/blog/images/face_gender/results/bad_faces.png)
*Fig. 13. Example test images in which some localized faces were assigned
incorrect gender labels.  Red [blue] bounding boxes enclose faces
classified as female [red].  The CNN was uncertain about gender for faces
marked by green bounding boxes.  A few faces in these images were also not
localized.*

## References

1.  See "Localizing Faces and Hands via Data Augmentation, Image Scaling
and Instance Segmentation" post dated Jun 27, 2016.

2.  S. Yang, P. Luo, C.C. Loy and X. Tang, "WIDER FACE: A Face Detection
Benchmark", arXiv: 1511.06523v1 (2015).

3.  D. King, dlib C++ library, http://dlib.net/ml.html.

4.  See http://www.openscenegraph.org.

5.  See http://www.face-rec.org/databases.

6.  G. Levi and T. Hassner, "Age and Gender Classification Using
Convolutional Neural Networks," IEEE Workshop on Analysis and Modeling of
Faces and Gestures (AMFG), at the IEEE Conf. on Computer Vision and Pattern
Recognition (CVPR), Boston, June 2015.

7.  See http://www.openu.ac.il/home/hassner/Adience/data.html.

8.  A. Bastanfard, M.A. Nik and M. M. Dehshibi, "Iranian Face Database with
Age, Pose and Expression", ICMV 2007.

9.  See http://www.iranprc.org/en/ifdb.ph.

10.  See ethereon.github.io/netscope/quickstart.html.

11.  K. Simonyan and A. Zisserman, *Very Deep Convolutional Networks for
Large-Scale Image Recognition*, arXiv 1409.1556, (2015).

12.  N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever and
R. Salakhudinov, "Dropout: A Simple Way to Prevent Neural Networks from
Overfitting,", J. Machine Learning Research (2014), 1929.

13.  See http://caffe.berkeleyvision.org.
