---
layout: post
title:  "Classifying Gender via a Convolutional Neural Network for Faces"
date:   2016-08-09 12:00:00
categories: Deep Learning
---

## Gender-labeled facial data

In our previous blog post, we localized faces within internet photos using
instance segmentation.  In this entry, we continue to accumulate and
investigate a sizable corpus of "faces in the wild".  But now our primary
focus is upon classifying rather than detecting faces.

Humans around the world may be described in terms of various
characteristics which can change on time scales ranging from seconds
(e.g. mood) to decades (e.g. age).  But for most people, gender labels are
constant throughout their lifetimes.  Moreover, physical differences exist
between male and female faces.  So we seek to build a classifer which can
identify gender based primarily upon face imagery input.

We begin by extending our dataset of internet images from our prior face
localization study.  We have collected 13740 images which intentionally
exhibit as much variety as possible.  2815 of the images originated from
the WIDER collection [ref], while the remaining pictures were manually
downloaded primarily from Google Images.  10492 photos in this corpus
contain 43752 human faces which were annotated with bounding boxes via
dlib's image labeling tool IMGLAB [ref].  The labeled faces belong to
people with differing attributes (e.g. gender, age, ethnicity) performing
varying activities (e.g. socializing, swimming, singing) in disparate
settings (e.g. hospital interiors, memorial services, sport stadiums).
Moreover, people's visages are oriented in variable poses (e.g. facing
camera, looking upwards, profile view), and many are partially occluded by
objects (e.g. eyeglasses, drinking cups, other people's heads).  We also
intentionally increased the number of internet images containing faces from
paintings and drawings from our previous face investigation.

While dlib's IMGLAB is convenient for drawing bounding boxes around
objects, it does not currently support hierarchical assignment of multiple
labels per box.  So we developed our own gender attribution tool based upon
the 3D graphics toolkit OpenSceneGraph [ref].  It imports and displays face
bounding boxes generated via IMGLAB. A user may press the "1", "2" or "0"
keys on the number pad to set gender attributes for all bounding boxes in
an image to male, female or unknown.  After a particular box is selected
via the mouse, its gender attribute may be altered to male, female or
unknown by pressing the "1", "2" or "~" keys near the upper left of a
keyboard.  We used this attribution tool to assign gender labels to 40K+
facial bounding boxes.

One example of an image containing multiple gender assignments is displayed
in figure 1.  Purple [cyan] bounding boxes enclose female [male] faces.
Yellow bounding boxes mark persons for which we were significantly
uncertain about their sex.  As we attempted to accurately attribute
thousands of different people appearing in internet images, we were
surprised by how often it is difficult to determine gender from facial and
other appearance cues.  In particular, we found distinguishing baby boys
from baby girls to be nearly impossible if their clothing provided no clear
cultural hints.  In other cases, one can deduce that blurry patches in
images must correspond to human faces.  But such patches may contain too
little information for a gender label to be confidently assigned.  All such
faces marked with "unknown" genders are ignored in our classification
pipeline.

![FaceGender]({{site.url}}/blog/images/face_gender/training/image_03815_crowd_genders_01.png)
*Fig. 1. An image attributed with gender labels.*

The bounding boxes in figure 1 are snuggly wrapped around individual faces.
But valuable information is contained in hair styles, neckware and
upper-body clothing for gender determination.  So we double all bounding
box widths and heights.  The resulting human head regions come in a wide
variety of pixel sizes.  But a convolutional neural network requires
uniformly-sized imagery input.  In particular, the CNN we developed takes
image chips with 96x96 pixel size.  So chips larger than 96x96 were
subsampled, while chips smaller than 96x96 were superposed onto black
backgrounds.  Examples of head chips extracted from figure 1 and spatially
resized are presented in figure 2.

![FaceGender]({{site.url}}/blog/images/face_gender/training/training_chips_106x106.png)
*Fig. 2. Three female and three male head chips extracted from figure
1.  The chips' pixel sizes are doubled compared to their tightly-cropped
facial bounding box progenitors.*

In order to expand the volume of our training and testing data, we searched
online for other sets of gender-labeled face image chips.  While many
facial databases can be found on the internet [see
www.face-rec.org/databases/], relatively few come with gender metadata.
One relatively recent dataset may be obtained from the OUI-Adience Face
Image Project from which we extracted 17457 photos [see
http://www.openu.ac.il/home/hassner/Adience/data.html; papers].  The
sources for these data are flickr albums, and Adience images are
consequently "in the wild".  We note, however, that flickr users are not
likely to be representative of the global human population.  We also
included 369 head shots of Iranian women available from the Iranian Face
Database [see http://www.iranprc.org/en/ifdb.ph; A. Bastanfard, M.A. Nik
and M. M. Dehshibi, "Iranian Face Database with Age, Pose and Expression",
ICMV 2007.]  After combining all these head shots, we have a total of 54760
image chips with male or female labels.

We randomly reserve 2.5% of the 50K+ chips for validation and another 7.5%
for testing.  Image augmentation is performed upon the remaining 90% of
chips in order to increase the variety of training samples.  The enlarged
bounding boxes surrounding people's heads are translated and rotated by
small amounts.  Half of all the augmented chips are also flipped
horizontally.  Constant random offsets are added to each pixel's hue,
saturation and value color coordinates.  Gaussian noise is also
intentionally injected into augmented images.  Figure 3 displays
representative examples of original and augmented image chips.

![FaceGender]({{site.url}}/blog/images/face_gender/training/augmented_female_face.png)

![FaceGender]({{site.url}}/blog/images/face_gender/training/augmented_male_face.png)

*Fig. 3. Examples of imagery augmentation.  Input female and male training
chips appear in the leftmost column of this figure.  Their output variants
are randomly translated, rotated, color-modulated and flipped relative to
the original images.*

For each of the training chips which originated from our highly-variable
set of faces, we add five perturbed counterparts.  But since training chips
from the Adience and Iran Faces databases exhibit less variety, we only add
one augmented variant per chip.  After augmentation, we end up with a total
of 260788 training chips which carry either male or female labels.

The final set of input data comes from images containing no facial content.
Working with our original set of 13.7K+ photos, we randomly choose numbers,
sizes and locations of bounding boxes in each picture.  Any random box
which overlaps a human head is ignored.  Nonface chips are also heavily
biased towards those with sizable image entropies.  We thus generate 99204
non-face image chips, and we reserve 0.5% [1.5%] for validation [testing].
Representative examples of such non-face chips are presented in figure 4.

![FaceGender]({{site.url}}/blog/images/face_gender/training/non_face_training_samples.png)
*Fig. 4.  Image chips containing no human face content.*

## CNN Architecture and Training


![FaceGender]({{site.url}}/blog/images/face_gender/training/padded_cropped_net.png)
*Fig. X. Training architecture for CNN used to classify facial genders.*


![FaceGender]({{site.url}}/blog/images/face_gender/training/padded_accuracy_curves.png)
*Fig. X. CNN model accuracy plotted as a function of training epoch.  The
cyan curve is a temporally smoothed version of the raw accuracy values for
the training set which are colored in dark blue.  The CNN's accuracy on a
separate validation set is depicted by the red curve.*


![FaceGender]({{site.url}}/blog/images/face_gender/results/padded_correct_female_classifications.png)
*Fig. X. Representative examples of test image chips correctly classified
by the CNN as female.*

![FaceGender]({{site.url}}/blog/images/face_gender/results/padded_correct_male_classifications.png)
*Fig. X. Representative examples of test image chips correctly classified
by the CNN as male.*


![FaceGender]({{site.url}}/blog/images/face_gender/results/padded_unsure_classifications.png)
*Fig. X. Examples of test image chips for which the CNN was significantly
unsure about gender classification.  The top [bottom] row contains female
[male] image chips.*


![FaceGender]({{site.url}}/blog/images/face_gender/results/padded_incorrect_classifications.png)
*Fig. X. Examples of test image chips for which the CNN assigned incorrect
gender classifications.  The top [bottom] row contains female [male] image chips.*


<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: center">Â </th>
      <th style="text-align: center">non- face</th>
      <th style="text-align: center">male face</th>
      <th style="text-align: center">female face</th>
      <th style="text-align: center">uncertain gender</th>
      <th style="text-align: center">% correct</th>
      <th style="text-align: center">% uncertain</th>
      <th style="text-align: center">% incorrect</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>non-face</strong></td>
      <td style="text-align: center">1006</td>
      <td style="text-align: center">10</td>
      <td style="text-align: center">13</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">97.76</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">2.24</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>male face</strong></td>
      <td style="text-align: center">51</td>
      <td style="text-align: center">1570</td>
      <td style="text-align: center">172</td>
      <td style="text-align: center">26</td>
      <td style="text-align: center">86.31</td>
      <td style="text-align: center">1.43</td>
      <td style="text-align: center">12.26</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>female gender</strong></td>
      <td style="text-align: center">29</td>
      <td style="text-align: center">294</td>
      <td style="text-align: center">1334</td>
      <td style="text-align: center">31</td>
      <td style="text-align: center">79.03</td>
      <td style="text-align: center">1.84</td>
      <td style="text-align: center">19.14</td>
    </tr>
  </tbody>
</table>


![FaceGender]({{site.url}}/blog/images/face_gender/results/good_all_faces.png)
*Fig. X. Example test images for which all faces were automatically localized 
and assigned correct gender labels.  Red [blue] bounding boxes enclose
female [male] faces.*


![FaceGender]({{site.url}}/blog/images/face_gender/results/good_most_faces.png)
*Fig. X. Example test images for which all faces that were localized were
assigned correct gender labels.  Red [blue] bounding boxes enclose female
[male] faces, while the CNN assigned no gender labels to green bounding
boxes.  A few faces in these images were not localized.*

![FaceGender]({{site.url}}/blog/images/face_gender/results/OK_faces.png)
*Fig. X. Example test images for which all faces that were localized were
assigned correct gender labels.  Several faces in these images were not
localized.*


![FaceGender]({{site.url}}/blog/images/face_gender/results/bad_faces.png)
*Fig. X. Example test images which which some faces that were localized
were assigned incorrect gender labels.  Red [blue] bounding boxes enclose
faces classified as female [red].  Some faces in these images were also not
localized.*

## References

*.  See "Detecting Faces via Deep Semantic Segmentation of Images" post
dated Jan 20, 2016.

*.  S. Yang, P. Luo, C.C. Loy and X. Tang, "WIDER FACE: A Face Detection
Benchmark", arXiv: 1511.06523v1 (2015).

*.  D. King, dlib C++ library, http://dlib.net/ml.html.

*.  See http://caffe.berkeleyvision.orig.

*.  G. Levi and T. Hassner, "Age and Gender Classification Using
Convolutional Neural Networks," IEEE Workshop on Analysis and Modeling of
Faces and Gestures (AMFG), at the IEEE Conf. on Computer Vision and Pattern
Recognition (CVPR), Boston, June 2015.

*.  See ethereon.github.io/netscope/quickstart.html
