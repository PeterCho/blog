---
layout: post
title:  "Visualizing Preferred Inputs, Activation Responses and Descriptor
Clusterings for Trained CNNs"
date:   2016-09-21 12:00:00
categories: Deep Learning
---

## Architectures, training and classification performance for two CNNs

Since the breakthrough paper of Krizhevsky et al in 2012 [ref], convolution
neural networks have repeatedly been demonstrated to perform a growing list
of useful image processing tasks.  But though their utility is now
quantitatively established, qualitative understanding of CNNs remains
limited.  So in this blog posting, we employ some previously developed
visualization techniques as well as present new methods to gain insight
into trained networks.  As we'll see, visualizing CNN results confirms some
existing lore and also reveals new surprises.

We work again with a corpus of female face, male face and non-face images
which we have described in previous postings [ref].  The pixel size for
each input image is 96x96.  In the past, we padded extracted faces smaller
than 96x96 with black pixels.  Now, we instead scale the smaller pixel
dimension of any input chip up to 96 while maintaining the original aspect
ratio.  To our surprise, we found this simple preprocessing change
significantly improves CNN classification.  Representative examples of
rescaled face inputs are pictured in fig XXX.

To avoid working with overly upsampled input faces, we now ignore any image
chip whose original horizontal pixel dimension is less than 20 pixels in
size.  Table 1 breaks down our ground-truthed female, male and non face
images into training, validation and testing sets.  As in our previous
posting [ref], we perform data augmentation on female and male facial
images in order to increase the training set size.  So our final training
set includes 416223 images = 160283 female faces + 153892 male faces +
102048 non-faces.

<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: center"> Class label </th>
      <th style="text-align: center">Training set</th>
      <th style="text-align: center">Validation set</th>
      <th style="text-align: center">Testing set</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Female face </td>
      <td style="text-align: center"> 28381 </td>
      <td style="text-align: center"> 629 </td>
      <td style="text-align: center"> 1962 </td>
    </tr>

    <tr>
      <td style="text-align: center">Male face </td>
      <td style="text-align: center"> 26176 </td>
      <td style="text-align: center"> 526 </td>
      <td style="text-align: center"> 1847 </td>
    </tr>

    <tr>
      <td style="text-align: center">Non-face </td>
      <td style="text-align: center"> 102048</td>
      <td style="text-align: center"> 496 </td>
      <td style="text-align: center"> 1060 </td>
    </tr>
  </tbody>
</table>

*Table 1.  Numbers of non-augmented image chips belonging to the training,
validation and testing data sets.*

We use the ground-truth data to train and evaluate two different
convolutional neural networks.  The first is the same as the one employed
in our previous blog entry [ref].  It contains 4 convolutional layers
followed by 3 fully connected layers.  ReLU thresholding and dropout are
performed after each computational layer.  We refer to this network as
Model I.  Its architecture is pictured on the left side of figure XX.

Our second model has 6 convolutional layers along with 3 fully connected
layers, and we refer to it as Model II.  ReLU thresholding and batch
normalization [ref] are performed after each computational layer Since
batch normalization effectively includes biases among its parameters, we
only learn weights for the 9 computational layers of Model II.  Moreover,
we dispense with dropout after the ReLU1 and ReLU2 operations, for batch
normalization is believed to perform some regularization.  We also set the
L2 regularization parameter 0.0005 to half that for Model I.  The
architecture for Model II is displayed on the right side of figure XX.

![CNNVis]({{site.url}}/blog/images/cnn_vis/padded_model2e_2t_architectures.jpg)
*Fig. *. Architectures for our Model I [left] and Model II [right] networks
designed to classify faces.  These graphs were generated by the web-based
Netscope visualization tool [ref].*

We train Models I and II using our O(416K) augmented images via the caffe
framework [ref].  The 0.003 base learning rate and 25K learning rate
step-down hyperparameters for Model I are less aggressive than their 0.03
and 10K Model II counterparts which batch normalization enables.  As the
learning curves plotted in figure XX demonstrate, fewer training epochs are
required to fit the O(11.6M) variables of Model II than the O(10.6M)
variables of Model I.

![CNNVis]({{site.url}}/blog/images/cnn_vis/model2e_vs_2t_training_curves.jpg)
*Fig. *. Learning curves for Model I [left] and Model II [right] plotted as
functions of training epoch. The cyan curves are temporally-smoothed
versions of the dark blue accuracy values for the training data.  The red
curves illustrate the CNNs' accuracy on held-out subsets of the training
data.*

More importantly, tables 2a and 2b quantify the improved classification
performance of Model II over Model I.  Model II's performance gain may be
partly attributed to its use of batch normalization which reduces the
potential for overfitting But Model II is also a deeper network than Model
I.  So its better performance is consistent with CNN lore.  


<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: center"> non- </th>
      <th style="text-align: center"> male </th>
      <th style="text-align: center"> female </th>
      <th style="text-align: center"> uncertain </th>
      <th style="text-align: center"> %  </th>
      <th style="text-align: center"> %  </th>
      <th style="text-align: center"> %  </th>
    </tr>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: center"> face </th>
      <th style="text-align: center"> face </th>
      <th style="text-align: center"> face </th>
      <th style="text-align: center"> gender </th>
      <th style="text-align: center"> correct </th>
      <th style="text-align: center"> uncertain </th>
      <th style="text-align: center"> incorrect </th>
    </tr>

  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>non-face</strong></td>
      <td style="text-align: center">1058</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">99.91</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0.09</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>male face</strong></td>
      <td style="text-align: center">128</td>
      <td style="text-align: center">1116</td>
      <td style="text-align: center">95</td>
      <td style="text-align: center">41</td>
      <td style="text-align: center">80.87</td>
      <td style="text-align: center">2.97</td>
      <td style="text-align: center">16.16</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>female face</strong></td>
      <td style="text-align: center">95</td>
      <td style="text-align: center">145</td>
      <td style="text-align: center">1130</td>
      <td style="text-align: center">48</td>
      <td style="text-align: center">79.69</td>
      <td style="text-align: center">3.39</td>
      <td style="text-align: center">16.93</td>
    </tr>
  </tbody>
</table>

*Table 2a. Model I classification results for a held-out truth set of 1059
non-face + 1380 male + 1418 female = 3857 image chips.  Model I correctly
classifies 85.66% of the test images, incorrectly classifies 12.03% and is
uncertain about 2.3%.*

<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: center"> non- </th>
      <th style="text-align: center"> male </th>
      <th style="text-align: center"> female </th>
      <th style="text-align: center"> uncertain </th>
      <th style="text-align: center"> %  </th>
      <th style="text-align: center"> %  </th>
      <th style="text-align: center"> %  </th>
    </tr>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: center"> face </th>
      <th style="text-align: center"> face </th>
      <th style="text-align: center"> face </th>
      <th style="text-align: center"> gender </th>
      <th style="text-align: center"> correct </th>
      <th style="text-align: center"> uncertain </th>
      <th style="text-align: center"> incorrect </th>
    </tr>

  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>non-face</strong></td>
      <td style="text-align: center">1053</td>
      <td style="text-align: center">3</td>
      <td style="text-align: center">3</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">99.53</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0.47</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>male face</strong></td>
      <td style="text-align: center">84</td>
      <td style="text-align: center">1158</td>
      <td style="text-align: center">111</td>
      <td style="text-align: center">27</td>
      <td style="text-align: center">83.91</td>
      <td style="text-align: center">1.96</td>
      <td style="text-align: center">14.13</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>female face</strong></td>
      <td style="text-align: center">62</td>
      <td style="text-align: center">136</td>
      <td style="text-align: center">1198</td>
      <td style="text-align: center">22</td>
      <td style="text-align: center">84.49</td>
      <td style="text-align: center">1.55</td>
      <td style="text-align: center">13.96</td>
    </tr>
  </tbody>
</table>

*Table 2b. Model II classification results for a held-out truth set of 1059
non-face + 1380 male + 1418 female = 3857 image chips.  Model II correctly
classifies 88.40% of the test images, incorrectly classifies 10.32% and is
uncertain about 1.27%.*

To gain intuition into the differences between Models I and II, it is
helpful to visualize what each network has learned in multiple ways.  We
consequently explore several visualizations of these trained
networks in the following sections.

## Preferred imagery inputs for trained CNN nodes

Traditional backpropagation iteratively exploits gradients to adjust the
weights of a neural network and minimize a loss function quantifying the
discrepancy between fixed input image labels and their classifier-generated
counterparts.  But backpropagation can also be used to generate images that
maximally activate neurons in a trained network [refs].  Network weights
are held fixed, and instead pixel values of input test images are
iteratively perturbed to maximize stimulation outputs of trained neurons.
We employ Justin Johnson's python script which calls caffe to reconstruct
such preferred input images [ref].

Previous researchers have found that regularization of preferred input
reconstruction plays a critical role in the quality of recovered images
[refs].  A large number of regularization parameters can be adjusted to
optimize image reconstructions.  Following Bengio et al, we perform random
searches over these hyperparameters [ref].  We have empirically found
significant differences among optimized reconstruction parameters between
Models I and II.  Moreover, visualization of nodes within different layers
of a CNN can benefit from different reconstruction parameter choices.

Preferred image reconstruction starts by seeding input chips with blurred
gaussian noise.  Different reconstruction results are therefore generated
by running backpropagation with different noise inputs.  For example, three
preferred image inputs for the female and male nodes in the FC7 layer of
Models I and II are pictured in figure XX.  Looking at these facial
reconstructions, we see that those from deeper Model II are obviously
higher fidelity than those from Model I.  Moreover, preferred inputs for
nodes in next-to-last fully connected layers typically contain multiple
instances of target objects as previous investigators have reported [ref].

![CNNVis]({{site.url}}/blog/images/cnn_vis/fc7_reconstructions_2e_2t.png)
*Fig. *. Three representative reconstructions of preferred image inputs for
the female and male face neurons in the final fully connected layers of
Model I [left] and Model II [right].*

We next reconstruct a preferred image input for every trained node in Model
I.  We then use an image graph visualization tool [ref] to explore
relationships among the nodes' reconstructions.  Reconstruction results are
presented in figure XX.  At the start of the figure's video, we see an
overall layout for the nodes and edges of Model I.  Edges are color-coded
according to their weights.  Hotter [cooler] edges correspond to larger
[small]er weight values following ReLU thresholding.

<a href="http://www.youtube.com/watch?feature=player_embedded&v=CSrUYNdaH08"
target="_blank"><img src="http://img.youtube.com/vi/CSrUYNdaH08/0.jpg"
alt="IMAGE ALT TEXT HERE" width="720" height="480" border="10" /></a>
*Fig. *.  Movie illustrating reconstructions of preferred input images for
neurons in Model I.  Click on image above in order to play video.*

After the movie in figure XX zooms into the top input data layer, we see
its three color channels represented by blue, green and red versions of an
input face image.  We also observe that the preferred image inputs for all
nodes in convolutional layer 1 are pure color chips.  This is in contrast
to other CNNs with first layer filters larger than 3x3 pixels whose
preferred inputs resemble Gabor edgelets and color splotches.  It is
important to note that pure color chip preference in the first
convolutional layer also occurs for the VGG-16 model [ref] which Model I
intentionally resembles.  As a sanity check on the first layer's
reconstruction results, one can confirm that nodes which prefer pure red,
blue or green inputs have strong weight connections to the red, blue or
green input face channels.  As the movie plays, we also see that a
purple-colored preferred input among the layer 1 nodes has strong edge
connections to both the red and blue input channels.

The nodes in convolutional layers 2, 3 and 4 of Model I favor input
textures whose complexity grows with increasing layer depth.  With some
imagination, one can see individual face parts like eyes and mouths among
the layer 4 reconstructions.  Entire faces appear among the preferred
inputs for nodes in fully connected layers 5 and 6.  Multiple face parts
often appear within individual FC5 and FC6 node reconstructions.  Other
nodes within the fully connected layers prefer complex texture inputs.
They consequently are stimulated by non-face input images.  Finally, the
last 3 nodes in layer 7 clearly respond to male face, female face and
non-face input images.

Reconstructed examples of preferred node inputs for Model II are
qualitatively similar to those for Model I.  But as the video in figure XX
demosntrates, reconstructed inputs at deeper levels of Model II are more
intricate than their counterparts for Model I.  In particular, we observe
parts of entire faces emerging within the last convolutional layer 4b.
And the fidelity for reconstructed male and female faces within the first
fully connected layer 5 is noteworthy.

<a href="http://www.youtube.com/watch?feature=player_embedded&v=8oyBVBwnb_Q"
target="_blank"><img src="http://img.youtube.com/vi/8oyBVBwnb_Q/0.jpg"
alt="IMAGE ALT TEXT HERE" width="720" height="480" border="10" /></a>
*Fig. *.  Movie illustrating reconstructions of preferred input images
for neurons in Model II.  Click on image above to play video.*

## Activation responses of trained CNN nodes to test images

According to conventional wisdom, early layers in CNNs extract basic
information from input images such as their gross color, edge and texture
contents.  On the other hand, more abstract knowledge is supposed to be
distilled in later layers.  In particular, a classifier's assignments
should be insensitive to geometry and coloring details of input images.
Indeed, training data augmentation specifically tries to increase network
insensitivity to such seemingly irrelevant low-level information.

To check this lore, we want to investigate neuron activations to a set of
test images and qualitatively assess their dependence upon the inputs.  We
do so for Model I in figure XX.  The video in this figure presents 50
female and 50 male face inputs which are ordered according to their
classification probabilities.  For each input image, the network neurons
are color-coded according to their stimulation responses.  Bright,
hot-colored neurons are activated more strongly than dim, cold-colored
neurons.

As the movie in figure XX plays and speeds up, we observe that activation
patterns within the convolution layers look fairly random.  In contrast,
the patterns of stimulation within the fully connected layers are much more
correlated.  While the overall magnitude of neuron responses decrease as
classifier probabilities go down, the relative strengths of neuron outputs
in layers 5 and 6 are remarkably stable.  

<a href="http://www.youtube.com/watch?feature=player_embedded&v=PW1DxeCQYO4"
target="_blank"><img src="http://img.youtube.com/vi/PW1DxeCQYO4/0.jpg"
alt="IMAGE ALT TEXT HERE" width="720" height="480" border="10" /></a>
*Fig. *.  Movie illustrating activation responses of trained neurons in
Model I to 50 female and 50 male test images.  Click on image above to play
video.*

The patterns among fully connected layer activations are even more striking
for Model II.  As the video in figure XX illustrates, relative hot and cold
neurons in the FC5 and FC6 layers are generally insensitive to input image
details.  Moreover, many of the neurons in the fully connected layers that
respond strongly to female faces to not turn on for male countenances and
vice-versa.  Interestingly, the first 43 neurons in FC6 are moderately
stimulated for both female and male inputs.  And their activation levels
remain steady even as overall female and male face classification
probabilities decline.  This pattern suggests the first 43 neurons in FC6
encode the concept of human face independent of gender.  In contrast, the
next O(150) FC6 neurons encode female and male gender information.

<a href="http://www.youtube.com/watch?feature=player_embedded&v=SUeiqIVD9gc"
target="_blank"><img src="http://img.youtube.com/vi/SUeiqIVD9gc/0.jpg"
alt="IMAGE ALT TEXT HERE" width="720" height="480" border="10" /></a>
*Fig. *.  Movie illustrating activation responses of trained neurons in
Model II to 50 female and 50 male test images.  Click on image above 
to play video.*

Reconstructed preferred image inputs support these hypotheses.  For
example, figure XX [YY] illustrates representative reconstructions for
neurons activated in the FC5, FC6 and FC7 layers of Model II by a female
[male] face.  Reconstructions for neurons which do not activate are
replaced by grey thumbnails in these figures.  For the FC5 layer, the
reconstructed inputs for neurons activated by the female [male] face look
female [male].  This same pattern generally holds for the FC6 layer except
for its first 43 neurons.  Reconstructions of preferred image inputs for
these initial FC6 nodes all contain a single eye which exhibits no obvious
gender characteristics.

![CNNVis]({{site.url}}/blog/images/cnn_vis/female_activated_neurons.png)
*Fig. *. Preferred input image reconstructions for some Model II neurons
activated by an input female face test image.*

![CNNVis]({{site.url}}/blog/images/cnn_vis/male_activated_neurons.png)
*Fig. *. Preferred input image reconstructions for some Model II neurons
activated by an input male face test image.*

Further support for our contention that the FC6 layer in Model II contains
general face, female face, male face and non-face neurons is provided by
figure XX.  Its movie displays network activations for 10 female and male
test images as well as 10 non-face inputs.  The face images were
intentionally selected based upon their relatively weak gender
classification probabilities.  Yet Model II has no trouble distinguishing
these faces from their non-face counterparts.  As the figure's movie plays,
we observe that the first 43 neurons in FC6 are again moderately activated
for all the face images.  Moreover, the last O(100) neurons are strongly
stimulated by all the non-face images.  These activation patterns strongly
suggest that Model II has hierarchically learned to firstly distinguish
between face and non-face categories and secondly separate faces into
female and male subclasses.

<a href="http://www.youtube.com/watch?feature=player_embedded&v=nZNLX5T9aAA"
target="_blank"><img src="http://img.youtube.com/vi/nZNLX5T9aAA/0.jpg"
alt="IMAGE ALT TEXT HERE" width="720" height="480" border="10" /></a>
*Fig. *.  Activation responses of Model II neurons to 10 female face, 10
male face and 10 non-face test images.  Though the classifier is uncertain
about gender for these examples, it has no difficulty distinguishing faces
from non-faces.  Click on image above to play video.*

Given Model II's apparent learning of gender-independent faces, we are
motivated to ask if this network might have learned other concepts which we
did not explicitly set out to teach it.  In particular, we wonder whether
this classifier might be able to distinguish among human traits such as age
or ethnicity, image properties like blurriness or monochromaticity, and/or
geometric differences such as facial pose.  To address these questions, we
employ an image search system as a final, useful visualization tool [ref]
in the next section.

## Trained CNN clusterings of test images


<a href="http://www.youtube.com/watch?feature=player_embedded&v=52YpP-hradg"
target="_blank"><img src="http://img.youtube.com/vi/52YpP-hradg/0.jpg"
alt="IMAGE ALT TEXT HERE" width="720" height="480" border="10" /></a>
*Fig. *.  Image graph for O(7K) test images generated from descriptors
extracted from fully connected layer 5 of Model II.  Click on image above
to play video.*


<a href="http://www.youtube.com/watch?feature=player_embedded&v=ZsO2qQhA9UY"
target="_blank"><img src="http://img.youtube.com/vi/ZsO2qQhA9UY/0.jpg"
alt="IMAGE ALT TEXT HERE" width="720" height="480" border="10" /></a>
*Fig. *.  Female and male faces oriented in a sequence of left-to-right
poses appear in the upper windows.  Their nearest neighbors in Model II FC5
layer descriptor space are displayed as thumbnails in the lower carousels.
Nearest neighbor face orientations are strongly correlated with the query
faces'.  Click on image above to play video.*




## References

*.  A. Krizhevsky, I. Sutskever and G.E. Hinton, "ImageNet Classification
with Deep Convolutional Neural Networks", NIPS (2012).

*.  See ethereon.github.io/netscope/quickstart.html.

*.  See "Classifying Gender in Internet Images via a Convolutional Neural
Network for Faces" (Aug 2016) entry in this blog.

*.  See "Detecting Faces and Hands via Data Augmentation, Image Scaling and
Instance Segmentation" (Jun 2016) entry in this blog.

*.  S. Ioffe and C. Szegedy, "Batch Normalization: Accelerating Deep Network
Training by Reducing Internal Covariate Shift", ICML (2015).

*.  See http://caffe.berkeleyvision.org.

*.  See https://github.com/jcjohnson/cnn-vis.

*.  K. Simonyan and A. Vedaldi and A. Zisserman, "Deep Inside Convolutional
Networks: Visualising Image Classification Models and Saliency Maps", ICLR
(2014).

*.  J. Yosinski, J. Clune, A. Nguyen, T. Fuchs, H. Lipson H (2015)
"Understanding Neural Networks Through Deep Visualization", ICML 2015 Deep
Learning workshop.

*.  [J.  Bergstra and Y. Bengio, "Random Search for Hyper-Parameter
Optimization", JMLR (2012).](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)


*.  Cho and Yee image search system paper
