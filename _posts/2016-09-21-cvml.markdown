---
layout: post
title:  "Visualizing Preferred Inputs, Activation Responses and Descriptor
Clusterings for Trained CNNs"
date:   2016-09-21 12:00:00
categories: Deep Learning
---

## Architectures, training and classification performance for two CNNs

Since the breakthrough paper of Krizhevsky et al in 2012 [ref], convolution
neural networks have been repeatedly demonstrated to perform a growing list
of useful image processing tasks.  But though their utility is
indisputable, intuitive understanding of CNNs remains limited.  So in this
blog posting, we employ some previously developed visualization techniques
as well as present new methods for gaining insight into trained networks.
As we'll see, visualizing CNN results confirms some existing lore and
reveals new surprises.

We work again with a corpus of female face, male face and non-face images
which has been described in previous postings [ref].  The input pixel size
for each image is 96x96.  In the past, we padded extracted faces smaller
than 96x96 with black pixels.  Now, we instead scale the smaller pixel
dimension of any input chip up to 96 while maintaining the original aspect
ratio.  To our surprise, we found this simple preprocessing change
significantly improves CNN classification performance.  Representative
examples of rescaled face and non-face inputs are pictured in fig XXX.

To avoid working with overly upsampled input faces, we ignore any image
chip whose original horizontal pixel dimension is less than 20 pixels in
size.  Table 1 breaks down our ground-truthed female, male and non face
images into training, validation and testing sets.  As in our previous
posting [ref], we perform data augmentation on female and male facial
images in order to increase the training set size.  So our final training
set includes 416223 images = 160283 female faces + 153892 male faces +
102048 non-faces.

<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: center"> Class label </th>
      <th style="text-align: center">Training set</th>
      <th style="text-align: center">Validation set</th>
      <th style="text-align: center">Testing set</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Female face </td>
      <td style="text-align: center"> 28381 </td>
      <td style="text-align: center"> 629 </td>
      <td style="text-align: center"> 1962 </td>
    </tr>

    <tr>
      <td style="text-align: center">Male face </td>
      <td style="text-align: center"> 26176 </td>
      <td style="text-align: center"> 526 </td>
      <td style="text-align: center"> 1847 </td>
    </tr>

    <tr>
      <td style="text-align: center">Non-face </td>
      <td style="text-align: center"> 102048</td>
      <td style="text-align: center"> 496 </td>
      <td style="text-align: center"> 1060 </td>
    </tr>
  </tbody>
</table>

*Table 1.  Numbers of non-augmented image chips belonging to the training,
validation and testing data sets.*

We use these ground-truth data to train and evaluate two different
convolutional neural networks.  The first is the same as the one employed
in our previous blog entry [ref].  It contains 4 convolutional layers
followed by 3 fully connected layers.  Dropout is performed after each
computational layer.  We refer to this network as Model I.  Its
architecture is pictured on the left side of figure XX.

Our second model has 6 convolutional layers along with 3 fully connected
layers, and we refer to it as Model II.  Batch normalization is performed
after each computational layer [ref].  Since batch normalization
effectively includes biases among its parmaeters, we only learn weights
within the 9 computationsl layers in Model II.  Moreover, we dispense with
dropout after the ReLU1 and ReLU2 operations, for batch normalization is
believed to perform some regularization.  We also set the L2 regularization
parameter 0.0005 to half that for Model I.  The architecture for Model II
is displayed on the right side of figure XX.

![CNNVis]({{site.url}}/blog/images/cnn_vis/padded_model2e_2t_architectures.jpg)
*Fig. *. Architectures for our Model I [left] and Model II [right] networks
designed to classify faces.  These graphs were generated by the web-based
Netscope visualization tool [ref].*

We train Models I and II using our O(416K) ground-truth images within the
caffe framework [ref].  The 0.003 base learning rate and 25K learning rate
step-down size for Model I are less aggressive than their 0.03 and 10K
Model II counterparts which batch normalization allows.  As the learning
curves plotted in figure XX demonstrate, fewer numbers of epochs are
required to train the O(11.6M) parameters of Model II than the O(10.6M)
parameters of Model I.

![CNNVis]({{site.url}}/blog/images/cnn_vis/model2e_vs_2t_training_curves.jpg)
*Fig. *. Learning curves for Model I [left] and Model II [right] plotted as
function of training epoch. The cyan curves are temporally-smoothed
versions of the dark blue accuracy values for the training data.  The red
curves illustrate the CNNs' accuracy on held-out subsets of the training
data.*

<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: center"> non- </th>
      <th style="text-align: center"> male </th>
      <th style="text-align: center"> female </th>
      <th style="text-align: center"> uncertain </th>
      <th style="text-align: center"> %  </th>
      <th style="text-align: center"> %  </th>
      <th style="text-align: center"> %  </th>
    </tr>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: center"> face </th>
      <th style="text-align: center"> face </th>
      <th style="text-align: center"> face </th>
      <th style="text-align: center"> gender </th>
      <th style="text-align: center"> correct </th>
      <th style="text-align: center"> uncertain </th>
      <th style="text-align: center"> incorrect </th>
    </tr>

  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>non-face</strong></td>
      <td style="text-align: center">1058</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">99.91</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0.09</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>male face</strong></td>
      <td style="text-align: center">128</td>
      <td style="text-align: center">1116</td>
      <td style="text-align: center">95</td>
      <td style="text-align: center">41</td>
      <td style="text-align: center">80.87</td>
      <td style="text-align: center">2.97</td>
      <td style="text-align: center">16.16</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>female face</strong></td>
      <td style="text-align: center">95</td>
      <td style="text-align: center">145</td>
      <td style="text-align: center">1130</td>
      <td style="text-align: center">48</td>
      <td style="text-align: center">79.69</td>
      <td style="text-align: center">3.39</td>
      <td style="text-align: center">16.93</td>
    </tr>
  </tbody>
</table>

More importantly, tables 2a and 2b quantify the improved classification
performance of Model II over Model I.  Part of Model II's performance gain
may be attributed to its use of batch normalization which reduces its
overfitting potential.  But Model II is also a deeper network than Model
I.  So its better performance is consistent with CNN lore.

*Table 2a. Model I classification results for a held-out truth set of 1059
non-face + 1380 male + 1418 female = 3857 image chips.  The model correctly
classifies 85.66% of the test images, incorrectly classifies 12.03% and is
uncertain about 2.3%.*

<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: center"> non- </th>
      <th style="text-align: center"> male </th>
      <th style="text-align: center"> female </th>
      <th style="text-align: center"> uncertain </th>
      <th style="text-align: center"> %  </th>
      <th style="text-align: center"> %  </th>
      <th style="text-align: center"> %  </th>
    </tr>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: center"> face </th>
      <th style="text-align: center"> face </th>
      <th style="text-align: center"> face </th>
      <th style="text-align: center"> gender </th>
      <th style="text-align: center"> correct </th>
      <th style="text-align: center"> uncertain </th>
      <th style="text-align: center"> incorrect </th>
    </tr>

  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>non-face</strong></td>
      <td style="text-align: center">1053</td>
      <td style="text-align: center">3</td>
      <td style="text-align: center">3</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">99.53</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0.47</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>male face</strong></td>
      <td style="text-align: center">84</td>
      <td style="text-align: center">1158</td>
      <td style="text-align: center">111</td>
      <td style="text-align: center">27</td>
      <td style="text-align: center">83.91</td>
      <td style="text-align: center">1.96</td>
      <td style="text-align: center">14.13</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>female face</strong></td>
      <td style="text-align: center">62</td>
      <td style="text-align: center">136</td>
      <td style="text-align: center">1198</td>
      <td style="text-align: center">22</td>
      <td style="text-align: center">84.49</td>
      <td style="text-align: center">1.55</td>
      <td style="text-align: center">13.96</td>
    </tr>
  </tbody>
</table>

*Table 2b. Model II classification results for a held-out truth set of 1059
non-face + 1380 male + 1418 female = 3857 image chips.  The model correctly
classifies 88.40% of the test images, incorrectly classifies 10.32% and is
uncertain about 1.27%.*

In order to gain a qualitative understanding of the differences between
Models I and II, it's helpful to visualize what each network has learned.
We consequently explore this direction in the next section.

## Preferred imagery inputs for trained CNN nodes

Traditional backpropagation involves adjusting the weights of a CNN in
order to minimize a loss function quantifying the discrepancy between fixed
input images and their classifier-generated labels.  But backpropagation
can also be used to generate images that maximally activate trained neurons
in a network [refs].  CNN weights are held fixed, and instead pixel values
of input test images are adjusted to maximize stimulation outputs of
trained neurons.  We employ Justin Johnson's python script which calls
caffe to reconstruct such preferred input images [ref].

Previous researchers have found that regularization of preferred input
reconstruction plays a critical role in the quality of recovered images
[refs].  A large number of regularization parameters can be adjusted in
order to optimize image reconstruction.  Following Bengio et al, we perform
random searches over these hyperparameters [ref].  We empirically found
significant differences among optimized reconstruction parameters between
Models I and II.  Moreover, visualization of nodes within different layers
of a CNN can benefit from different reconstruction parameter choices.




![CNNVis]({{site.url}}/blog/images/cnn_vis/fc7_reconstructions_2e_2t.png)
*Fig. *. Three representative reconstructions of preferred image inputs for
the female and male face neurons in the final fully connected layers of
Model I [left] and Model II [right].*


<a
href="http://www.youtube.com/watch?feature=player_embedded&v=CSrUYNdaH08"
target="_blank"><img src="http://img.youtube.com/vi/CSrUYNdaH08/0.jpg"
alt="IMAGE ALT TEXT HERE" width="720" height="480" border="10" /></a>
*Fig. *.  Movie illustrating reconstructions of preferred input images for
neurons in Model I.  Click on image above in order to play video.*

<a
href="http://www.youtube.com/watch?feature=player_embedded&v=8oyBVBwnb_Q"
target="_blank"><img src="http://img.youtube.com/vi/8oyBVBwnb_Q/0.jpg"
alt="IMAGE ALT TEXT HERE" width="720" height="480" border="10" /></a>
*Fig. *.  Movie illustrating reconstructions of preferred input images
for neurons in Model II.  Click on image above to play video.*



## Activation responses of trained CNN nodes to test images

<a
href="http://www.youtube.com/watch?feature=player_embedded&v=PW1DxeCQYO4"
target="_blank"><img src="http://img.youtube.com/vi/PW1DxeCQYO4/0.jpg"
alt="IMAGE ALT TEXT HERE" width="720" height="480" border="10" /></a>
*Fig. *.  Movie illustrating activation responses of trained neurons in
Model I to 50 female and 50 male test images.  Click on image above to play
video.*


<a href="http://www.youtube.com/watch?feature=player_embedded&v=SUeiqIVD9gc"
target="_blank"><img src="http://img.youtube.com/vi/SUeiqIVD9gc/0.jpg"
alt="IMAGE ALT TEXT HERE" width="720" height="480" border="10" /></a>
*Fig. *.  Movie illustrating activation responses of trained neurons in
Model II to 50 female and 50 male test images.  Click on image above 
to play video.*


![CNNVis]({{site.url}}/blog/images/cnn_vis/female_activated_neurons.png)
*Fig. *. Preferred input image reconstructions for some Model II neurons
activated by an input female face test image.*


![CNNVis]({{site.url}}/blog/images/cnn_vis/male_activated_neurons.png)
*Fig. *. Preferred input image reconstructions for some Model II neurons
activated by an input male face test image.*


<a href="http://www.youtube.com/watch?feature=player_embedded&v=nZNLX5T9aAA"
target="_blank"><img src="http://img.youtube.com/vi/nZNLX5T9aAA/0.jpg"
alt="IMAGE ALT TEXT HERE" width="720" height="480" border="10" /></a>
*Fig. *.  Activation responses of Model II neurons to 10 female face, 10
male face and 10 non-face test images.  Though the classifier is uncertain
about gender for these examples, it has no difficulty distinguishing faces
from non-faces.  Click on image above to play video.*


## Trained CNN clusterings of test images


<a
href="http://www.youtube.com/watch?feature=player_embedded&v=52YpP-hradg"
target="_blank"><img src="http://img.youtube.com/vi/52YpP-hradg/0.jpg"
alt="IMAGE ALT TEXT HERE" width="720" height="480" border="10" /></a>
*Fig. *.  Image graph for O(7K) test images generated from descriptors
extracted from fully connected layer 5 of Model II.  Click on image above
to play video.*


<a href="http://www.youtube.com/watch?feature=player_embedded&v=ZsO2qQhA9UY"
target="_blank"><img src="http://img.youtube.com/vi/ZsO2qQhA9UY/0.jpg"
alt="IMAGE ALT TEXT HERE" width="720" height="480" border="10" /></a>
*Fig. *.  Female and male faces oriented in a sequence of left-to-right
poses appear in the upper windows.  Their nearest neighbors in Model II FC5
layer descriptor space are displayed as thumbnails in the lower carousels.
Nearest neighbor face orientations are strongly correlated with the query
faces'.  Click on image above to play video.*




## References

*.  A. Krizhevsky, I. Sutskever and G.E. Hinton, "ImageNet Classification
with Deep Convolutional Neural Networks", NIPS (2012).

*.  See ethereon.github.io/netscope/quickstart.html.

*.  See "Classifying Gender in Internet Images via a Convolutional Neural
Network for Faces" (Aug 2016) entry in this blog.

*.  See "Detecting Faces and Hands via Data Augmentation, Image Scaling and
Instance Segmentation" (Jun 2016) entry in this blog.

*.  S. Ioffe and C. Szegedy, "Batch Normalization: Accelerating Deep Network
Training by Reducing Internal Covariate Shift", ICML (2015).

*.  caffe ref


*.  Justin Johnson's cnn-vis project, See https://github.com/jcjohnson/cnn-vis.

*.  K. Simonyan and A. Vedaldi and A. Zisserman, "Deep Inside Convolutional
Networks: Visualising Image Classification Models and Saliency Maps", ICLR
(2014).

*.  J. Yosinski, J. Clune, A. Nguyen, T. Fuchs, H. Lipson H (2015)
"Understanding Neural Networks Through Deep Visualization", ICML 2015 Deep
Learning workshop.

*.  Network drawing reference

*.  Bengio paper about randomly selecting hyperparameters

*.  Cho and Yee image search system paper
