---
layout: post
title:  "Teaching a Deep RL Agent to Lazily Win at Atari Pong"
date:   2016-12-30 12:00:00
categories: Deep Learning
use_math: true
---

## Reinforcement learning vs supervised learning 

Reinforcement Learning $\small($ RL $\small)$ is typically described in
terms of an agent interacting with an *a priori* unknown environment.  The
environment's starting state is first sampled from some underlying
distribution.  The agent observes the state and performs some action.  The
environment then issues a reward whose dependence upon the state and action
is initially unknown and stochastic.  The basic goal of RL is to improve
the agent's policy function which maps from environment state history to
agent action so that its cumulative reward is maximized.

Supervised Learning $\small($ SL $\small)$ may be regarded as a special
case of Reinforcement Learning.  The environment first samples pairs of
input states and corresponding output labels from some distribution.  The
agent observes an input state and makes a prediction for the output label.
The environment subsequently issues a negative reward which depends upon
the disparity between the predicted and genuine output labels.  The basic
goal of SL is to improve the agent's prediction function which maps from
input state to output label so that its cumulative negative reward is
minimized.

It is useful to draw a schematic picture which highlights similarities and
differences between reinforcement and supervised learning.  Figure 1
presents such a diagram adapted from refs [1] and [2].

![PongRL]({{site.url}}/blog/images/pong_policy_learning/RL_vs_SL.png)
*Fig. 1. Schematic comparison between reinforcement and supervised
learning.  RL time-dependent states, actions and rewards are represented by
$s_t$, $a_t$ and $r_t$.  SL time-independent input states, prediction model
and output labels are denoted by $x_i$, $h$ and $y_i$.*

As the figure indicates, reinforcement learning is fundamentally more
complex than supervised learning.  In particular, RL must contend with
several challenges not present in SL:

+  Stateful world problem

   An RL agent interacts with an environment whose later states depend upon
   earlier states as well as earlier agent actions.  In contrast, input states
   in supervised learning problems do not depend upon time nor each other.

+  Reward delay problem

   An RL agent can experience a long delay between the instant when it
   performs some action and the time when the environment issues an
   associated reward signal.  In the interim, the agent generally performs
   other actions which become entangled with the eventual reward.  In
   contrast, reward signals in supervised learning settings are immediate
   and in one-to-one correspondence with agent predictions.  

+  Exploration vs exploitation problem

   An RL agent needs encouragement to search unfamiliar parts of state
   space to avoid becoming trapped in locally maximal but globally suboptimal
   performance regions.  In contrast, an SL agent generally observes all input
   states within its training environment.  

+  Step size problem

   An RL agent which works with an initially large step size can be forced
   into some poor-performance region of state space from which it may never
   escape even if its step size is later diminished.  In contrast, an SL agent
   often finds a good local minimum region as its step size is annealed.

Similarities and differences between reinforcement and supervised learning
extend to their loss functions.  It is useful to start by deriving the
cross entropy loss for SL classification.  Given a set of training inputs
$x_i$ and outputs $y_i$, we let $q$ represent the probability distribution
for the truth data and $p$ denote the probability distribution for the
prediction model.  Since $p$ should be as close as possible to $q$, the
Kullback-Leibler divergence of $p$ from $q$ should be minimal.

The KL divergence enters into the definition of cross entropy between $q$
and $p$:

$$ H(q,p) = H(q) + D_{KL}(q || p)  $$

$$ -E_q (\log p) = -E_q(\log q) + E_q \bigl(\log(q / p) \bigr) \> . $$

The ground-truth entropy $H(q)$ may be regarded as an unimportant
invariant.  So minimizing $D_{KL}$ with respect to $p$ is equivalent to
minimizing $H(q,p)$.  The cross entropy loss function assumes the form

$$ \begin{align}
L_{SL} & = - 
        \sum_{ \substack{ {\rm training} \\ {\rm samples} \, x, y} }
        q(x) \log p(y | x) \\
	& = - {1 \over N} \sum_{i = 1}^N \log p(y_i|x_i) \> . \\
\end{align}
$$

In order to begin converting this SL expression into an RL loss function,
we first rename variables following figure 1.  Input $x_i$ is replaced by
state $s_t$ and output label $y_i$ by sampled action $a_t$. The agent is
assumed to follow a stochastic policy $\pi(a|s)$ which specifies the
probability it will execute action $a$ when the environment is in state
$s$.  Policy $\pi$ thus takes the place of probability distribution $p$.

RL loss also depends upon cumulative returns which are quantified by three
related functions.  The first "state value" function

$$ V^\pi(s) = E \bigl[ r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \cdots | 
s_t = s \bigr] $$

measures expected future return when the environment starts in state $s$
and follows policy $\pi$.  The discount factor $0 \le \gamma \le 1$
appearing in this equation reduces long term dependencies by downweighting
future rewards.  The second "state-action value" function

$$ Q^\pi(s,a) = E \bigl[ r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \cdots | 
s_t = s, a_t = a \bigr]
$$

measures expected future return given starting environment state $s$ and
agent action $a$.  The third "advantage" function

$$ A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s) $$

quantifies how much more valuable is the agent's performing action $a$
rather than the expected action coming from policy $\pi$.

The reinforcement learning loss function incorporates the advantage
function estimator

$$ \hat{A}_t = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \cdots - V^\pi_t $$

as training sample coefficients:

$$ L_{RL} = - {1 \over T} \sum_{t = 0}^{T-1} \hat{A}_t \log \pi(a_t |
s_t). $$

Probabilities for good actions with positive advantages are consequently
increased in this RL loss expression, while bad actions with negative
advantages are discouraged.  Unlike $L_{SL}$ which is always positive, the
overall sign of $L_{RL}$ is unknown since it generally contains comparable
numbers of positive and negative terms.  But the magnitude of the RL loss
should generally decrease as training proceeds.

It is useful to draw one more analogy between RL and SL when neural
networks are employed as function approximators.  For classification
problems, we can train a network to map inputs onto probability
distributions over output labels.  For sequential decision making problems,
neural nets may be used to map environment states onto policies for agent
actions.  The resulting SL distribution $p(y|x; \theta)$ and RL policy
policy $\pi(a|s;\theta)$ then depend parameterically upon a vector $\theta$
of neural network weights.  In both cases, the network weights may be
iteratively refined via gradient descent of their respective loss
functions:

$$ \begin{align}
-\nabla_{\theta} L_{SL} & =  
	 {1 \over N} \sum_{i = 1}^N \nabla_{\theta} \log p(y_i|x_i; \theta) \\

-\nabla_{\theta} L_{RL} & =  
	 {1 \over T} \sum_{t = 0}^{T-1} \hat{A}_t \nabla_{\theta} 
         \log \pi(a_t|s_t; \theta) \> . \\
\end{align}
$$

Equation 4 represents the "policy gradient" approach to reinforcement
learning which may be applied to a wide range of problems.  Provided
stateful world, reward delay, exploration vs exploitation and step size
problems are not too severe, it can yield interesting results.

## Deep RL system 

Games provide good test environments for reinforcement learning.  After the
release of the Arcade Learning Environment $\small($ ALE $\small)$ in 2013
[3], Deep Mind researchers published several impressive Atari game learning
results based upon Deep Q networks [4,5].  Following this breakthrough,
Atari games in particular and computer games in general have become popular
benchmarks among RL practitioners for developing new algorithms and
evaluating their performance.  Here we explore the policy gradient method's
ability to learn to play the Atari game Pong.

We adopt the general approach presented by Andrej Kaparthy in his lucid
"Pong from Pixels" blog [6].  Since Pong screens are so simple, we can
retain their essential information content even after aggressively cropping
and subsampling raw 160 x 210 images down to 32 x 32.  Following Kaparthy,
we work with temporally differenced screens rather than sets of successive
ALE frames.  But we choose to retain all pixels corresponding to the
agent's paddle to prevent its disappearing from differenced images when the
paddle remains stationary.  We further quantize differenced pixel values
into zeros and ones.  So the Pong environment state is compressed at each
time step into a 1024 binary vector $\small($ see figure 2 $\small)$.

![PongRL]({{site.url}}/blog/images/pong_policy_learning/preprocessing.png)
*Fig. 2. Raw ALE frames are reduced to 32 x 32 binary state vectors
after cropping, downsampling and temporal differencing.*

The preprocessed screen vector becomes the input layer to a policy network.
As its input signal is sparse, the standard neural network approach of
extracting imagery features via an initial set of convolutional layers
seems like overkill.  We instead experimented with various fully connected
network configurations and settled upon the one with two hidden layers
sketched in figure 3.  Following reference [6], we neglect biases in this
network.  But unlike [6], we include three softmax nodes in the network's
output layer which compute agent probabilities for paddle up, down and no
movement actions.

![PongRL]({{site.url}}/blog/images/pong_policy_learning/network.png)
*Fig. 3.  Schematic diagram of the network which maps compressed Pong
screen states into action probabilities.*

The 35K+ networkm weights in figure 3 are initialized according to the
gaussian distribution ${\cal N}(0,\sqrt{2 / n_{in}})$ [7].  In order to
avoid weight death during training, we employ leaky ReLU rather than
conventional ReLU nonlinearities in each hidden layer node [8].  We also
impose L2-regularization to discourage weight values from growing without
bound.

The RL agent plays against a built-in computer AI during each episode of
Pong.  The agent receives a +1 reward whenever the AI fails to hit the ball
and a -1 penalty if it misses.  Episodes terminate when either player
reaches a score of 21.  We compute and store state, action and reward
tuples for 20K ALE frames.  Gradient descent is then performed via RMS
propagation [9], and network weights are updated.  The batch of training
samples is subsequently discarded, and the next set of 20K tuples is
collected.

Our RL system for Pong is fairly simple.  It depends upon a set of
hyperparameters whose empirically determined values are listed in Table 1.
For pedagogical purposes, we choose to implement the system's neural
network from scratch in C++ rather than employ one of the many existing
deep learning software packages.  Fortunately, the network is sufficiently
compact that it effectively finishes training overnight when run on a
modern CPU.

<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: left">Hyperparameter</th>
      <th style="text-align: center">Value    </th>
      <th style="text-align: center">Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Epoch frames</td>
      <td style="text-align: center">5E4</td>
      <td style="padding: 0 35px">Number of ALE frames per epoch</td>
    </tr>
    <tr>
      <td style="text-align: left">Batch size</td>
      <td style="text-align: center">2E4</td>
      <td style="padding: 0 35px">  Number of training inputs per
gradient descent computation</td>
    </tr>
    <tr>
      <td style="text-align: left">Learning rate</td>
      <td style="text-align: center">1E-3</td>
      <td style="padding: 0 35px"> Learning rate for RMS propagation</td>
    </tr>
    <tr>
      <td style="text-align: left">RMSProp decay rate</td>
      <td style="text-align: center">0.9</td>
      <td style="padding: 0 35px">Decay rate for RMS propagation weights cache</td>
    </tr>
    <tr>
      <td style="text-align: left">RMSProp denom const</td>
      <td style="text-align: center">1E-5</td>
      <td style="padding: 0 35px">Constant preventing zero RMS propagation denominator</td>
    </tr>
    <tr>
      <td style="text-align: left">Leaky ReLU slope</td>
      <td style="text-align: center">1E-2</td>
      <td style="padding: 0 35px">Slope for negative domain of Leaky
ReLU nonlinearities</td>
    </tr>
    <tr>
      <td style="text-align: left"> $\lambda$ </td>
      <td style="text-align: center">1E-2</td>
      <td style="padding: 0 35px">L2 regularization coefficient</td>
    </tr>
    <tr>
      <td style="text-align: left"> $\gamma$ </td>
      <td style="text-align: center">0.99</td>
      <td style="padding: 0 35px">RL discount factor</td>
    </tr>

  </tbody>
</table>

*Table 1.  List of hyperparameter values used within the Pong RL system.*


## Hyperactive and lazy RL agent Pong playing

When training neural networks from scratch and searching for reasonable
hyperparameter values, it is important to have multiple diagnostics to
monitor learning progress.  One basic sanity check is to plot the values of
random weights as the network evolves.  We can then see if learning stalls
at any point.  For example, we observed weights dying in an earlier version
of the neural network pictured in figure 3 which utilized ReLU
nonlinearities.  After replacing ReLU nonlinearities by leaky ReLU
functions, we never again saw weight movement totally cease.  Instead,
random weights typically evolved like those displayed in figure 4 for one
particular training session spanning 2000 epochs of our Pong RL agent.

![PongRL]({{site.url}}/blog/images/pong_policy_learning/montage_weights.jpg)
*Fig. 4. Training evolution of 10 random weights selected from the input,
1st and 2nd hidden layers of the neural network in figure 3.*

Loss function behavior provides another useful check on network training.
Since RL loss can be either positive or negative, we plot the logarithm of
its magnitude as a function of training epoch in figure 5.  The loss
function is obviously noisy.  But the temporally smoothed blue curves
appearing in the figure reassuringly exhibit downward trends.

![PongRL]({{site.url}}/blog/images/pong_policy_learning/montage_loss.jpg)
*Fig. 5. Plots of "hyperactive" [LHS] and "lazy" [RHS] RL agent loss function
magnitudes on a logarithmic scale as functions of training epoch.*

Since deep reinforcement learning starts with a policy network that is
randomly initialized, we expect an agent to take many poor actions early on
in training.  For example, our Pong player begins by losing almost every
match to the built-in AI opponent.  Consequently, the agent's discounted
reward at each time step is $-\gamma^n$ where $n$ denotes the number of ALE
frames until it misses the ball.  But as training progresses, the agent
should learn to favor good moves over bad ones.  So its discounted eventual
reward averaged over all episode time steps should increase.  The
temporally smoothed curves in figure 6 illustrate such learning behavior.

![PongRL]({{site.url}}/blog/images/pong_policy_learning/montage_erewards.jpg)
*Fig. 6. Plots of "hyperactive" [LHS] and "lazy" [RHS] RL agent average
discounted eventual rewards per episode plotted as functions of training
epoch.*

As the RL agent becomes more skilled, matches against the built-in AI grow
longer as it more frequently returns Pong shots.  Game duration
consequently goes up during early network training.  It is worth noting
that the agent can win games against the AI opponent even if its averaged
discounted eventual reward is slightly negative.  In particular, long
volleys which the agent ultimately loses decrease its eventual reward score
more than short winning volleys increase its averaged tally.  Of course,
the agent's overall game score also increases as its pong playing ability
improves.  Eventually, the number of ALE frames per game goes down once the
agent's expertise enables it to rapidly win.  Figures 7 and 8 exhibit all
of these learning indications.

![PongRL]({{site.url}}/blog/images/pong_policy_learning/montage_rewards.jpg)
*Fig. 7. Differences between game scores for "hyperactive" and "lazy" RL
agents and built-in AI plotted as functions of training epoch.  The maximum
score which either player can obtain is 21.*

![PongRL]({{site.url}}/blog/images/pong_policy_learning/montage_frames.jpg)
*Fig. 8. Number of ALE frames per Pong episode for "hyperactive" and "lazy" RL
agents plotted as functions of training epoch.*

Of course, the most compelling demonstration of Pong reinforcement learning
is actual agent play against the built-in AI.  The movie excerpt in figure
9 shows the endgame for episode 3600 which corresponds to epoch 854.87 in
the LHS plots of figures 5 through 8.  Around this point in training, the
agent starts playing Pong slightly better than its opponent.  Indeed, the
agent beats the AI 21 to 20 at the end of the movie in figure 9.  With more
training, the RL agent discovers a systematic defect in the AI's game
playing.  It learns to exploit this weakness and consistently crushes its
opponent after 2000 training epochs.

<a
href="http://www.youtube.com/watch?feature=player_embedded&v=xDoBMdJOYco"
target="_blank"><img src="http://img.youtube.com/vi/xDoBMdJOYco/0.jpg"
alt="IMAGE ALT TEXT HERE" width="700" height="466" border="10" /></a>
*Fig. 9.  Movie excerpt of a "hyperactive" RL agent playing Pong against
the built-in AI near the end of episode 3600.  Click on image above to
start the YouTube video.*

Our deep agent learns to win at Atari Pong, but its paddle movement
exhibits significant high frequency jitter.  The agent's jerky motions are
distracting and seem unnecessary.  So we call such playing "hyperactive".
It's reasonable to ask if some low-pass temporal filtering can be performed
to reduce the agent's jittery motion.

We consequently conducted more experiments with the same policy network
setup as pictured in figure 3.  But we had the environment modify the ALE's
per-frame reward by a small penalty proportional to the agent's
instantaneous acceleration magnitude:

$$ {\rm Modified \> reward} = {\rm ALE \> reward}
 - \alpha \bigl( y[n] - 2 y[n - 1] + y[n-2] \bigr). $$

Here $y[n]$ denotes the paddle's vertical coordinate at the $n^{th}$
timestep.  After some empirical testing, we set the proportionality
coefficient for the paddle acceleration penalty term equal to $\alpha =
0.001$.

Depending upon the policy network's random initialization, the inclusion of
the anti-jitter reward signal sometimes causes the RL agent to become
trapped in state space regions where it never learns to play pong
effectively.  But in other cases, the weak penalty modification clearly
induces the RL agent to play pong more like the way a human does.  In
particular, the agent frequently selects the "no operation" action when the
pong ball is located far away from its paddle.  As the video clip in figure
10 illustrates, the agent exends expends much less paddle energy when
following its new policy rather than its previous hyperactive counterpart.
We describe the more relaxed RL agent which avoids unnecessary paddle
accelerations as a "lazy" Pong player.

<a href="http://www.youtube.com/watch?feature=player_embedded&v=e1Xji8v7MK4"
target="_blank"><img src="http://img.youtube.com/vi/e1Xji8v7MK4/0.jpg"
alt="IMAGE ALT TEXT HERE" width="700" height="466" border="10" /></a>
*Fig. 10.  Movie excerpt of a "lazy" RL agent playing Pong against the
built-in AI near the end of episode 5000.  Click on image above to start
the YouTube video.*

We close by noting that one RL agent trained with the anti-acceleration
reward signal stumbled upon the ultimate lazy player's strategy.  In
particular, it found that by moving a few steps away from its starting
position and staying put, the agent could return every shot sent to its
side of the court by the AI opponent.  This "super lazy" agent could thus
win Pong games 21 to 0 without ever moving after its first few steps!  The
AI opponent did not always oblige the RL agent by shooting balls straight
at its fixed location.  So this "super lazy" learned policy is not as good
as the one corresponding to the reward curve on the RHS of figure 7.  But
it is amusing to watch the movie in figure 11 of this randomly discovered
strategy for playing Pong.

<a href="http://www.youtube.com/watch?feature=player_embedded&v=PsVPIyWaWtU"
target="_blank"><img src="http://img.youtube.com/vi/PsVPIyWaWtU/0.jpg"
alt="IMAGE ALT TEXT HERE" width="700" height="466" border="10" /></a>
*Fig. 11.  Movie excerpt of a "super lazy" RL agent playing Pong against the
built-in AI.  Click on image above to start the YouTube video.*

## References

1.  [J. Schulman, "Deep Reinforcement Learning, Lecture 1: Introduction" UC
Berkeley course CS 294, $\small{(2015)}$](http://rll.berkeley.edu/deeprlcourse-fa15/docs/2015.08.26.Lecture01Intro.pdf)

2.  [J. Schulman, N. Heess, T. Weber and P. Abbeel, "Gradient Estimation
using Stochastic Computation Graphs", arXiv: 1506.05254v3 
$\small{(2016)}$.](https://arxiv.org/pdf/1506.05254v3.pdf)

3.  [M. G. Bellemare, Y. Naddaf, J. Veness and M. Bowling, "The Arcade
Learning Environment: An Evaluation Platform for General Agents", J. AI
Research 47 $\small{(2013)}$
253.](http://www.arcadelearningenvironment.org)

4.  [V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou,
D. Wierstra and M. Riedmiller, "Playing Atari with Deep Reinforcement
Learning" $\small{(2013)}$.](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)

5.  [V. Mnih et al, "Human-Level Control through Deep Reinforcement
Learning", Nature 518 $\small{(2015)}$
529.](http://files.davidqiu.com/research/nature14236.pdf)

6.  [A. Kaparthy, "Deep Reinforcement Learning: Pong from Pixels",
$\small{(2016)}$.]( http://karpathy.github.io/2016/05/31/rl/)

7. [K. He, X. Zhang, S. Ren and J. Sun, "Delving Deep into Rectifiers:
Surpassing Human-Level Performance on ImageNet Classification",
arXiv:1502.01852v1 $\small{(2015)}$.](https://arxiv.org/pdf/1502.01852.pdf)

8. [A.L. Maas, A. Y. Hannun, A.Y. Ng, "Rectifier Nonlinearities Improve
Neural Network Acoustic Models", Proc. 30th ICML $\small{(2013)}$.](https://web.stanford.edu/~awni/papers/relu_hybrid_icml2013_final.pdf)

9. [G. Hinton, "Neural Networks for Machine Learning", Slide 29 of Lecture
6 Coursera notes $\small{(2012)}$.](https://www.coursera.org/learn/neural-networks)
