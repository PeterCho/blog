---
layout: post
title:  "Learning to Play Atari Pong via Policy Gradients"
date:   2016-12-30 12:00:00
categories: Deep Learning
use_math: true
---

## Reinforcement learning vs supervised learning 

Reinforcement Learning $(\rm RL)$ is typically described in terms of an
agent interacting with an *a priori* unknown environment.  The
environment's starting state is first sampled from some underlying
distribution.  The agent observes the state and performs some action.  The
environment then issues a reward whose dependence upon the state and action
is initially unknown and stochastic.  The basic goal in RL is to improve
the agent's policy function which maps from environment state history to
agent action so that the cumulative reward is maximized.

Supervised Learning $(\rm SL)$ may be regarded as a special case of
Reinforcement Learning.  The environment first samples pairs of input
states and corresponding output labels from some distribution.  The agent
observes an input state and makes a prediction for the output label.  The
environment subsequently issues a negative reward which depends upon the
disparity between the predicted and genuine output labels.  The basic goal
in SL is to improve the agent's prediction function which maps from input
state to output label so that the cumulative negative reward is minimized.

It's useful to draw a schematic picture which highlights similarities and
differences between reinforcement and supervised learning.  Figure 1
presents such a diagram adapted from refs [] and [].

![PongRL]({{site.url}}/blog/images/pong_policy_learning/SL_vs_RL.png)
*Fig. 1. Schematic comparison between reinforcement and supervised
learning.  For RL, time-dependent states, actions and rewards are
represented by $s_t$, $a_t$ and $r_t$.  For SL, time-independent input
states, prediction model and output labels are denoted by $x_i$, $h$ and
$y_i$.*

As the figure indicates, reinforcement learning is fundamentally more
complex than supervised learning.  In particular, RL must contend with
several challenges not present in SL:

+  Stateful world problem

   An RL agent interacts with an environment whose later states depend upon
   earlier states as well as earlier agent actions.  In contrast, input states
   in supervised learning problems do not depend upon time nor each other.

+  Reward delay problem

   An RL agent can experience a long delay between the instant when it
   performs some relevant action and the time when the environment issues an
   associated reward signal.  In the interim, the agent generally performs
   other actions which become entangled with the eventual reward.  In
   contrast, reward signals in supervised learning settings are immediate
   and in one-to-one correspondence with agent predictions.  

+  Exploration vs exploitation problem

   An RL agent needs encouragement to search unfamiliar parts of state
   space to avoid getting trapped into locally maximal but globally suboptimal
   performance regions.  In contrast, an SL agent generally observes all input
   states within its training environment.  

+  Step size problem

   An RL agent which works with an initially large step size can be forced
   into some poor-performance region of state space from which it may never
   escape even if its step size is later diminished.  In contrast, an SL agent
   often finds a good local minimum region as its step size is annealed.

Similarities and differences between reinforcement and supervised learning
extend to their loss functions.  It is useful to start by deriving the
cross entropy loss for SL classification.  Given a set of training inputs
$x_i$ and outputs $y_i$ , we let $q$ represent the probability distribution
for the truth data and $p$ denote the probability distribution for the
prediction model.  Since $p$ should be as close as possible to $q$, the
Kullback-Leibler divergence of $p$ from $q$ should be minimal.  The KL
divergence enters into the definition of cross entropy between $q$ and $p$:

$$ H(q,p) = H(q) + D_{KL}(q || p)  $$

$$ -E_q (\log p) = -E_q(\log q) + E_q \bigl(\log(q / p) \bigr) . $$

As the ground-truth entropy $H(q)$ may be regarded as an unimportant
invariant, minimizing $D_{KL}$ with respect to $p$ is equivalent to
minimizing $H(q,p)$.  The cross entropy loss function consequently assumes
the form 

$$ \begin{align}
{\cal{L}}_{SL} & = - \sum_{ {\rm training \, samples} \, x, y} 
        q(x) \log p(y | x) \\
	& = - {1 \over N} \sum_{i = 1}^N \log p(y_i|x_i) \\
\end{align}
$$

In order to transition from supervised to reinforcement learning problems,
we follow figure 1 and substitute state $s_t$ for input $x_i$.  The agent is
assumed to follow a stochastic policy $\pi(a|s)$ which specifies the
probability it will execute action $a$ when the environment is in state
$s$.  We therefore replace SL output label $y_i$ by RL sampled action
$a_t$.


Reinforcement learning also utilizes three functions to quantify expected
cumulative returns given particular environment states and agent actions.
Firstly, the state-value function

$$ V^\pi(s) = E \bigl[ r_0 + r_1 + r_2 + \cdots | s_0 = s \bigr] $$

measures expected future return when the environment starts in state $s$
and follows policy $\pi$.  Secondly, the state-action value function

$$ Q^\pi(s,a) = E \bigl[ r_0 + r_1 + r_2 + \cdots | s_0 = s, a_0 = a \bigr]
$$

measures expected future return given starting environment state $s$ and
agent action $a$.  Finally, the advantage function

$$ A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s) $$

quantifies how much more valuable is the agent's performing action $a$
rather than its expected counterpart coming from policy $\pi$.  

The reinforcement learning loss function is analogous to that in eqn 2 for
supervised learning.  But the advantage function is used to modulate
individual loss terms:

$$ {\cal{L}}_{RL} = - {1 \over T} \sum_{t = 0}^{T-1} A_t \log p(a_t | s_t). $$

Probabilities for good actions with positive advantages are consequently
increased, while probabilities for bad actions with negative advantages are
decreased.

## Deep RL system architecture

![PongRL]({{site.url}}/blog/images/pong_policy_learning/preprocessing.png)
*Fig. XX. Raw ALE frames are converted into 32 x 32 input state vectors
after cropping, downsampling and differencing.*

<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: left">Hyperparameter</th>
      <th style="text-align: center">Value    </th>
      <th style="text-align: left">Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Epoch frames</td>
      <td style="text-align: center">5E4</td>
      <td style="text-align: left">Number of ALE frames per epoch</td>
    </tr>
    <tr>
      <td style="text-align: left">Batch size</td>
      <td style="text-align: center">2E4</td>
      <td style="text-align: left">  Number of training inputs per
gradient descent computation</td>
    </tr>
    <tr>
      <td style="text-align: left">Learning rate</td>
      <td style="text-align: center">1E-3</td>
      <td style="text-align: left">  Learning rate for RMSProp</td>
    </tr>
    <tr>
      <td style="text-align: left">RMSProp decay rate</td>
      <td style="text-align: center">0.9</td>
      <td style="text-align: left">Decay rate for RMSprop weights cache</td>
    </tr>
    <tr>
      <td style="text-align: left">RMSProp denom const</td>
      <td style="text-align: center">1E-5</td>
      <td style="text-align: left">Constant preventing zero RMSProp denominator</td>
    </tr>
    <tr>
      <td style="text-align: left">Leaky ReLU slope</td>
      <td style="text-align: center">1E-2</td>
      <td style="text-align: left">Slope for negative inputs in Leaky
ReLU nonlinearity</td>
    </tr>
    <tr>
      <td style="text-align: left"> $\lambda$ </td>
      <td style="text-align: center">1E-2</td>
      <td style="text-align: left">L2 regularization coefficient</td>
    </tr>
    <tr>
      <td style="text-align: left"> $\gamma$ </td>
      <td style="text-align: center">0.99</td>
      <td style="text-align: left">RL discount factor</td>
    </tr>

  </tbody>
</table>

*Table 1.  List of hyperparameter values used within the Deep RL system.*


## Pong results


![PongRL]({{site.url}}/blog/images/pong_policy_learning/padded_eventual_rewards.jpg)
*Fig. XX. RL agent's eventual reward averaged over an entire episode
plotted as a function of training epoch.  The agent wins against its AI
opponent when its average eventual reward value becomes positive.*



![PongRL]({{site.url}}/blog/images/pong_policy_learning/padded_delta_game_scores.jpg)
*Fig. XX. Difference between game scores for RL agent and built-in AI
plotted as a function of training epoch.  The maximum score which either
player can obtain is 21.*

![PongRL]({{site.url}}/blog/images/pong_policy_learning/padded_frames_history.jpg)
*Fig. XX. Number of ALE frames per Pong episode plotted as a function of
training epoch.*


<a href="http://www.youtube.com/watch?feature=player_embedded&v=5C4_ztYXZZY"
target="_blank"><img src="http://img.youtube.com/vi/5C4_ztYXZZY/0.jpg"
alt="IMAGE ALT TEXT HERE" width="720" height="480" border="10" /></a>
*Fig. XX.  Movie excerpt of Deep RL agent trained via policy gradients playing
against built-in Pong AI.  Click on image above to play video.*


## References

*.  [J. Schulman, "Deep Reinforcement Learning, Lecture 1: Introduction" UC
Berkeley course CS 294, ((2015))](http://rll.berkeley.edu/deeprlcourse-fa15/docs/2015.08.26.Lecture01Intro.pdf)

*.  [J. Schulman, N. Heess, T. Weber and P. Abbeel, "Gradient Estimation
using Stochastic Computation Graphs", arXiv: 1506.05254v3 ((2016)).](https://arxiv.org/pdf/1506.05254v3.pdf)

*.  [M. G. Bellemare, Y. Naddaf, J. Veness and M. Bowling, "The Arcade
Learning Environment: An Evaluation Platform for General Agents", J. AI
Research 47 ((2013)) 253.](http://www.arcadelearningenvironment.org)

*.  [A. Kaparthy, "Deep Reinforcement Learning: Pong from Pixels", ((2016)).](
http://karpathy.github.io/2016/05/31/rl/)

*. [G. Hinton, "Neural Networks for Machine Learning", Slide 29 from
Lecture 6 Coursera notes,
((2012)).](https://www.coursera.org/learn/neural-networks)

*. [K. He, X. Zhang, S. Ren and J. Sun, "Delving Deep into Rectifiers:
Surpassing Human-Level Performance on ImageNet Classification",
arXiv:1502.01852v1 ((2015)).](https://arxiv.org/pdf/1502.01852.pdf)
