---
layout: post
title:  "Learning to Play Atari Pong via Policy Gradients"
date:   2016-12-30 12:00:00
categories: Deep Learning
use_math: true
---

## Supervised learning vs reinforcement learning

![PongRL]({{site.url}}/blog/images/pong_policy_learning/SL_vs_RL.png)
*Fig. 1. Schematic comparison between supervised learning
with reinforcement learning.  Figure adapted from refs [] and [].*


We start by deriving the cross entropy loss function for supervised
learning classification problems.  Given a set of inputs $x_i$ with
corresponding output labels $y_i$ , we let $q$ represent the probability
distribution for the training data and $p$ denote the prediction model's
probability distribution.  Since $p$ should be as close as possible to $q$,
the Kullback-Leibler divergence of $p$ from $q$ should be minimal.  The KL
divergence $D_{KL}(q || p)$ enters into the definition of the cross entropy
between $q$ and $p$:

$$ H(q,p) = H(q) + D_{KL}(q || p) $$

where the entropy $H(q)$ of the ground-truth $q$ distribution may be
regarded as an unimportant invariant.  So minimizing $D_{KL}$ with respect
to $p$ is equivalent to minimizing $H(q,p)$ with respect to $p$.  We may
consequently set the initial loss function as

$$ \begin{align}
\cal{L} & = H(q,p) \\
        & = E_q (-\log p) \\
        & = - \sum_{ {\rm training \, samples} \, x} q(x) \log p(x) \\
	& = \sum_{i = 1}^N {1 \over N} \cdot \bigl[ -\log p(y_i|x_i) \bigr] \\
	& \equiv {1 \over N} \sum_{i = 1}^N \cal{L}_i (y_i|x_i) \\
\end{align}
$$

For reinforcement learning problems, each output label $y_i$ is replaced by
a sampled action $a_i$.  Individual contributions in eqn (5) are also
modulated by "advantage" coefficients which quantify how much more valuable
were the sampled actions than their expected counterparts:

$$ \cal{L}_i = A_i \log p(a_i | x_i). $$


## Deep RL system architecture

![PongRL]({{site.url}}/blog/images/pong_policy_learning/preprocessing.png)
*Fig. XX. Raw ALE frames are converted into 32 x 32 input state vectors
after cropping, downsampling and differencing.*

Here $0 \le \alpha \le 1$ represents a TD learning rate, while $0 \le
\gamma \le 1$ denotes a discount factor for future rewards [9].

## Pong results



![PongRL]({{site.url}}/blog/images/pong_policy_learning/padded_delta_game_scores.jpg)
*Fig. XX. Difference between game scores for RL agent and built-in AI
plotted as a function of training epoch.  The maximum score which either
player can obtain is 21.*

![PongRL]({{site.url}}/blog/images/pong_policy_learning/padded_eventual_rewards.jpg)
*Fig. XX. RL agent's eventual reward averaged over an entire episode
plotted as a function of training epoch.  The agent wins against its AI
opponent when its average eventual reward value becomes positive.*


![PongRL]({{site.url}}/blog/images/pong_policy_learning/padded_frames_history.jpg)
*Fig. XX. Number of ALE frames per Pong episode plotted as a function of
training epoch.*


<a href="http://www.youtube.com/watch?feature=player_embedded&v=5C4_ztYXZZY"
target="_blank"><img src="http://img.youtube.com/vi/5C4_ztYXZZY/0.jpg"
alt="IMAGE ALT TEXT HERE" width="720" height="480" border="10" /></a>
*Fig. XX.  Movie excerpt of Deep RL agent trained via policy gradients playing
against built-in Pong AI.  Click on image above to play video.*


## References

*.  [J. Schulman, "Deep Reinforcement Learning, Lecture 1: Introduction" UC
Berkeley course CS 294, ((2015))](http://rll.berkeley.edu/deeprlcourse-fa15/docs/2015.08.26.Lecture01Intro.pdf)

*.  [J. Schulman, N. Heess, T. Weber & P. Abbeel, "Gradient Estimation
using Stochastic Computation Graphs", arXiv: 1506.05254v3 ((2016)).](https://arxiv.org/pdf/1506.05254v3.pdf)

*.  [A. Kaparthy, "Deep Reinforcement Learning: Pong from Pixels", ((2016)).](
http://karpathy.github.io/2016/05/31/rl/)

*. [G. Hinton, "Neural Networks for Machine Learning", Slide 29 from
Lecture 6 Coursera notes,
((2012)).](https://www.coursera.org/learn/neural-networks)

