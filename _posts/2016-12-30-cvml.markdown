---
layout: post
title:  "Learning to Play Atari Pong via Policy Gradients"
date:   2016-12-30 12:00:00
categories: Deep Learning
use_math: true
---

## Reinforcement learning vs supervised learning 

Reinforcement Learning $(\rm RL)$ is typically described in terms of an
agent interacting with an *a priori* unknown environment.  The
environment's starting state is first sampled from some underlying
distribution.  The agent observes the state and performs some action.  The
environment then issues a reward whose dependence upon the state and action
is initially unknown and stochastic.  The basic goal in RL is to improve
the agent's policy function which maps from environment state history to
agent action so that the cumulative reward is maximized.

Supervised Learning $(\rm SL)$ may be regarded as a special case of
Reinforcement Learning.  The environment first samples pairs of input
states and corresponding output labels from some distribution.  The agent
observes an input state and makes a prediction for the output label.  The
environment subsequently issues a negative reward which depends upon the
disparity between the predicted and genuine output labels.  The basic goal
in SL is to improve the agent's prediction function which maps from input
state to output label so that the cumulative negative reward is minimized.

It's useful to draw a schematic picture which highlights similarities and
differences between reinforcement and supervised learning.  Figure 1
presents such a diagram adapted from refs [] and [].

![PongRL]({{site.url}}/blog/images/pong_policy_learning/SL_vs_RL.png)
*Fig. 1. Schematic comparison between reinforcement and supervised
learning.  For RL, time-dependent states, actions and rewards are
represented by $s_t$, $a_t$ and $r_t$.  For SL, time-independent input
states, prediction model and output labels are denoted by $x_i$, $h$ and
$y_i$.*

As the figure indicates, reinforcement learning is fundamentally more
complex than supervised learning.  In particular, RL must contend with
several challenges not present in SL:

+  Stateful world problem

   An RL agent interacts with an environment whose later states depend upon
   earlier states as well as earlier agent actions.  In contrast, input states
   in supervised learning problems do not depend upon time nor each other.

+  Reward delay problem

   An RL agent can experience a long delay between the instant when it
   performs some relevant action and the time when the environment issues an
   associated reward signal.  In the interim, the agent generally performs
   other actions which become entangled with the eventual reward.  In
   contrast, reward signals in supervised learning settings are immediate
   and in one-to-one correspondence with agent predictions.  

+  Exploration vs exploitation problem

   An RL agent needs encouragement to search unfamiliar parts of state
   space to avoid getting trapped into locally maximal but globally suboptimal
   performance regions.  In contrast, an SL agent generally observes all input
   states within its training environment.  

+  Step size problem

   An RL agent which works with an initially large step size can be forced
   into some poor-performance region of state space from which it may never
   escape even if its step size is later diminished.  In contrast, an SL agent
   often finds a good local minimum region as its step size is annealed.

Similarities and differences between reinforcement and supervised learning
extend to their loss functions.  It is useful to start by deriving the
cross entropy loss for SL classification.  Given a set of training inputs
$x_i$ and outputs $y_i$ , we let $q$ represent the probability distribution
for the truth data and $p$ denote the probability distribution for the
prediction model.  Since $p$ should be as close as possible to $q$, the
Kullback-Leibler divergence of $p$ from $q$ should be minimal.  The KL
divergence enters into the definition of cross entropy between $q$ and $p$:

$$ H(q,p) = H(q) + D_{KL}(q || p)  $$

$$ -E_q (\log p) = -E_q(\log q) + E_q \bigl(\log(q / p) \bigr) . $$

As the ground-truth entropy $H(q)$ may be regarded as an unimportant
invariant, minimizing $D_{KL}$ with respect to $p$ is equivalent to
minimizing $H(q,p)$.  The cross entropy loss function consequently assumes
the form 

$$ \begin{align}
{\cal{L}}_{SL} & = - 
        \sum_{ \substack{ {\rm training} \\ {\rm samples} \, x, y} }
        q(x) \log p(y | x) \\
	& = - {1 \over N} \sum_{i = 1}^N \log p(y_i|x_i) \\
\end{align}
$$

In order to transition from supervised to reinforcement learning problems,
we follow figure 1 and substitute state $s_t$ for input $x_i$.  The agent is
assumed to follow a stochastic policy $\pi(a|s)$ which specifies the
probability it will execute action $a$ when the environment is in state
$s$.  We therefore replace SL output label $y_i$ by RL sampled action
$a_t$.


Reinforcement learning also utilizes three functions to quantify expected
cumulative returns given particular environment states and agent actions.
Firstly, the state-value function

$$ V^\pi(s) = E \bigl[ r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \cdots | 
s_t = s \bigr] $$

measures expected future return when the environment starts in state $s$
and follows policy $\pi$.  The discount factor $0 < \gamma < 1$ appearing
in this expression reduces long term dependencies by downweighting future
rewards.  Secondly, the state-action value function

$$ Q^\pi(s,a) = E \bigl[ r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \cdots | 
s_t = s, a_t = a \bigr]
$$

measures expected future return given starting environment state $s$ and
agent action $a$.  Finally, the advantage function

$$ A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s) $$

quantifies how much more valuable is the agent's performing action $a$
rather than the expected action coming from policy $\pi$.

The reinforcement learning loss function is similar to that in eqn 2 for
supervised learning.  But an advantage function estimator

$$ \hat{A}_t = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \cdots - V^\pi_t $$

is used to modulate its individual terms:

$$ {\cal{L}}_{RL} = - {1 \over T} \sum_{t = 0}^{T-1} \hat{A}_t \log \pi(a_t |
s_t). $$

Probabilities for good actions with positive advantages are consequently
increased by this RL loss function, while bad actions with negative
advantages are discouraged.

It is useful to draw one more analogy between RL and SL when neural
networks are employed as function approximators.  For classification
problems, we can train a network to map inputs onto probability
distributions over output labels.  Similarly for sequential decision making
problems, neural nets may be used to map environment states onto policies
for agent actions.  The resulting stochastic policy $\pi(a|s;\theta)$ then
depends parameterically upon a vector $\theta$ of neural network weights.


## Deep RL system architecture

Games provide good test environments for reinforcement learning.  Following
the release of the Arcade Learning Environment in 2013 [ref], Deep Mind
researchers published several impressive Atari game learning results based
upon Deep Q networks [refs].  After this breakthrough, Atari games in
particular and computer games in general have become popular benchmarks
among RL practioners for developing new algorithms and evaluating their
performance.  In this post, we explore the ability of deep RL methods to
play the basic Atari game Pong.

Our investigation adopts the general approach outlined by Andrej Kaparthy
in his "Pong from Pixels" blog report [ref].  Since Pong screens are so
simple, we can retain their essential information content even after
aggressively cropping and subsampling raw 160 x 210 images down to 32 x 32.
We further follow Kaparthy and work with temporally differenced screens
rather than sets of temporally successive ALE frames.  We also quantize all
differenced pixels values into zeros and ones.  As figure XX illustrates,
the Pong environment state is compressed at each time step into a 1024
binary vector.

![PongRL]({{site.url}}/blog/images/pong_policy_learning/preprocessing.png)
*Fig. XX. Raw ALE frames are converted into 32 x 32 binary state vectors
after cropping, downsampling and differencing.*














<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: left">Hyperparameter</th>
      <th style="text-align: center">Value    </th>
      <th style="text-align: center">Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Epoch frames</td>
      <td style="text-align: center">5E4</td>
      <td style="padding: 0 35px">Number of ALE frames per epoch</td>
    </tr>
    <tr>
      <td style="text-align: left">Batch size</td>
      <td style="text-align: center">2E4</td>
      <td style="padding: 0 35px">  Number of training inputs per
gradient descent computation</td>
    </tr>
    <tr>
      <td style="text-align: left">Learning rate</td>
      <td style="text-align: center">1E-3</td>
      <td style="padding: 0 35px"> Learning rate for RMSProp</td>
    </tr>
    <tr>
      <td style="text-align: left">RMSProp decay rate</td>
      <td style="text-align: center">0.9</td>
      <td style="padding: 0 35px">Decay rate for RMSprop weights cache</td>
    </tr>
    <tr>
      <td style="text-align: left">RMSProp denom const</td>
      <td style="text-align: center">1E-5</td>
      <td style="padding: 0 35px">Constant preventing zero RMSProp denominator</td>
    </tr>
    <tr>
      <td style="text-align: left">Leaky ReLU slope</td>
      <td style="text-align: center">1E-2</td>
      <td style="padding: 0 35px">Slope for negative inputs in Leaky
ReLU nonlinearity</td>
    </tr>
    <tr>
      <td style="text-align: left"> $\lambda$ </td>
      <td style="text-align: center">1E-2</td>
      <td style="padding: 0 35px">L2 regularization coefficient</td>
    </tr>
    <tr>
      <td style="text-align: left"> $\gamma$ </td>
      <td style="text-align: center">0.99</td>
      <td style="padding: 0 35px">RL discount factor</td>
    </tr>

  </tbody>
</table>

*Table 1.  List of hyperparameter values used within the Deep RL system.*


## Pong training results


![PongRL]({{site.url}}/blog/images/pong_policy_learning/padded_eventual_rewards.jpg)
*Fig. XX. RL agent's eventual reward averaged over an entire episode
plotted as a function of training epoch.  The agent wins against its AI
opponent when its average eventual reward value becomes positive.*


![PongRL]({{site.url}}/blog/images/pong_policy_learning/weight_evolution_montage_1K.jpg)
*Fig. XX. Training evolution of 10 random weights selected from the 0th,
1st and 2nd hidden layers of the neural network.*



![PongRL]({{site.url}}/blog/images/pong_policy_learning/padded_delta_game_scores.jpg)
*Fig. XX. Difference between game scores for RL agent and built-in AI
plotted as a function of training epoch.  The maximum score which either
player can obtain is 21.*

![PongRL]({{site.url}}/blog/images/pong_policy_learning/padded_frames_history.jpg)
*Fig. XX. Number of ALE frames per Pong episode plotted as a function of
training epoch.*


<a href="http://www.youtube.com/watch?feature=player_embedded&v=5C4_ztYXZZY"
target="_blank"><img src="http://img.youtube.com/vi/5C4_ztYXZZY/0.jpg"
alt="IMAGE ALT TEXT HERE" width="720" height="480" border="10" /></a>
*Fig. XX.  Movie excerpt of RL agent trained via policy gradients playing
against built-in Pong AI.  Click on image above to play video.*


## References

*.  [J. Schulman, "Deep Reinforcement Learning, Lecture 1: Introduction" UC
Berkeley course CS 294, ((2015))](http://rll.berkeley.edu/deeprlcourse-fa15/docs/2015.08.26.Lecture01Intro.pdf)

*.  [J. Schulman, N. Heess, T. Weber and P. Abbeel, "Gradient Estimation
using Stochastic Computation Graphs", arXiv: 1506.05254v3 ((2016)).](https://arxiv.org/pdf/1506.05254v3.pdf)

*.  [M. G. Bellemare, Y. Naddaf, J. Veness and M. Bowling, "The Arcade
Learning Environment: An Evaluation Platform for General Agents", J. AI
Research 47 ((2013)) 253.](http://www.arcadelearningenvironment.org)

*.  [V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou,
D. Wierstra and M. Riedmiller, "Playing Atari with Deep Reinforcement
Learning" ((2013)).](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)

*.  [V. Mnih et al, "Human-Level Control through Deep Reinforcement
Learning", Nature 518 ((2015)) 529.](http://files.davidqiu.com/research/nature14236.pdf)

*.  [A. Kaparthy, "Deep Reinforcement Learning: Pong from Pixels", ((2016)).](
http://karpathy.github.io/2016/05/31/rl/)

*. [G. Hinton, "Neural Networks for Machine Learning", Slide 29 from
Lecture 6 Coursera notes,
((2012)).](https://www.coursera.org/learn/neural-networks)

*. [K. He, X. Zhang, S. Ren and J. Sun, "Delving Deep into Rectifiers:
Surpassing Human-Level Performance on ImageNet Classification",
arXiv:1502.01852v1 ((2015)).](https://arxiv.org/pdf/1502.01852.pdf)
