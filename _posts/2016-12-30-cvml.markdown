---
layout: post
title:  "Learning to Play Atari Pong via Policy Gradients"
date:   2016-12-30 12:00:00
categories: Deep Learning
use_math: true
---

## Reinforcement learning vs supervised learning 

Reinforcement Learning $(\rm RL)$ is typically described in terms of an
agent interacting with an a priori unknown environment.  The environment's
starting state is first sampled from some underlying distribution.  The
agent observes the state and performs some action.  The environment then
issues a reward whose dependence upon the state and action is initially
unknown and stochastic.  The basic goal in RL is to improve the agent's
policy function mapping from environment state history to agent action so
that the cumulative reward is maximized.

Supervised Learning $(\rm SL)$ may be regarded as a special case of
Reinforcement Learning.  The environment first samples pairs of input
states and corresponding output labels from some distribution.  The agent
observes an input state and makes a prediction for the output label.  The
environment subsequently issues a negative reward which depends upon the
disparity between the predicted and genuine output labels.  The basic goal
in SL is to improve the agent's mapping function from input state to output
label so that the cumulative negative reward is minimized.

It's useful to draw a picture which highlights similarities and differences
between supervised and reinforcement learning.  

![PongRL]({{site.url}}/blog/images/pong_policy_learning/SL_vs_RL.png)
*Fig. 1. Schematic comparison between supervised learning
with reinforcement learning.  Figure adapted from refs [] and [].*

RL is fundamentally more complicated than SL.

*.  Stateful world problem

  An RL agent interacts with an environment whose later states depend upon
earlier states as well as earlier agent actions.  In contrast, input states
in supervised learning problems are independent of time and each other.

*.  Action-reward timing problem

 An RL agent can experience a long delay between the instant when it
 performs some relevant action and the time when the environment issues
 an associated reward signal.  In the interim, the agent generally
 performs many other actions.  In contrast, reward signals in supervised
 learning settings are in one-to-one correspondence with agent predictions.

*.  Exploration vs exploitation problem

 An RL agent needs to be encouraged to search unfamiliar parts of state
space to avoid getting trapped in locally maximal but globally suboptimal
performance regions.  In contrast, an SL agent generally observes all input
states within its training environment.

*.  Step size problem

 An RL agent which works with an initially large step size can be forced
into some poor-performance region of state space from which it may never
escape even if its step size is later diminished.  In contrast, an SL agent
often settles into a reasonable local minimum as its step size is annealed.













We start by deriving the cross entropy loss function for supervised
learning classification problems.  Given a set of inputs $x_i$ with
corresponding output labels $y_i$ , we let $q$ represent the probability
distribution for the training data and $p$ denote the prediction model's
probability distribution.  Since $p$ should be as close as possible to $q$,
the Kullback-Leibler divergence of $p$ from $q$ should be minimal.  The KL
divergence $D_{KL}(q || p)$ enters into the definition of the cross entropy
between $q$ and $p$:

$$ H(q,p) = H(q) + D_{KL}(q || p) $$

where the entropy $H(q)$ of the ground-truth $q$ distribution may be
regarded as an unimportant invariant.  So minimizing $D_{KL}$ with respect
to $p$ is equivalent to minimizing $H(q,p)$ with respect to $p$.  We may
consequently set the initial loss function as

$$ \begin{align}
\cal{L} & = H(q,p) \\
        & = E_q (-\log p) \\
        & = - \sum_{ {\rm training \, samples} \, x} q(x) \log p(x) \\
	& = \sum_{i = 1}^N {1 \over N} \cdot \bigl[ -\log p(y_i|x_i) \bigr] \\
	& \equiv {1 \over N} \sum_{i = 1}^N \cal{L}_i (y_i|x_i) \\
\end{align}
$$

For reinforcement learning problems, each output label $y_i$ is replaced by
a sampled action $a_i$.  Individual contributions in eqn (5) are also
modulated by "advantage" coefficients which quantify how much more valuable
were the sampled actions than their expected counterparts:

$$ \cal{L}_i = A_i \log p(a_i | x_i). $$


## Deep RL system architecture

![PongRL]({{site.url}}/blog/images/pong_policy_learning/preprocessing.png)
*Fig. XX. Raw ALE frames are converted into 32 x 32 input state vectors
after cropping, downsampling and differencing.*




<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: center">Hyperparameter</th>
      <th style="text-align: center">Value</th>
      <th style="text-align: center">Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Batch size</td>
      <td style="text-align: center">2E4</td>
      <td style="text-align: center">Number of training inputs for each
gradient descent computation</td>
    </tr>
    <tr>
      <td style="text-align: center">Learning rate</td>
      <td style="text-align: center">1E-3</td>
      <td style="text-align: center">Learning rate for RMSProp</td>
    </tr>
    <tr>
      <td style="text-align: center">RMSProp decay rate</td>
      <td style="text-align: center">0.9</td>
      <td style="text-align: center">Decay rate for RMSprop weights cache</td>
    </tr>
    <tr>
      <td style="text-align: center">RMSProp denom const</td>
      <td style="text-align: center">1E-5</td>
      <td style="text-align: center">Constant added to squared gradient in
denominator of RMSProp update</td>
    </tr>
    <tr>
      <td style="text-align: center">lambda</td>
      <td style="text-align: center">1E-2</td>
      <td style="text-align: center">L2 regularization coefficient</td>
    </tr>
    <tr>
      <td style="text-align: center">Leaky ReLU slope</td>
      <td style="text-align: center">1E-2</td>
      <td style="text-align: center">Slope for negative inputs in Leaky
ReLU nonlinearity</td>
    </tr>
    <tr>
      <td style="text-align: center">gamma</td>
      <td style="text-align: center">0.99</td>
      <td style="text-align: center">RL discount factor</td>
    </tr>
    <tr>
      <td style="text-align: center">Epoch frames</td>
      <td style="text-align: center">5E4</td>
      <td style="text-align: center">Number of ALE frames per epoch</td>
    </tr>
  </tbody>
</table>

*Table 1.  List of hyperparameter values used in our Deep RL system.*


## Pong results


![PongRL]({{site.url}}/blog/images/pong_policy_learning/padded_eventual_rewards.jpg)
*Fig. XX. RL agent's eventual reward averaged over an entire episode
plotted as a function of training epoch.  The agent wins against its AI
opponent when its average eventual reward value becomes positive.*



![PongRL]({{site.url}}/blog/images/pong_policy_learning/padded_delta_game_scores.jpg)
*Fig. XX. Difference between game scores for RL agent and built-in AI
plotted as a function of training epoch.  The maximum score which either
player can obtain is 21.*

![PongRL]({{site.url}}/blog/images/pong_policy_learning/padded_frames_history.jpg)
*Fig. XX. Number of ALE frames per Pong episode plotted as a function of
training epoch.*


<a href="http://www.youtube.com/watch?feature=player_embedded&v=5C4_ztYXZZY"
target="_blank"><img src="http://img.youtube.com/vi/5C4_ztYXZZY/0.jpg"
alt="IMAGE ALT TEXT HERE" width="720" height="480" border="10" /></a>
*Fig. XX.  Movie excerpt of Deep RL agent trained via policy gradients playing
against built-in Pong AI.  Click on image above to play video.*


## References

*.  [J. Schulman, "Deep Reinforcement Learning, Lecture 1: Introduction" UC
Berkeley course CS 294, ((2015))](http://rll.berkeley.edu/deeprlcourse-fa15/docs/2015.08.26.Lecture01Intro.pdf)

*.  [J. Schulman, N. Heess, T. Weber and P. Abbeel, "Gradient Estimation
using Stochastic Computation Graphs", arXiv: 1506.05254v3 ((2016)).](https://arxiv.org/pdf/1506.05254v3.pdf)

*.  [M. G. Bellemare, Y. Naddaf, J. Veness and M. Bowling, "The Arcade
Learning Environment: An Evaluation Platform for General Agents", J. AI
Research 47 ((2013)) 253.](http://www.arcadelearningenvironment.org)

*.  [A. Kaparthy, "Deep Reinforcement Learning: Pong from Pixels", ((2016)).](
http://karpathy.github.io/2016/05/31/rl/)

*. [G. Hinton, "Neural Networks for Machine Learning", Slide 29 from
Lecture 6 Coursera notes,
((2012)).](https://www.coursera.org/learn/neural-networks)

*.  [Leaky ReLU reference]
