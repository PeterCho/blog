---
layout: post
title:  "Teaching an Agent to Play Atari Breakout via Imitation and 
Reinforcement Learning"
date:   2017-03-26 12:00:00
categories: Deep Learning
use_math: true
---

## An RL trap

In this blog posting, we return to Atari games in the Arcade Learning
Environment $($ALE$)$ [ref] which provide interesting and fun laboratories
for studying deep Reinforcement Learning.  Here we focus upon the game
"Breakout" which seems like a minor variant of "Pong".  Both are one-player
games which involve hitting a moving ball with a paddle.  Points are earned
in Breakout each time the ball strikes a brick within a colored horizontal
array appearing near the top of the game window.  The Breakout player's
goal is simply to strike as many colored bricks as possible with the ball
in order to maximize the total game score.

In our previous Pong blog writeup [], we found that Reinforcement Learning
$($RL$)$ methods based upon policy gradient methods worked well.  So it is
natural to hypothesize the same should be true for Breakout.  But deep RL
training via policy gradients rapidly leads to the agent becoming trapped
in highly-suboptimal local maxima within state space.  One of the worst
strategies learned by the agent is exhibited in the movie of figure 1:

<a href="http://www.youtube.com/watch?feature=player_embedded&v=rEzXFA_58vw"
target="_blank"><center><img align="center" src="http://img.youtube.com/vi/rEzXFA_58vw/0.jpg" width="600" height="400" /></center></a>

*Fig. 1.  Movie of an RL agent playing Atari Breakout after being trained
via conventional Deep Reinforcement Learning.  The agent unsatisfactorily
learns to score a few points by pinning its paddle to the game window's
left wall.  Click on image above to start the YouTube video.*

Since Breakout always starts with the first ball being fired towards the
lower left corner of the game window, the agent learns it can score a few
points by sliding its paddle all the way to the left wall and reamining
fixed there.  But the agent fails to realize that its total score would be
improved if it were to chase the ball after the first return and not remain
pinned against the left wall.

The movie in figure 1 vividly demonstrates the general RL problem
exploration vs exploitation.  An agent needs to be encouraged to search
unfamiliar regions of state space to avoid entrapment in locally maximal
but globally suboptimal performance regions.  Other authors have noted this
wall-pinning malady for Reinforcement Learning of Breakout [].  Total
scores reported in prior policy gradient [] and evolution strategy []
studies of multiple Atari games also exhibit striking dips for the
particular case of Breakout.  The low Breakout scores among otherwise
impressive results for many other Atari games are a clear sign of local
maxima trapping.

To prod the RL agent to explore more of the Breakout state space, we first
incorporated a penalty into the reward function each time the paddle misses
the ball.  We also tried adding a bonus into the reward proportional to the
total number of episode frames in order to encourage long game play.  We
even experimented with rewarding paddle movement with the hope of
preventing the agent from becoming pinned against a game window wall.  But
none of these simple adjustments to the reward function had much positive
effect upon the agent's learned behavior.

We next conducted several reward shaping experiments.  Following Ng et al
[1999], we supplemented the reward function with a bonus potential which
peaked at the game window's horizontal center and fell to zero at the side
walls.  Depending upon the potential's magnitude, the agent did learn to
avoid wall pinning.  But it then tended to confine its movements near the
game window's center region.  So the agent again failed to learn to chase
the ball whenever it approached the bottom of the screen.

After witnessing disappointing agent behavior from hundreds of experiments,
we concluded that policy gradient RL methods for Atari Breakout require
much more aggressive encouragement of state-space exploration.  We
consequently investigated deep imitation learning in which a "student"
agent is rewarded for learning to mimic the behavior of some "teacher".  As
we demonstrate in the next section, this quasi-supervised approach can
successfully train a policy neural network to reasonably play Atari
Breakout.

## Imitation Learning

The basic goal of Imitation Learning $($IL$)$ is to estimate the policy of
some expert based upon samples of its behavior.  Imitation Learning has
been studied by researchers for years, and it is also known in the
literature as Apprenticeship Learning, Learning from Demonstration and
Learning by Watching.  For highly complex problems such as robot arm
grappling or autonomous vehicle driving where humans can supply expert
training data, IL offers a practical means for beginning to learn policies
whose reward functions may be difficult to even formulate [Abbeel & Ng].
Keeping in mind such potentially important real-world applications, we are
motivated to study Imitation Learning within the much simpler context of
Atari Breakout

We begin by constructing a reasonably proficient Breakout player that we
refer to as a "teacher AI".  The teacher first needs to know the kinematics
of the paddle and ball in each ALE frame.  The paddle's instantaneous
location ${\bf p}(t)$ is readily extracted from the center-of-mass of
non-wall pixels in the last row of the Breakout game window.  The ball's
position ${\bf b}(t)$ may similarly be recovered by calculating the
centroid of the difference between two consecutive frames.  We take the
ball's velocity to equal ${\bf v}(t-1) = 0.5 \bigl[ {\bf b}(t) - {\bf
b}(t-2) \bigr]$.  Though this expression is delayed in time, it typically
encodes the correct instantaneous two-dimensional velocity since the ball
moves in straight lines between occasional bounces.

When the ball travels vertically upwards in the game window, we choose to
keep the teacher's paddle stationary.  When the ball heads downwards, it is
straightforward to predict the ball's future location ${\bf b}(t_{\rm last
\, row})$ when it will reach the last row even if it bounces off a wall on
the way down.  The difference $\Delta(t) = {\bf b}(t_{\rm last \, row}) -
{\bf p}(t)$ between the ball's predicted and the paddle's current pixel
positions in the last row then guides the teacher's stochastic policy.  The
AI responds to large positive $($negative$)$ values for $\Delta$ by moving
the paddle to the right $($left$)$, while it executes no operation if the
magnitude of $\Delta$ is close to zero.  As the movie in figure 2
illustrates, this simple strategy enables the teacher AI to sustain long
rallies, break through the wall of colored bricks and rack up a large
score.

<a href="http://www.youtube.com/watch?feature=player_embedded&v=GkygRAtxcss"
target="_blank"><center><img src="http://img.youtube.com/vi/GkygRAtxcss/0.jpg"
width="600" height="400" /></center></a>
*Fig. 2.  Movie of the "teacher AI" playing Breakout.  Click
on image above to start the YouTube video.*

With a decent teacher demonstrator in hand, we can investigate how well a
"student" agent learns to mimic its behavior.  We employ deep supervised
learning to develop a map from input states to output student actions.  In
order to reduce the size of the student's policy network, raw ALE frames
are reduced to sparse vectors following the image cropping, downsampling
and temporal differencing steps pictured in figure 3.  After trying a few
different vector quantization schemes, we further simplify the state vector
by resetting all its non-zero values to unity.

![Breakout]({{site.url}}/blog/images/breakout/preprocessing.jpg)
*Fig. 3. Raw ALE frames are cropped to remove irrelevant pixel content and
downsampled by a factor of two in both horizontal and vertical directions.
Consecutive pairs of subsampled frames are then temporally differenced to
produce $72 \times 80=5760$ binary state vectors.*

The binary state vectors become inputs to a fully-connected network whose
architecture is schematically illustrated in figure 4.  To our surprise, we
found piecewise-linear approximations to tahn functions work well as node
activations.  Such hybrid nonlinearities run at the same speed as ReLU or
leaky ReLU thresholding, yet they lead to better game play.  We initialize
the neural network's 6109 biases to zero, and we Xavier initialize its
754,048 weights.  During training, 50% dropout is performed on hidden layer
nodes to prevent overfitting [ref].  L2-regularization is also employed to
discourage weight values from diverging.

![Breakout]({{site.url}}/blog/images/breakout/BreakoutNetwork.jpg)
*Fig. 4. Schematic diagram of student policy network which maps input
Breakout states onto output paddle actions.*

Various supervised learning approaches may be considered for training the
student agent to mimic its teacher AI.  Since imitation will never be
perfect, the student will encounter input states not observed by its expert
mentor.  So we avoid training the student network to classify input states
and output actions coming from just the teacher's policy.  Instead, the
student must follow its own policy and receive teacher feedback at each
timestep as to how well it responded.

We experimented with several different reward schemes that encourage
Imitation Learning.  We settled upon the student agent earning a $+1$
reward whenever its policy action for a particular ALE frame matches that
of the teacher AI.  Otherwise, it receives a penalty whose magnitude grows
with the difference $\Delta(t) = {\bf b}(t_{\rm last \, row}) - {\bf p}(t)$
between the predicted future law-row ball and the paddle's current pixel
positions.  We took the penalty growth to be approximately linear for $0
\le |\Delta| \le 12$ and afterwards flat for $|\Delta| > 12$.  Since the
student receives supervised feedback at each time step, we set the RL
discount factor $\gamma = 0$.

State, action and reward tuples are computed and stored within a replay
memory for batches of 20K ALE frames.  Inspired by the priority queue ideas
of ref [], we also calculated replay probabilities for each stored sample.
Inputs states for which the ball is located near the bottom of the game
window are more important than those for which it is located near the top.
Similarly, states wherein the ball moves upwards in the game window are
require less agent attention than those for which the ball moves downwards.
Heuristically assigned replay probabilities encode the stored states'
relative importance.

Once the replay memory is filled, the saved tuples are sampled in random
order according to their replay probabilities which typically selected
O(8K) from the 20K batch members.  Gradient descent is subsequently
performed via RMS propagation [Hinton ref], and student policy network
weights are updated. The batch of training samples is discarded, and the
next set of 20K tuples is collected.

Our IL system for teaching the student agent to play Breakout depends upon
a set of hyperparameters whose empirically determined values are summarized
in Table 1.  As for our previous RL investigations, we choose to implement
the system's neural network in own C++ code rather than utilize an existing
deep learning software package.  The neural network is sufficiently compact
that it finishes training overnight when run on a modern CPU.

<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: left">Hyperparameter</th>
      <th style="text-align: center">Value    </th>
      <th style="text-align: center">Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Epoch frames</td>
      <td style="text-align: center">5E4</td>
      <td style="padding: 0 35px">Number of ALE frames per epoch</td>
    </tr>
    <tr>
      <td style="text-align: left">Batch size</td>
      <td style="text-align: center">2E4</td>
      <td style="padding: 0 35px">  Number of training inputs per
gradient descent computation</td>
    </tr>
    <tr>
      <td style="text-align: left">Base learning rate</td>
      <td style="text-align: center">3E-3</td>
      <td style="padding: 0 35px"> Learning rate for RMS propagation</td>
    </tr>
    <tr>
      <td style="text-align: left">RMSProp decay rate</td>
      <td style="text-align: center">0.9</td>
      <td style="padding: 0 35px">Decay rate for RMS propagation weights cache</td>
    </tr>
    <tr>
      <td style="text-align: left">RMSProp denom const</td>
      <td style="text-align: center">1E-5</td>
      <td style="padding: 0 35px">Constant preventing zero RMS propagation denominator</td>
    </tr>
    <tr>
      <td style="text-align: left">$p_{\rm dropout}$</td>
      <td style="text-align: center">0.5</td>
      <td style="padding: 0 35px">Hidden layer dropout probability</td>
    </tr>
    <tr>
      <td style="text-align: left"> $\lambda$ </td>
      <td style="text-align: center">1E-3</td>
      <td style="padding: 0 35px">L2 regularization coefficient</td>
    </tr>
    <tr>
      <td style="text-align: left"> $\gamma$ </td>
      <td style="text-align: center">0</td>
      <td style="padding: 0 35px">RL discount factor</td>
    </tr>

  </tbody>
</table>

*Table 1.  List of hyperparameter values used within IL system to train a
"student" agent to mimic a "teacher" AI.*


## RL refinement of imitative game play


![Breakout]({{site.url}}/blog/images/breakout/zeroth_layer_weights.png)
*Fig. XX.  Trained values for 4 of the 128 weights within the first hidden
layer of the policy network.  Bright warm [cool] colors correspond to
positive [negative] weight values.  Dark colors indicate weight
coefficients close to zero.*

<a href="http://www.youtube.com/watch?feature=player_embedded&v=uBwWALDdSCA"
target="_blank"><center><img src="http://img.youtube.com/vi/uBwWALDdSCA/0.jpg" width="600" height="400"/></center></a> 
*Fig. XX.  Movie of student agent playing Breakout after its policy network
is refined by conventional deep Reinforcement Learning.  Click on image
above to start the YouTube video.*

![Breakout]({{site.url}}/blog/images/breakout/montage_frames_scores.jpg)
*Fig. XX.  Total number of ALE frames and episode scores plotted for 50 test
Atari breakout games.  Solid green, blue and red curves correspond to
naive, imitating and refined RL agents.  Dashed horizontal lines illustrate
median values for total frame numbers and game scores.*



## References

*.  [M. G. Bellemare, Y. Naddaf, J. Veness and M. Bowling, "The Arcade
Learning Environment: An Evaluation Platform for General Agents", J. AI
Research 47 $\small{(2013)}$ 253.](http://www.arcadelearningenvironment.org)

*.  [See "Teaching a Deep RL Agent to Lazily Win at Atari Pong" post in this
blog series,  $\small{(2016)}$.](http://petercho.github.io/blog/)

*.  [J. Schulman, "Exploration" lecture notes $\small{(2015)}$.](
http://rll.berkeley.edu/deeprlcourse-fa15/docs/2015.10.07.Exploration.pdf)

*.  [J. Schulman, S. Levine, P. Moritz, M. Jordan and P. Abbeel, "Trust
Region Policy Optimization," Proceedings of the 31st International
Conference on Machine Learning
$\small{(2015)}$.](https://arxiv.org/pdf/1502.05477.pdf)

*.  [T. Salimans, J. Ho, X. Chen and I. Sutskever, "Evolution Strategies as
a Scalable Alternative to Reinforcement Learning," arXiv:1703.03864
$\small{(2017)}$.](https://arxiv.org/pdf/1703.03864.pdf)

*.  [A.Y. Ng, D. Harada and S. Russell, "Policy Invariance Under Reward
Transformations: Theory and Application to Reward Shaping," Proceedings of
the 16th international Conference on Machine Learning
$\small{(1999)}$.](http://ai.stanford.edu/~ang/papers/shaping-icml99.pdf)

*.  [P. Abbeel and A.Y. Ng, "Apprenticeship Learning via Inverse
Reinforcement Learning," Proceedings of the 21st International Conference on
Machine Learning $\small{(2004)}$.](http://ai.stanford.edu/~ang/papers/icml04-apprentice.pdf)

*.  [N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever and
R. Salakhutdinov, "Dropout: A Simple Way to Prevent Neural Networks from
Overfitting", J. Machine Learning Research $\small{(2014)}$
1929.](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)

*. [T. Schaul, J. Quan, I. Antonoglou and D. Silver, "Prioritized
Experience Replay," International Conference on Learning Representations
$\small(2016)$.](https://arxiv.org/pdf/1511.05952.pdf)

*. [G. Hinton, "Neural Networks for Machine Learning", Slide 29 of Lecture
6 Coursera notes $\small{(2012)}$.](https://www.coursera.org/learn/neural-networks)

