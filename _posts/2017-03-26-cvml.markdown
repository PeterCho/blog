---
layout: post
title:  "Teaching an Agent to Play Atari Breakout via Imitation and 
Reinforcement Learning"
date:   2017-03-26 12:00:00
categories: Deep Learning
use_math: true
---

## An RL trap

In this blog posting, we return to working with Atari games within the
Arcade Learning Environment $($ALE$)$ [1] that provide interesting and fun
laboratories for studying deep Reinforcement Learning.  Here we focus upon
"Breakout" which superficially seems like a minor variant of "Pong".  Both
are single-player games involving a paddle hitting a moving ball.  Points
are earned in Breakout each time the ball strikes a colored brick within 6
rows near the top of the game window.  The player's goal is to strike as
many colored bricks as possible with the ball in order to maximize the
total game score.

In our previous Pong blog writeup [2], we found that Reinforcement Learning
$($RL$)$ methods based upon policy gradient methods yielded highly
effective agent behavior.  So it is natural to hypothesize the same should
be true for Breakout.  But deep RL training via policy gradients instead
leads the agent to become trapped in poor local maxima of state space.  One
of the worst strategies learned by the agent is exhibited in the movie of
figure 1:

<a href="http://www.youtube.com/watch?feature=player_embedded&v=rEzXFA_58vw"
target="_blank"><center><img align="center" src="http://img.youtube.com/vi/rEzXFA_58vw/0.jpg" width="600" height="400" /></center></a>

*Fig. 1.  Movie of an RL agent playing Atari Breakout after being trained
via conventional Deep Reinforcement Learning.  The agent unsatisfactorily
learns to score a few points by pinning its paddle to the game window's
left wall.  Click on image above to start the YouTube video.*

Since Breakout always starts with the first ball being fired towards the
lower left corner of the game window, the agent learns it can score a few
points by sliding its paddle all the way to the left wall and staying
there.  But the agent fails to realize that its total score could be
greatly improved if it were to chase the ball after the first return and
not remain pinned against the left wall.

The movie in figure 1 vividly demonstrates the general problem of
exploration vs exploitation in Reinforcement Learning.  An agent usually
needs encouragement to search unfamiliar regions of state space and avoid
entrapment in locally maximal but globally suboptimal performance zones.
For the particular case of Breakout, other authors have previously noted
the wall-pinning malady [3].  Total scores reported in prior policy
gradient [4] and evolution strategy [5] studies of multiple Atari games
also exhibit striking dips for Breakout.  Low scores for Breakout among
otherwise impressive Atari game results are a strong indication of local
maxima trapping.

To prod the RL agent to explore more of the Breakout state space, we first
incorporated a penalty into the reward function each time the paddle missed
the ball.  We also tried adding a bonus proportional to the total number of
episode frames in order to encourage long game play.  We even experimented
with rewarding non-zero paddle movement in the hopes of preventing paddle
pinning.  But none of these simple adjustments to the reward function had
much positive impact upon the agent's learned behavior.

We next conducted several reward shaping experiments.  Following Ng et al
[6], we supplemented the reward function with a potential that peaked at
the game window's center and fell to zero at the side walls.  Depending
upon the potential function's magnitude, the agent did learn to avoid wall
pinning.  But it then tended to confine its movements near the game
window's center region.  So the agent again failed to realize that chasing
the ball whereever it approaches the bottom of the game window is basically
a good idea.

After observing lousy agent behavior in hundreds of experiments, we were
forced to conclude that policy gradient methods for Atari Breakout require
much more aggressive encouragement of state-space exploration.  We
consequently turned to deep Imitation Learning in which a "student" agent
is rewarded for mimicing the behavior of some "teacher".  As we shall see
in the next section, this quasi-supervised approach can successfully train
a policy neural network to reasonably play Atari Breakout.

## Imitation Learning

The basic goal of Imitation Learning $($IL$)$ is to estimate the policy of
some expert based upon samples of its behavior.  Imitation Learning has
been studied by researchers for years, and it is also known as
Apprenticeship Learning, Learning from Demonstration and Learning by
Watching in the literature.  For highly complex problems such as robot arm
grappling or autonomous vehicle driving where expert training data can be
supplied by humans, IL offers a practical means for learning policies whose
reward functions may be difficult to even formulate [7].  Keeping in mind
such long-range applications, we are motivated to study Imitation Learning
within the much simpler context of Atari Breakout

We begin by constructing a reasonably proficient Breakout player that we
refer to as a "teacher AI".  The teacher first needs to know the kinematics
of the paddle and ball in each ALE frame.  The paddle's instantaneous
location ${\bf p}(t)$ is readily extracted from the center-of-mass of
non-wall pixels in the last row of the Breakout game window.  The ball's
position ${\bf b}(t)$ may similarly be recovered by calculating the
centroid of the difference between two consecutive frames with excised last
rows.  We take the ball's velocity to equal ${\bf v}(t-1) = 0.5 \bigl[ {\bf
b}(t) - {\bf b}(t-2) \bigr]$.  Though this expression is delayed in time,
it typically encodes the correct instantaneous two-dimensional velocity as
the ball moves in straight lines between occasional bounces.

When the ball travels vertically upwards in the game window, we choose to
keep the teacher's paddle stationary.  When the ball heads downwards, it is
straightforward to predict the ball's future location ${\bf b}(t_{\rm last
\, row})$ when it will reach the last row even if it bounces off a wall on
the way down.  The difference $\Delta(t) = {\bf b}(t_{\rm last \, row}) -
{\bf p}(t)$ between the ball's predicted and the paddle's current pixel
positions in the last row guides the teacher's stochastic policy.  The AI
responds to large positive $($negative$)$ values for $\Delta$ by moving the
paddle to the right $($left$)$, while it executes no operation when
$|\Delta| \simeq 0$.  As the movie in figure 2 illustrates, this simple
strategy enables the teacher AI to sustain long rallies, break through the
wall of colored bricks and rack up a large game score.

<a href="http://www.youtube.com/watch?feature=player_embedded&v=GkygRAtxcss"
target="_blank"><center><img src="http://img.youtube.com/vi/GkygRAtxcss/0.jpg"
width="600" height="400" /></center></a>
*Fig. 2.  Movie of the "teacher AI" playing Breakout.  Click
on image above to start the YouTube video.*

With a decent demonstrator in hand, we can investigate how well a "student"
agent learns to mimic its behavior.  We employ deep supervised learning to
develop a map from input states to output paddle actions.  In order to
reduce the size of the student's policy network, raw ALE frames are
transformed into sparse vectors following the image cropping, downsampling
and temporal differencing steps pictured in figure 3.  After trying a few
different vector quantization schemes, we further simplify the state vector
by resetting all its non-zero values to unity.

![Breakout]({{site.url}}/blog/images/breakout/preprocessing.jpg)
*Fig. 3. Raw ALE frames are cropped to remove irrelevant pixel content and
downsampled by a factor of two in both horizontal and vertical directions.
Consecutive pairs of subsampled frames are then temporally differenced to
produce $72 \times 80=5760$ state vectors.*

Binary state vectors become inputs to a fully-connected network whose
architecture is schematically illustrated in figure 4.  To our surprise, we
found piecewise-linear approximations to tahn functions work well as node
activations.  Such hybrid nonlinearities run at the same speed as ReLU or
leaky ReLU thresholding, yet they lead to better game play.  We initialize
the neural network's 6109 biases to zero, and we Xavier initialize its
754,048 weights.  During training, 50% dropout is performed on hidden layer
nodes to prevent overfitting [8].  L2-regularization is also employed to
discourage weight values from diverging.

![Breakout]({{site.url}}/blog/images/breakout/BreakoutNetwork.jpg)
*Fig. 4. Schematic diagram of the IL policy network which maps input
Breakout states onto output paddle actions.*

Various supervised learning approaches may be considered for training the
student agent to mimic the teacher AI.  Since imitation is imperfect, the
student encounters input states not observed by its expert mentor.  So we
avoid training the student network to classify input states and output
actions coming from just the teacher's policy.  Instead, the student must
follow its own policy and receive teacher feedback at each timestep as to
how well it performed.

We experimented with several different reward schemes that promote
Imitation Learning.  We settled upon granting a $+1$ reward whenever the
student's policy action for a particular ALE frame matches that of the
teacher.  Otherwise, the student agent receives a penalty whose magnitude
grows with the difference $\Delta(t) = {\bf b}(t_{\rm last \, row}) - {\bf
p}(t)$ between the predicted future last-row ball and the paddle's current
pixel positions.  We took the penalty growth to be approximately linear for
$0 \le |\Delta| \le 12$ and flat for $|\Delta| > 12$.  Since the student
receives supervised feedback at each time step, the RL discount factor
$\gamma$ is set to zero.

State, action and reward tuples are stored within a replay memory for
batches of 20K ALE frames.  Inspired by the priority experience ideas of
[9], we also assigned replay probabilities to each stored sample.  Input
states for which the ball is located near the bottom of the game window are
more important than those for which it is located near the top.  Similarly,
states where the ball moves upwards in the game window require less agent
attention than those for where the ball moves downwards.  The replay
probabilities heuristically encode stored states' relative importance.

When the replay memory is filled, the saved tuples are selected in random
order according to their replay probabilities.  Gradient descent is
performed via RMS propagation [10] on the $O($8K$)$ tuples typically
sampled from the 20K batch members.  Once the student policy network's
weights and biases are updated, the current batch of training samples is
discarded.  A new set of 20K tuples is subsequently collected.

Our IL system for teaching the student agent to play Breakout depends upon
multiple hyperparameters whose manually optimized values are summarized in
Table 1.  As for our previous RL investigations, we implement the system's
neural network in own C++ code rather than utilize an existing deep
learning software package.  The neural network is sufficiently compact that
it finishes training overnight when run on a modern CPU.

<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: left">Hyperparameter</th>
      <th style="text-align: center">Value    </th>
      <th style="text-align: center">Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Epoch frames</td>
      <td style="text-align: center">5E4</td>
      <td style="padding: 0 35px">Number of ALE frames per epoch</td>
    </tr>
    <tr>
      <td style="text-align: left">Batch size</td>
      <td style="text-align: center">2E4</td>
      <td style="padding: 0 35px">  Number of training inputs per
gradient descent computation</td>
    </tr>
    <tr>
      <td style="text-align: left">Base learning rate</td>
      <td style="text-align: center">3E-3</td>
      <td style="padding: 0 35px"> Learning rate for RMS propagation</td>
    </tr>
    <tr>
      <td style="text-align: left">RMSProp decay rate</td>
      <td style="text-align: center">0.9</td>
      <td style="padding: 0 35px">Decay rate for RMS propagation weights cache</td>
    </tr>
    <tr>
      <td style="text-align: left">RMSProp denom const</td>
      <td style="text-align: center">1E-5</td>
      <td style="padding: 0 35px">Constant preventing zero RMS propagation denominator</td>
    </tr>
    <tr>
      <td style="text-align: left">$p_{\rm dropout}$</td>
      <td style="text-align: center">0.5</td>
      <td style="padding: 0 35px">Hidden layer dropout probability</td>
    </tr>
    <tr>
      <td style="text-align: left"> $\lambda$ </td>
      <td style="text-align: center">1E-3</td>
      <td style="padding: 0 35px">L2 regularization coefficient</td>
    </tr>
    <tr>
      <td style="text-align: left"> $\gamma$ </td>
      <td style="text-align: center">0</td>
      <td style="padding: 0 35px">RL discount factor</td>
    </tr>

  </tbody>
</table>

*Table 1.  List of hyperparameters used within the IL system to train a
"student" agent to mimic a "teacher" AI.*

As the student agent learns to mimic the teacher AI, we expect the
frequency for which its actions match those of the expert player to
increase over time.  Similarly, the difference $|\Delta|$ between the
student's and teacher's positioning of the paddle ought to diminish.  The
two training curves in figure 5 exhibit both trends.  The matching action
fraction slowly rises with training epoch, while the $|\Delta|$ curve
slowly falls.  After 200 epochs, 55% of the student's actions match on
average those of its teacher, while the mean difference between their
paddle locations equals 13 pixels.  The student learns to mimic its
teacher's gross movements.  But as we previously saw for policy networks
trained to play Atari Pong [2], the student's actions exhibit
high-frequency jitter while the expert's do not.  Such jittering prevents
the matching fraction from nearly approaching unity and $|\Delta|$ from
closely approaching zero.

![Breakout]({{site.url}}/blog/images/breakout/IL_training_results.jpg)
*Fig. 5. Instantaneous $($red$)$ and smoothed $($blue$)$ Imitation Learning
training results for the student agent.  LHS: Fraction of student agent's
actions which match those of the teacher AI.  RHS: Difference $|\Delta|$
measured in pixels between the ball's predicted and the paddle's current
pixel positions within the last row of the game window.*

Given that a conventional RL agent trained to play Breakout almost always
falls into a local maximum trap, it is reassuring to observe the IL student
does not suffer this same fate.  Probability density distributions for the
paddle's horizontal position are plotted in figure 6 for the conventional
RL and student IL agents at two representative epoch snapshots.  The
density for the former is strongly peaked near the origin indicating that
its paddle is pinned against the game window's left wall.  In contrast, the
distribution for the latter is spread out over the paddle's entire range of
motion.  So training an agent via Imitation Learning enables it to start
reasonably playing Atari Breakout.

![Breakout]({{site.url}}/blog/images/breakout/padded_lhspin_vs_imit.jpg)
*Fig. 6. Two probability density snapshots for the paddle's horizontal
position measured in pixels.  The green curve illustrates a conventional RL
agent being trapped by a local maximum with its paddle pinned to the left
wall.  The blue curve indicates the IL agent learns to move everywhere as
it anticipates the ball's bottom row position.*

## RL refinement of imitative game play

The exploration vs exploitation problem represents a significant challenge
in many Reinforcement Learning settings.  Starting from a policy network
seeded with noise, a deep RL system can become stuck in some
poor-performance region of state space from which it never escapes.  But if
the initial policy network already exhibits reasonable behavior, subsequent
Reinforcement Learning may improve its performance.  Such refinement is
similar in spirit to network finetuning which is commonly performed for
supervised image classification.  So we investigate here whether further
training via Reinforcement Learning can improve the student agent's game
play.

We set the RL reward for each Breakout frame equal to that returned by the
Arcade Learning Environment which is nonzero only immediately after the
ball hits a colored brick.  Adopting an RL discount factor of $\gamma =
0.95$ propagates positive environmental feedback to approximately
$1/(1-\gamma) = 20$ frames before a brick striking event.  We also
introduce a $-50$ penalty into the RL reward function whenever the paddle
misses the ball.  But the missed-ball penalty is regarded as a singular
temporal event and therefore not discounted backwards in time by $\gamma$.

Taking all other hyperparameter values in the RL system to equal those for
the IL system, we retrain the agent's policy network.  We then test several
different snapshots of the refined policy on an independently randomized
set of Breakout games.  On the left side of figure 7, total durations of 50
test episodes measured in ALE frames are plotted for the agent's initial IL
and optimally refined RL policies.  Total game scores for the same 50 test
episodes are displayed on the figure's right side.  Though the total game
durations and scores exhibit significant random fluctuations, their median
values depicted by dashed horizontal lines in the graphs quantitatively
demonstrate that RL finetuning improves the agent's performance.

![Breakout]({{site.url}}/blog/images/breakout/montage_frames_scores.jpg)
*Fig. 7.  Total number of ALE frames and episode scores for 50 test Atari
breakout games.  Solid green, blue and red curves correspond to
conventional, imitative and refined RL agents.  Dashed horizontal lines
illustrate median values for total frame numbers and game scores.*

The agent's qualitative game playing abilities and limitations are apparent
within the final movie of figure 8.  It generally anticipates the gross
location of the ball in the game window's last row.  The agent also
sustains rallies involving multiple bounces at low glancing angles.  But
its paddle still exhibits high frequency jitter which sometimes causes it
to miss a passing ball.  And we have not observed the agent following the
particular policy used to generate the movie break through all six layers
of colored bricks.  So although the refined agent's performance is
quantitatively and qualitatively much superior to that of the first
conventional RL agent which got trapped in a bad state space region, it
still has much room left for future improvement.

<a
href="http://www.youtube.com/watch?feature=player_embedded&v=uBwWALDdSCA"
target="_blank"><center><img
src="http://img.youtube.com/vi/uBwWALDdSCA/0.jpg" width="600"
height="400"/></center></a> *Fig. 8.  Student agent playing Breakout after
its policy network is refined by deep Reinforcement Learning.  Click on
image above to start the YouTube video.*

## References

1.  [M. G. Bellemare, Y. Naddaf, J. Veness and M. Bowling, "The Arcade
Learning Environment: An Evaluation Platform for General Agents", J. AI
Research 47 $\small{(2013)}$ 253.](http://www.arcadelearningenvironment.org)

2.  [See "Teaching a Deep RL Agent to Lazily Win at Atari Pong" post in this
blog series,  $\small{(2016)}$.](http://petercho.github.io/blog/)

3.  [J. Schulman, "Exploration" lecture notes $\small{(2015)}$.](
http://rll.berkeley.edu/deeprlcourse-fa15/docs/2015.10.07.Exploration.pdf)

4.  [J. Schulman, S. Levine, P. Moritz, M. Jordan and P. Abbeel, "Trust
Region Policy Optimization," Proceedings of the 31st International
Conference on Machine Learning
$\small{(2015)}$.](https://arxiv.org/pdf/1502.05477.pdf)

5.  [T. Salimans, J. Ho, X. Chen and I. Sutskever, "Evolution Strategies as
a Scalable Alternative to Reinforcement Learning," arXiv:1703.03864
$\small{(2017)}$.](https://arxiv.org/pdf/1703.03864.pdf)

6.  [A.Y. Ng, D. Harada and S. Russell, "Policy Invariance Under Reward
Transformations: Theory and Application to Reward Shaping," Proceedings of
the 16th international Conference on Machine Learning
$\small{(1999)}$.](http://ai.stanford.edu/~ang/papers/shaping-icml99.pdf)

7.  [P. Abbeel and A.Y. Ng, "Apprenticeship Learning via Inverse
Reinforcement Learning," Proceedings of the 21st International Conference on
Machine Learning $\small{(2004)}$.](http://ai.stanford.edu/~ang/papers/icml04-apprentice.pdf)

8.  [N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever and
R. Salakhutdinov, "Dropout: A Simple Way to Prevent Neural Networks from
Overfitting", J. Machine Learning Research $\small{(2014)}$
1929.](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)

9. [T. Schaul, J. Quan, I. Antonoglou and D. Silver, "Prioritized
Experience Replay," International Conference on Learning Representations
$\small(2016)$.](https://arxiv.org/pdf/1511.05952.pdf)

10. [G. Hinton, "Neural Networks for Machine Learning", Slide 29 of Lecture
6 Coursera notes
$\small{(2012)}$.](https://www.coursera.org/learn/neural-networks)

