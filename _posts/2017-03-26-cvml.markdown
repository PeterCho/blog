---
layout: post
title:  "Teaching an Agent to Play Atari Breakout via Imitative
Reinforcement Learning"
date:   2017-03-26 12:00:00
categories: Deep Learning
use_math: true
---

## Conventional RL trapping

In this blog posting, we return to Atari games which provide interesting
and fun laboratories for studying deep Reinforcement Learning $($RL$)$.
Here we focus upon the game "Breakout" which seems like a minor variant of
"Pong".  Both are one-player games which involve hitting a moving ball with
a paddle.  Points are earned in Breakout each time the ball strikes a brick
within a colored horizontal array appearing near the top of the game
window.  The Breakout player's goal is simply to strike as many colored
bricks as possible with the ball in order to maximize the total game score.

In our previous Pong blog writeup [], we found that RL methods based upon
policy gradient methods worked well.  So it is natural to hypothesize the
same should be true for Breakout.  But deep RL training via policy
gradients rapidly leads to the agent becoming trapped in highly-suboptimal
local maxima within state space.  One of the worst strategies learned by
the agent is exhibited in the movie of figure 1:

<a href="http://www.youtube.com/watch?feature=player_embedded&v=rEzXFA_58vw"
target="_blank"><center><img align="center" src="http://img.youtube.com/vi/rEzXFA_58vw/0.jpg" width="600" height="400" /></center></a>

*Fig. 1.  Movie of an RL agent playing Atari Breakout after being trained
via conventional Deep Reinforcement Learning.  The agent unsatisfactorily
learns to score a few points by pinning its paddle to the game window's
left wall.  Click on image above to start the YouTube video.*

Since Breakout always starts with the first ball being fired towards the
lower left corner of the game window, the agent learns it can score a few
points by sliding its paddle all the way to the left wall and reamining
fixed there.  But the agent fails to realize that its total score would be
improved if it were to chase the ball after the first return and not remain
pinned against the left wall.

The movie in figure 1 vividly demonstrates the general RL problem
exploration vs exploitation.  An agent needs to be encouraged to search
unfamiliar regions of state space to avoid entrapment in locally maximal
but globally suboptimal performance regions.  Other authors have noted this
wall-pinning malady for Reinforcement Learning of Breakout [].  Total
scores reported in prior policy gradient [] and evolution strategy []
studies of multiple Atari games also exhibit striking dips for the
particular case of Breakout.  The low Breakout scores among otherwise
impressive results for many other Atari games are a clear sign of local
maxima trapping.

To prod the RL agent to explore more of the Breakout state space, we first
incorporated a penalty into the reward function each time the paddle misses
the ball.  We also tried adding a bonus into the reward proportional to the
total number of episode frames in order to encourage long game play.  We
even experimented with rewarding paddle movement with the hope of
preventing the agent from becoming pinned against a game window wall.  But
none of these simple adjustments to the reward function had much positive
effect upon the agent's learned behavior.

We next conducted several reward shaping experiments.  Following Ng et al
[1999], we supplemented the reward function with a bonus potential which
peaked at the game window's horizontal center and fell to zero at the side
walls.  Depending upon the potential's magnitude, the agent did learn to
avoid wall pinning.  But it then tended to confine its movements near the
game window's center region.  So the agent again failed to learn to chase
the ball whenever it approached the bottom of the screen.

After witnessing disappointing agent behavior from hundreds of experiments,
we concluded that policy gradient RL methods for Atari Breakout require
much more aggressive encouragement of state-space exploration.  We
consequently investigated deep imitation learning in which a "student"
agent is rewarded for learning to mimic the behavior of some "teacher".  As
we demonstrate in the next section, this quasi-supervised approach can
successfully train a policy neural network to reasonably play Atari
Breakout.

## Imitation learning

The basic goal of Imitation Learning is to estimate the policy of some
expert based upon samples of its behavior.  Imitation Learning (IL) has
been studied by researchers for years, and it is also called Apprenticeship
Learning, Learning from Demonstration and Learning by Watching in the RL
literature.  For highly complex problems such as autonomous vehicle driving
or robot arm operation, IL offers a practical means for beginning to learn
policies whose reward functions may be difficult to even formulate [Abbeel
& Ng].  Keeping in mind such potentially important real-world applications,
we are motivated to study Imitation Learning within the much simpler
context of Atari Breakout



For pedagogical purposes, we implement the hangman game RL system from
scratch in C++ rather than employ an existing deep learning software
package.  As it is fairly simple, RL training finishes on a modern CPU in a
few hours.  We consequently ran several hundred experiments to search for
reasonable system hyperparameters.  Table 2 lists our set of
empirically-derived hyperparameter values.


![Breakout]({{site.url}}/blog/images/breakout/preprocessing.jpg)
*Fig. XX. Raw ALE frames are reduced to 5760-dimensional binary vectors
after cropping, downsampling and temporal differencing.*

<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: left">Hyperparameter</th>
      <th style="text-align: center">Value    </th>
      <th style="text-align: center">Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Epoch frames</td>
      <td style="text-align: center">5E4</td>
      <td style="padding: 0 35px">Number of ALE frames per epoch</td>
    </tr>
    <tr>
      <td style="text-align: left">Batch size</td>
      <td style="text-align: center">2E4</td>
      <td style="padding: 0 35px">  Number of training inputs per
gradient descent computation</td>
    </tr>
    <tr>
      <td style="text-align: left">Base learning rate</td>
      <td style="text-align: center">3E-3</td>
      <td style="padding: 0 35px"> Learning rate for RMS propagation</td>
    </tr>
    <tr>
      <td style="text-align: left">RMSProp decay rate</td>
      <td style="text-align: center">0.9</td>
      <td style="padding: 0 35px">Decay rate for RMS propagation weights cache</td>
    </tr>
    <tr>
      <td style="text-align: left">RMSProp denom const</td>
      <td style="text-align: center">1E-5</td>
      <td style="padding: 0 35px">Constant preventing zero RMS propagation denominator</td>
    </tr>
    <tr>
      <td style="text-align: left">$p_{\rm dropout}$</td>
      <td style="text-align: center">0.5</td>
      <td style="padding: 0 35px">Hidden layer dropout probability</td>
    </tr>
    <tr>
      <td style="text-align: left"> $\lambda$ </td>
      <td style="text-align: center">1E-3</td>
      <td style="padding: 0 35px">L2 regularization coefficient</td>
    </tr>
    <tr>
      <td style="text-align: left"> $\gamma$ </td>
      <td style="text-align: center">0</td>
      <td style="padding: 0 35px">RL discount factor</td>
    </tr>

  </tbody>
</table>

*Table 1.  List of hyperparameter values used to train a "student" agent to
mimic a "teacher" AI.*


<a href="http://www.youtube.com/watch?feature=player_embedded&v=GkygRAtxcss"
target="_blank"><center><img src="http://img.youtube.com/vi/GkygRAtxcss/0.jpg"
width="600" height="400" /></center></a>
*Fig. XX.  Movie of a "teacher" AI's approach to playing Breakout.  Click
on image above to start the YouTube video.*

<a href="http://www.youtube.com/watch?feature=player_embedded&v=r7llzBLIfGI"
target="_blank"><center><img
src="http://img.youtube.com/vi/r7llzBLIfGI/0.jpg" width="600"
height="400"/></center></a> 
*Fig. XX.  Movie of a "student" RL agent playing Breakout after having
learned to imitate its AI "teacher".  Click on image above to start the
YouTube video.*

## Conventional RL refining 


![Breakout]({{site.url}}/blog/images/breakout/zeroth_layer_weights.png)
*Fig. XX.  Trained values for 4 of the 128 weights within the first hidden
layer of the policy network.  Bright warm [cool] colors correspond to
positive [negative] weight values.  Dark colors indicate weight
coefficients close to zero.*


*Fig. XX.  Movie of an imitative agent playing Breakout after its neural
network is refined by conventional deep reinforcement learning.  Click on
image above to start the YouTube video.*

![Breakout]({{site.url}}/blog/images/breakout/montage_frames_scores.jpg)
*Fig. XX.  Total number of ALE frames and episode scores plotted for 50 test
Atari breakout games.  Solid green, blue and red curves correspond to
naive, imitating and refined RL agents.  Dashed horizontal lines illustrate
median values for total frame numbers and game scores.*



## References

*.  [See "Teaching a Deep RL Agent to Lazily Win at Atari Pong" post in this
blog series,  $\small{(2016)}$.](http://petercho.github.io/blog/)

*.  [J. Schulman, "Exploration" lecture notes $\small{(2015)}$.](
http://rll.berkeley.edu/deeprlcourse-fa15/docs/2015.10.07.Exploration.pdf)

*.  [J. Schulman, S. Levine, P. Moritz, M. Jordan and P. Abbeel, "Trust
Region Policy Optimization," Proceedings of the 31st International
Conference on Machine Learning
$\small{(2015)}$.](https://arxiv.org/pdf/1502.05477.pdf)

*.  [T. Salimans, J. Ho, X. Chen and I. Sutskever, "Evolution Strategies as
a Scalable Alternative to Reinforcement Learning," arXiv:1703.03864
$\small{(2017)}$.](https://arxiv.org/pdf/1703.03864.pdf)

*.  [A.Y. Ng, D. Harada and S. Russell, "Policy Invariance Under Reward
Transformations: Theory and Application to Reward Shaping," Proceedings of
the 16th international Conference on Machine Learning
$\small{(1999)}$.](http://ai.stanford.edu/~ang/papers/shaping-icml99.pdf)

*. [G. Hinton, "Neural Networks for Machine Learning", Slide 29 of Lecture
6 Coursera notes $\small{(2012)}$.](https://www.coursera.org/learn/neural-networks)

