---
layout: post
title:  "Teaching an Agent to Play Atari Breakout via Imitative
Reinforcement Learning"
date:   2017-03-26 12:00:00
categories: Deep Learning
use_math: true
---

## Local minimum trapping

<a href="http://www.youtube.com/watch?feature=player_embedded&v=rEzXFA_58vw"
target="_blank"><center><img align="center" src="http://img.youtube.com/vi/rEzXFA_58vw/0.jpg" width="600" height="400" /></center></a>

*Fig. XX.  Movie of an RL agent playing Atari Breakout after being trained
via conventional Deep Reinforcement Learning.  Agent rapidly learns to
score a few points by pinning its paddle on the game window's left wall.
Click on image above to start the YouTube video.*


## Imitation learning

For pedagogical purposes, we implement the hangman game RL system from
scratch in C++ rather than employ an existing deep learning software
package.  As it is fairly simple, RL training finishes on a modern CPU in a
few hours.  We consequently ran several hundred experiments to search for
reasonable system hyperparameters.  Table 2 lists our set of
empirically-derived hyperparameter values.

<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: left">Hyperparameter</th>
      <th style="text-align: center">Value    </th>
      <th style="text-align: center">Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Batch size</td>
      <td style="text-align: center">1E4</td>
      <td style="padding: 0 35px">  Number of training inputs per
gradient descent computation</td>
    </tr>
    <tr>
      <td style="text-align: left">Learning rate</td>
      <td style="text-align: center">3E-4</td>
      <td style="padding: 0 35px"> Learning rate for RMS propagation</td>
    </tr>
    <tr>
      <td style="text-align: left">RMSProp decay rate</td>
      <td style="text-align: center">0.9</td>
      <td style="padding: 0 35px">Decay rate for RMS propagation weights cache</td>
    </tr>
    <tr>
      <td style="text-align: left">RMSProp denom const</td>
      <td style="text-align: center">1E-5</td>
      <td style="padding: 0 35px">Constant preventing zero RMS propagation denominator</td>
    </tr>
    <tr>
      <td style="text-align: left">Leaky ReLU slope</td>
      <td style="text-align: center">1E-2</td>
      <td style="padding: 0 35px">Slope for negative domain of Leaky
ReLU nonlinearities</td>
    </tr>
    <tr>
      <td style="text-align: left"> $\lambda$ </td>
      <td style="text-align: center">1E-4</td>
      <td style="padding: 0 35px">L2 regularization coefficient</td>
    </tr>
    <tr>
      <td style="text-align: left"> $\gamma$ </td>
      <td style="text-align: center">0.9</td>
      <td style="padding: 0 35px">RL discount factor</td>
    </tr>

  </tbody>
</table>

*Table 2.  Hyperparameter values used in the hangman game RL system.*


<a href="http://www.youtube.com/watch?feature=player_embedded&v=GkygRAtxcss"
target="_blank"><center><img src="http://img.youtube.com/vi/GkygRAtxcss/0.jpg"
width="600" height="400" /></center></a>
*Fig. XX.  Movie of a "teacher" AI's approach to playing Breakout.  Click
on image above to start the YouTube video.*

<a href="http://www.youtube.com/watch?feature=player_embedded&v=r7llzBLIfGI"
target="_blank"><center><img
src="http://img.youtube.com/vi/r7llzBLIfGI/0.jpg" width="600"
height="400"/></center></a> 
*Fig. XX.  Movie of a "student" RL agent playing Breakout after having
learned to imitate its AI "teacher".  Click on image above to start the
YouTube video.*

## RL refinement

*Fig. XX.  Movie of an imitative agent playing Breakout after its neural
network is refined by conventional deep reinforcement learning.  Click on
image above to start the YouTube video.*

![Breakout]({{site.url}}/blog/images/breakout/montage_frames_scores.jpg)
*Fig. XX.  Total number of ALE frames and episode scores plotted for 50 test
Atari breakout games.  Solid green, blue and red curves correspond to
naive, imitating and refined RL agents.  Dashed horizontal lines illustrate
median values for total frame numbers and game scores.*


## References

8. [G. Hinton, "Neural Networks for Machine Learning", Slide 29 of Lecture
6 Coursera notes $\small{(2012)}$.](https://www.coursera.org/learn/neural-networks)

9.  [P. Bachman, A. Sordoni and A. Trischler, "Towards Information-Seeking
Agents", arXiv:1612.02705v1 $\small{(2016)}$.](https://arxiv.org/abs/1612.02605)

