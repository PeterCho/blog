---
layout: post
title:  "Teaching an Agent to Play Atari Breakout via Imitation and 
Reinforcement Learning"
date:   2017-03-26 12:00:00
categories: Deep Learning
use_math: true
---

## An RL trap

In this blog posting, we return to Atari games in the Arcade Learning
Environment $($ALE$)$ [ref] which provide interesting and fun laboratories
for studying deep Reinforcement Learning.  Here we focus upon the game
"Breakout" which seems like a minor variant of "Pong".  Both are
single-player games which involve hitting a moving ball with a paddle.
Points are earned in Breakout each time the ball strikes a brick within a
colored horizontal array appearing near the top of the game window.  The
Breakout player's goal is simply to strike as many colored bricks as
possible with the ball in order to maximize the total game score.

In our previous Pong blog writeup [], we found that Reinforcement Learning
$($RL$)$ methods based upon policy gradient methods worked well.  So it is
natural to hypothesize the same should be true for Breakout.  But deep RL
training via policy gradients rapidly leads to the agent becoming trapped
in highly-suboptimal local maxima within state space.  One of the worst
strategies learned by the agent is exhibited in the movie of figure 1:

<a href="http://www.youtube.com/watch?feature=player_embedded&v=rEzXFA_58vw"
target="_blank"><center><img align="center" src="http://img.youtube.com/vi/rEzXFA_58vw/0.jpg" width="600" height="400" /></center></a>

*Fig. 1.  Movie of an RL agent playing Atari Breakout after being trained
via conventional Deep Reinforcement Learning.  The agent unsatisfactorily
learns to score a few points by pinning its paddle to the game window's
left wall.  Click on image above to start the YouTube video.*

Since Breakout always starts with the first ball being fired towards the
lower left corner of the game window, the agent learns it can score a few
points by sliding its paddle all the way to the left wall and reamining
fixed there.  But the agent fails to realize that its total score would be
improved if it were to chase the ball after the first return and not remain
pinned against the left wall.

The movie in figure 1 vividly demonstrates the general RL problem
exploration vs exploitation.  An agent needs to be encouraged to search
unfamiliar regions of state space to avoid entrapment in locally maximal
but globally suboptimal performance regions.  Other authors have noted this
wall-pinning malady for Reinforcement Learning of Breakout [].  Total
scores reported in prior policy gradient [] and evolution strategy []
studies of multiple Atari games also exhibit striking dips for the
particular case of Breakout.  The low Breakout scores among otherwise
impressive results for many other Atari games are a clear sign of local
maxima trapping.

To prod the RL agent to explore more of the Breakout state space, we first
incorporated a penalty into the reward function each time the paddle misses
the ball.  We also tried adding a bonus into the reward proportional to the
total number of episode frames in order to encourage long game play.  We
even experimented with rewarding paddle movement with the hope of
preventing the agent from becoming pinned against a game window wall.  But
none of these simple adjustments to the reward function had much positive
effect upon the agent's learned behavior.

We next conducted several reward shaping experiments.  Following Ng et al
[1999], we supplemented the reward function with a bonus potential which
peaked at the game window's horizontal center and fell to zero at the side
walls.  Depending upon the potential's magnitude, the agent did learn to
avoid wall pinning.  But it then tended to confine its movements near the
game window's center region.  So the agent again failed to learn to chase
the ball whenever it approached the bottom of the screen.

After witnessing disappointing agent behavior from hundreds of experiments,
we concluded that policy gradient RL methods for Atari Breakout require
much more aggressive encouragement of state-space exploration.  We
consequently investigated deep imitation learning in which a "student"
agent is rewarded for learning to mimic the behavior of some "teacher".  As
we demonstrate in the next section, this quasi-supervised approach can
successfully train a policy neural network to reasonably play Atari
Breakout.

## Imitation Learning

The basic goal of Imitation Learning $($IL$)$ is to estimate the policy of
some expert based upon samples of its behavior.  Imitation Learning has
been studied by researchers for years, and it is also known in the
literature as Apprenticeship Learning, Learning from Demonstration and
Learning by Watching.  For highly complex problems such as robot arm
grappling or autonomous vehicle driving where humans can supply expert
training data, IL offers a practical means for beginning to learn policies
whose reward functions may be difficult to even formulate [Abbeel & Ng].
Keeping in mind such potentially important real-world applications, we are
motivated to study Imitation Learning within the much simpler context of
Atari Breakout

We begin by constructing a reasonably proficient Breakout player that we
refer to as a "teacher AI".  The teacher first needs to know the kinematics
of the paddle and ball in each ALE frame.  The paddle's instantaneous
location ${\bf p}(t)$ is readily extracted from the center-of-mass of
non-wall pixels in the last row of the Breakout game window.  The ball's
position ${\bf b}(t)$ may similarly be recovered by calculating the
centroid of the difference between two consecutive frames.  We take the
ball's velocity to equal ${\bf v}(t-1) = 0.5 \bigl[ {\bf b}(t) - {\bf
b}(t-2) \bigr]$.  Though this expression is delayed in time, it typically
encodes the correct instantaneous two-dimensional velocity since the ball
moves in straight lines between occasional bounces.

When the ball travels vertically upwards in the game window, we choose to
keep the teacher's paddle stationary.  When the ball heads downwards, it is
straightforward to predict the ball's future location ${\bf b}(t_{\rm last
\, row})$ when it will reach the last row even if it bounces off a wall on
the way down.  The difference $\Delta(t) = {\bf b}(t_{\rm last \, row}) -
{\bf p}(t)$ between the ball's predicted and the paddle's current pixel
positions in the last row then guides the teacher's stochastic policy.  The
AI responds to large positive $($negative$)$ values for $\Delta$ by moving
the paddle to the right $($left$)$, while it executes no operation if the
magnitude of $\Delta$ is close to zero.  As the movie in figure 2
illustrates, this simple strategy enables the teacher AI to sustain long
rallies, break through the wall of colored bricks and rack up a large
score.

<a href="http://www.youtube.com/watch?feature=player_embedded&v=GkygRAtxcss"
target="_blank"><center><img src="http://img.youtube.com/vi/GkygRAtxcss/0.jpg"
width="600" height="400" /></center></a>
*Fig. 2.  Movie of the "teacher AI" playing Breakout.  Click
on image above to start the YouTube video.*

With a decent teacher demonstrator in hand, we can investigate how well a
"student" agent learns to mimic its behavior.  We employ deep supervised
learning to develop a map from input states to output student actions.  In
order to reduce the size of the student's policy network, raw ALE frames
are reduced to sparse vectors following the image cropping, downsampling
and temporal differencing steps pictured in figure 3.  After trying a few
different vector quantization schemes, we further simplify the state vector
by resetting all its non-zero values to unity.

![Breakout]({{site.url}}/blog/images/breakout/preprocessing.jpg)
*Fig. 3. Raw ALE frames are cropped to remove irrelevant pixel content and
downsampled by a factor of two in both horizontal and vertical directions.
Consecutive pairs of subsampled frames are then temporally differenced to
produce $72 \times 80=5760$ binary state vectors.*

The binary state vectors become inputs to a fully-connected network whose
architecture is schematically illustrated in figure 4.  To our surprise, we
found piecewise-linear approximations to tahn functions work well as node
activations.  Such hybrid nonlinearities run at the same speed as ReLU or
leaky ReLU thresholding, yet they lead to better game play.  We initialize
the neural network's 6109 biases to zero, and we Xavier initialize its
754,048 weights.  During training, 50% dropout is performed on hidden layer
nodes to prevent overfitting [ref].  L2-regularization is also employed to
discourage weight values from diverging.

![Breakout]({{site.url}}/blog/images/breakout/BreakoutNetwork.jpg)
*Fig. 4. Schematic diagram of student policy network which maps input
Breakout states onto output paddle actions.*

Various supervised learning approaches may be considered for training the
student agent to mimic its teacher AI.  Since imitation will never be
perfect, the student will encounter input states not observed by its expert
mentor.  So we avoid training the student network to classify input states
and output actions coming from just the teacher's policy.  Instead, the
student must follow its own policy and receive teacher feedback at each
timestep as to how well it responded.

We experimented with several different reward schemes that encourage
Imitation Learning.  We settled upon the student agent earning a $+1$
reward whenever its policy action for a particular ALE frame matches that
of the teacher AI.  Otherwise, it receives a penalty whose magnitude grows
with the difference $\Delta(t) = {\bf b}(t_{\rm last \, row}) - {\bf p}(t)$
between the predicted future law-row ball and the paddle's current pixel
positions.  We took the penalty growth to be approximately linear for $0
\le |\Delta| \le 12$ and afterwards flat for $|\Delta| > 12$.  Since the
student receives supervised feedback at each time step, we set the RL
discount factor $\gamma = 0$.

State, action and reward tuples are computed and stored within a replay
memory for batches of 20K ALE frames.  Inspired by the priority queue ideas
of ref [], we also calculated replay probabilities for each stored sample.
Inputs states for which the ball is located near the bottom of the game
window are more important than those for which it is located near the top.
Similarly, states wherein the ball moves upwards in the game window are
require less agent attention than those for which the ball moves downwards.
Heuristically assigned replay probabilities encode the stored states'
relative importance.

Once the replay memory is filled, the saved tuples are sampled in random
order according to their replay probabilities.  Gradient descent is
subsequently performed via RMS propagation [Hinton ref] on the O(8K)
samples that are typically selected from the 20K batch members.  The
student policy network's weights and biases are updated, and the current
batch of training samples is discarded.  A new set of 20K tuples is
subsequently collected.

Our IL system for teaching the student agent to play Breakout depends upon
multiple hyperparameters whose manually optimized values are summarized in
Table 1.  As for our previous RL investigations, we implement the system's
neural network in own C++ code rather than utilize an existing deep
learning software package.  The neural network is sufficiently compact that
it finishes training overnight when run on a modern CPU.

<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: left">Hyperparameter</th>
      <th style="text-align: center">Value    </th>
      <th style="text-align: center">Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Epoch frames</td>
      <td style="text-align: center">5E4</td>
      <td style="padding: 0 35px">Number of ALE frames per epoch</td>
    </tr>
    <tr>
      <td style="text-align: left">Batch size</td>
      <td style="text-align: center">2E4</td>
      <td style="padding: 0 35px">  Number of training inputs per
gradient descent computation</td>
    </tr>
    <tr>
      <td style="text-align: left">Base learning rate</td>
      <td style="text-align: center">3E-3</td>
      <td style="padding: 0 35px"> Learning rate for RMS propagation</td>
    </tr>
    <tr>
      <td style="text-align: left">RMSProp decay rate</td>
      <td style="text-align: center">0.9</td>
      <td style="padding: 0 35px">Decay rate for RMS propagation weights cache</td>
    </tr>
    <tr>
      <td style="text-align: left">RMSProp denom const</td>
      <td style="text-align: center">1E-5</td>
      <td style="padding: 0 35px">Constant preventing zero RMS propagation denominator</td>
    </tr>
    <tr>
      <td style="text-align: left">$p_{\rm dropout}$</td>
      <td style="text-align: center">0.5</td>
      <td style="padding: 0 35px">Hidden layer dropout probability</td>
    </tr>
    <tr>
      <td style="text-align: left"> $\lambda$ </td>
      <td style="text-align: center">1E-3</td>
      <td style="padding: 0 35px">L2 regularization coefficient</td>
    </tr>
    <tr>
      <td style="text-align: left"> $\gamma$ </td>
      <td style="text-align: center">0</td>
      <td style="padding: 0 35px">RL discount factor</td>
    </tr>

  </tbody>
</table>

*Table 1.  List of hyperparameter values used within IL system to train a
"student" agent to mimic a "teacher" AI.*

As the student agent learns to mimic the teacher AI, we expect the
frequency for which its actions match those of the expert player to
increase over time.  Similarly, the difference $|\Delta|$ between the
student's and teacher's positioning of the paddle ought to diminish.  The
two training curves in figure 5 exhibit both trends.  The matching action
fraction slowly rises with training epoch, while the $|\Delta|$ curve
slowly falls.  After 200 epochs, 55% of the student's actions match on
average those of its teacher, while the mean difference between their
paddle locations equals 13 pixels.  The student learns to mimic its
teacher's gross movements.  But as we previously saw for policy networks
trained to play Atari Pong [Cho], the student's actions exhibit
high-frequency jitter while the expert's do not.

![Breakout]({{site.url}}/blog/images/breakout/IL_training_results.jpg)
*Fig. 5. Imitation Learning training results for the student agent.  LHS:
Fraction of student agent's actions which match those of its teacher AI.
RHS: Difference $|\Delta|$ measured in pixels between the ball's predicted
and the paddle's current pixel positions in the last row of the game
window.*

Given that a conventional RL agent trained to play Breakout nearly always
falls into a local maximum trap, it is reassuring to observe that the IL
student does not suffer this same fate.  Probability density distributions
for the paddle's horizontal position are plotted in figure 6 for the
conventional RL and student IL agents at two representative epoch
snapshots.  The density for the former is strongly peaked near the origin
indicating that its paddle is pinned against the game window's left wall.
In contrast, the distribution for the latter is spread out over the
paddle's entire range of motion.  So training an agent via Imitation
Learning enables it to start reasonably playing Atari Breakout.

![Breakout]({{site.url}}/blog/images/breakout/padded_lhspin_vs_imit.jpg)
*Fig. 6. Two probability density snapshots for paddle's horizontal position
measured in pixels.  The green curve illustrates a conventional RL agent
falls being trapped by a local maximum with its paddle pinned to the left
wall.  The blue curve indicates the IL agent learns to move everywhere as
it anticipates the ball's bottom row position.*

## RL refinement of imitative game play

The exploration vs exploitation problem represents a significant challenge
in many Reinforcement Learning settings.  Starting from a policy network
initialized with noise, a deep RL system can become stuck in some
poor-performance region of state space from which it never escapes.  But if
the initial policy network already exhibits reasonable behavior, subsequent
Reinforcement Learning may improve its performance.  Such refinement is
similar in spirit to network finetuning which is commonly performed for
supervised image classification.  So we investigate here whether further
training via Reinforcement Learning can improve the student agent's game
play.

We set the RL reward for each Breakout frame equal to that returned by the
Arcade Learning Environment which is nonzero only immediately after the
ball hits a colored brick.  Adopting an RL discount factor of $\gamma =
0.95$ propagates positive environmental feedback to approximately
$1/(1-\gamma) = 20$ frames before a brick striking event.  We also
introduce a $-50$ penalty into the RL reward function whenever the paddle
misses the ball.  But the ball-missing penalty is regarded as a singular
temporal event not discounted backwards in time by $\gamma$.

Taking all other hyperparameter values in the RL system to equal those for
the initial IL system, we retrain the agent's policy network.  We then test
several different snapshots of the refined policy on an independently
randomized set of Breakout games.  On the left side of figure 7, total
durations of 50 test episodes measured in ALE frames are plotted for the
agent's initial IL and optimally refined RL policies.  Total game scores
for the same 50 test episodes are displayed on the figure's right side.
Though the total game durations and scores exhibit significant random
fluctuations, their median values depicted by dashed horizontal lines in
the graphs quantitatively demonstrate that RL finetuning improves the
agent's performance.

![Breakout]({{site.url}}/blog/images/breakout/montage_frames_scores.jpg)
*Fig. 7.  Total number of ALE frames and episode scores plotted for 50
test Atari breakout games.  Solid green, blue and red curves correspond to
conventional, imitative and refined RL agents.  Dashed horizontal lines
illustrate median values for total frame numbers and game scores.*

![Breakout]({{site.url}}/blog/images/breakout/zeroth_layer_weights.png)
*Fig. XX.  Trained values for 4 of the 128 weights within the first hidden
layer of the policy network.  Bright warm [cool] colors correspond to
positive [negative] weight values.  Dark colors indicate weight
coefficients close to zero.*

The agent's qualitative game playing abilities are illustrated within the
movie in figure 8.  It generally anticipates the gross location of the ball
within the game window's last row.  The agent also sustains rallies
involving multiple bounces at glancing angles.  But its paddle exhibits
unfortunate high frequency jitter which sometimes causes it to miss a
passing ball.  And we have not observed the agent following the particular
policy used to generate the movie break through all five layers of colored
bricks.  So although the refined agent's performance is quantitatively and
qualitatively much superior to that of the first agent which got trapped in
a bad region of state space, there is still much room left for future
learning.

<a href="http://www.youtube.com/watch?feature=player_embedded&v=uBwWALDdSCA"
target="_blank"><center><img src="http://img.youtube.com/vi/uBwWALDdSCA/0.jpg" width="600" height="400"/></center></a> 
*Fig. XX.  Movie of student agent playing Breakout after its policy network
is refined by conventional deep Reinforcement Learning.  Click on image
above to start the YouTube video.*

## References

*.  [M. G. Bellemare, Y. Naddaf, J. Veness and M. Bowling, "The Arcade
Learning Environment: An Evaluation Platform for General Agents", J. AI
Research 47 $\small{(2013)}$ 253.](http://www.arcadelearningenvironment.org)

*.  [See "Teaching a Deep RL Agent to Lazily Win at Atari Pong" post in this
blog series,  $\small{(2016)}$.](http://petercho.github.io/blog/)

*.  [J. Schulman, "Exploration" lecture notes $\small{(2015)}$.](
http://rll.berkeley.edu/deeprlcourse-fa15/docs/2015.10.07.Exploration.pdf)

*.  [J. Schulman, S. Levine, P. Moritz, M. Jordan and P. Abbeel, "Trust
Region Policy Optimization," Proceedings of the 31st International
Conference on Machine Learning
$\small{(2015)}$.](https://arxiv.org/pdf/1502.05477.pdf)

*.  [T. Salimans, J. Ho, X. Chen and I. Sutskever, "Evolution Strategies as
a Scalable Alternative to Reinforcement Learning," arXiv:1703.03864
$\small{(2017)}$.](https://arxiv.org/pdf/1703.03864.pdf)

*.  [A.Y. Ng, D. Harada and S. Russell, "Policy Invariance Under Reward
Transformations: Theory and Application to Reward Shaping," Proceedings of
the 16th international Conference on Machine Learning
$\small{(1999)}$.](http://ai.stanford.edu/~ang/papers/shaping-icml99.pdf)

*.  [P. Abbeel and A.Y. Ng, "Apprenticeship Learning via Inverse
Reinforcement Learning," Proceedings of the 21st International Conference on
Machine Learning $\small{(2004)}$.](http://ai.stanford.edu/~ang/papers/icml04-apprentice.pdf)

*.  [N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever and
R. Salakhutdinov, "Dropout: A Simple Way to Prevent Neural Networks from
Overfitting", J. Machine Learning Research $\small{(2014)}$
1929.](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)

*. [T. Schaul, J. Quan, I. Antonoglou and D. Silver, "Prioritized
Experience Replay," International Conference on Learning Representations
$\small(2016)$.](https://arxiv.org/pdf/1511.05952.pdf)

*. [G. Hinton, "Neural Networks for Machine Learning", Slide 29 of Lecture
6 Coursera notes $\small{(2012)}$.](https://www.coursera.org/learn/neural-networks)

