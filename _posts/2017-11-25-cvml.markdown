---
layout: post
title:  "Classifying Cat and Dog Images via an Ensemble of CNNs"
date:   2017-11-25 12:00:00
categories: Deep Learning
use_math: true
---

## Revisiting a Kaggle competition

In September 2013, Kaggle sponsored a competition to create an algorithm to
classify cat and dog images [1].  This challenge provided a training
archive of 25K labeled images.  It further supplied a testing corpus of 10K
unlabeled images of cats and dogs.  A total of 215 teams participated in
the competition.  The first place winner in 2014, Pierre Sermanet, achieved
a classification accuracy of 98.9% using his OverFeat deep learning library
[2].  The second place winner, Zhao Xing, attained 98.3% accuracy.

In this blog post, we revisit this fun problem and apply deep learning
techniques $\small($e.g. batch normalization [3]$\small)$, optimizers
$\small($e.g. Adam [4]$\small)$, frameworks $\small($e.g. TensorFlow
[5]$\small)$ and architectures $\small($e.g. DenseNet [6]$\small)$ which
were developed after the original challenge ended.  Like many Kaggle
competition participants, we work with an ensemble of trained Convolutional
Neural Networks $\small($CNNs$\small)$ in order to maximize classification
accuracy.  But rather than finetune various networks pre-trained on
ImageNet data, we are more interested in training CNNs from scratch.
Moreover, we are willing to sacrifice some classification accuracy for
training speed.  Such tradeoffs are commonly made for "real-world"
classification problems.

A relatively small number of the Kaggle training images were mislabeled,
and none of the testing images were annotated.  So we first manually fixed
the training data and labeled the testing data.  We also discarded Kaggle
pictures containing no cat or dog along with photos containing both cats
and dogs [7].  Our cleaned training and testing sets respectively contain
24922 and 9970 images.

At the start of each training session, we randomly split 85% of the input
24922 images into a training subset and the remaining 15% into validation
data.  To decrease training time, we subsample all 35K Kaggle images down
to 160 $\times$ 160 pixel size without preserving aspect ratio $\small($see
figure 1$\small)$.  Dynamic image augmentation of the training subset is
subsequently performed to increase its diversity and regularize neural
network training.  Following common practice, we randomly crop a 148
$\times$ 148 chip from each input 160 $\times$ 160 image and horizontally
flip with 50% probability.  Since cats and dogs are deformable objects that
may appear in highly variable poses, we also randomly rotate 12% of the
training images by 90, 180 or 270 degrees.  Furthermore, we globally alter
the hue, saturation and value color contents for a maximum of 20% of
training images.  Examples of augmented versions of the 160 $\times$ 160
image in figure 1 are displayed in figure 2.  We note that random cropping
is also performed on validation and testing samples so that CNN input is
always 148 $\times$ 148 in size.

![CatsDogs]({{site.url}}/blog/images/cats_and_dogs/cat_orig_downsampled.png)
*Fig. 1.  An original 499 $\times$ 427 Kaggle image of a cat is downsampled
to 160 $\times$ 160.*

![CatsDogs]({{site.url}}/blog/images/cats_and_dogs/montage_augment.png)
*Fig. 2.  Examples of training data augmentation include random
combinations of image crops, horizontal flips, 90 degree rotations and
global color alterations.*

In this Kaggle competition redux, we work with the popular TensorFlow
machine learning framework [5].  The TensorBoard feature of TensorFlow may
be used to display input images as training proceeds.  Figure 3 presents
dynamically augmented cat and dog training samples visualized via
TensorBoard.

![CatsDogs]({{site.url}}/blog/images/cats_and_dogs/training_images.png)
*Fig. 3.  Tensorboard display of dynamically augmented training images.*


## CNN architectures

Over the course of 100+ cat vs dog classification experiments, we
investigated a variety of CNN architectures and training strategies in
search of those yielding maximal validation accuracies.  While running
these experiments on the Kaggle data corpus, we noted several
"rules-of-thumb" which we strongly suspect hold for other image
classification problems:

* Deeper networks based upon 3x3 convolution kernels did not yield better
validation results than networks with fewer 5x5 convolution kernels layers
and comparable receptive field sizes.

*  Networks with 2 pooling layers did not yield better validation results
than those with 3 pooling layers.

* Networks with extra convolution layers interspersed among the 3 pooling
layers did not yield better validation results than networks with 3
up-front pooling layers.  The latter more aggressively reduce image size
and consequently train faster than the former.

* Adding dropout layers to networks already instrumented with batch
normalization did not help reduce data overfitting.

* A final fully connected layer did not benefit from having more than 6
times as many nodes as the number of classification states.

We found the following "super-layer" combinations of convolutions with 5x5
kernels, batch normalization, leaky ReLU activation [8], 2x2 pooling,
DenseNets with 5x5 kernels [6] and fully connected layers to be useful CNN
building blocks:

CBLP: Conv5, Batch norm, Leaky ReLU, Pool

CBL: Conv5, Batch norm, Leaky ReLU

Dense-n: n-layer DenseNet with Batch norm and Leaky ReLU but no bottleneck layer

FBL: Fully connected, Batch norm, Leaky ReLU

SOFT:  Fully connected, Softmax.

We choose these super-layers' parameters and their numbers in deep network
configurations so that total CNN receptive field and training image pixel
sizes are comparable.  The best three architectures we found for
classifying 148 $\times$ 148 cat and dog images all have receptive field
size R=157 [9]:

Architecture I with 9 nonlinear super-layers and 2.5M trainable parameters:

$\qquad$ 3 $\times$ [CBLP], 4 $\times$ [CBL], 2 $\times$ [FBL], SOFT

Architecture II with 10 nonlinear super-layers and 2.9M trainable parameters:

$\qquad$ 2 $\times$ [CBLP], 2 $\times$ [CBL], CBLP, 3 $\times$ [CBL], 2
$\times$ [FBL], SOFT

Architecture III with 10 nonlinear super-layers including 5 dense blocks
and 9.1M trainable parameters:
    
$\qquad$ 2$\times$ [CBLP], Dense-2, CBLP, Dense-3, 2$\times$ [FBL], SOFT

As we shall see, each of these CNN architectures can yield binary
classification accuracies on validation imagery greater than 97%.

## CNN training and ensemble testing

Deep learning usually requires setting several hyperparameters whose values
must be empirically determined.  Some hyperparameter choices such as
optimizer type are discrete, while others like base learning rate are
continuous.  Table 1 lists hyperparameter values which worked well with CNN
architectures I, II and III for classifying cats and dogs.  It is worth
noting that this table does not include any dropout probability nor weight
decay coefficient.  We found that batch normalization plus data
augmentation provided sufficient regularization to prevent significant
overfitting.

<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: left">Hyperparameter description</th>
      <th style="text-align: center">Value    </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Number of training chips per epoch</td>
      <td style="text-align: center">22434</td>
    </tr>
    <tr>
      <td style="text-align: left">Number of validation chips per epoch</td>
      <td style="text-align: center">2493</td>
    </tr>
    <tr>
      <td style="text-align: left">Minibatch size</td>
      <td style="text-align: center">100</td>
    </tr>
    <tr>
      <td style="text-align: left">Optimizer type</td>
      <td style="text-align: center">Adam</td>
    </tr>
    <tr>
      <td style="text-align: left">Base learning rate</td>
      <td style="text-align: center">0.01</td>
    </tr>
    <tr>
      <td style="text-align: left">Leaky ReLU slope coefficient</td>
      <td style="text-align: center">0.1</td>
    </tr>
    <tr>
      <td style="text-align: left">Loss function</td>
      <td style="text-align: center">Cross entropy</td>
    </tr>
</tbody>
</table>

*Table 1.  List of hyperparameters employed for CNN architectures I, II and
III.*

As training proceeds, we maintain a list $L_{incorrect}$ of incorrectly
classified training samples whose size is capped at the number of training
chips per epoch.  We randomly replace entries in training minibatches with
samples from $L_{incorrect}$.  Once an incorrect training sample is reused,
the probability of deleting it from $L_{incorrect}$ decreases from 1.0 to
0.25 during a training session.  And the probability of using previous
incorrectly-classified training samples linearly ramps up from 0 to 0.9
over an experiment.  The classifier is consequently exposed to increasingly
difficult training samples as the CNN learns.

Learning curves for one particular training session are plotted in figure 4.
Fractional classification accuracies on both the training and
validation data sets exhibit significant fluctuations.  But their time
averages generally rise as training proceeds.  We note that aggressive data
augmentation along with hard negative mining renders training samples more
challenging to classify than their validation counterparts.  So the green
learning curve for the training subset improves more slowly than the
purple curve for validation data in figure 4.  Yet by the end of the
experiment, both reach similar classification accuracy fractions.  We
consequently do not observe signs of significant overfitting or
underfitting in this plot or others like it for CNN architectures I,
II and III.

![CatsDogs]({{site.url}}/blog/images/cats_and_dogs/class_accur.png)
*Fig. 4.  Tensorboard display of fractional classification accuracy for
training $\small($green$\small)$ and validation $\small($purple$\small)$
data sets for one CNN architecture I experiment.*

After running a series of training experiments, we harvest 8 TensorFlow
models frozen at epochs with maximal validation accuracies ranging from
97.3% to 97.5%.  Two of the frozen models come from different epochs within
one training session for an architecture I CNN.  Two others come from
different epochs in another experiment for a CNN with architecture II.  The
8 individual models subsequently become inputs to a CNN ensemble.  Given an
input image, we sum cat and dog classification probabilities from each of
the 8 models.  Classification state is then determined by the maximum
cumulative score.  This aggregation approach effectively yields a weighted
voting scheme for binary classification.

Inference results from an ensemble of classifiers are generally better than
those from any of its individual constituents [10].  We perform ensemble
classification on our cleaned Kaggle test set of 9970 cat and dog images
that we manually labeled.  The ensemble of 8 trained CNNs achieves a
classification accuracy of 98.0% on the held-out test set.  While our
classification performance does not exceed the 2014 competition winner's,
we did not employ any external data from sources like ImageNet, and we
intentionally worked with downsampled versions of the Kaggle imagery for
speed purposes.

Representative examples of correctly classified test images are presented
in figure 5.  Successfully classified photos display cats and dogs in a
variety of poses, often include human owners or more than one animal, and
sometimes contain occluding foreground objects like cages.  It is
interesting to compare examples of correctly classified test images with
those in figure 6 depicting incorrect classification results.  In some
cases, wrongly classified animals appear small due to physical size or
distance from the camera.  In others, the animals' orientations are
uncommon.  Yet after having looked through the O$($200$)$ of O$($10K$)$
test images which were incorrectly classified by the CNN ensemble, we did
not see obvious signals that differentiate them from correctly classified
photos.

![CatsDogs]({{site.url}}/blog/images/cats_and_dogs/montage_correct_cats.png)
![CatsDogs]({{site.url}}/blog/images/cats_and_dogs/montage_correct_dogs.png)
*Fig. 5.  Representative examples of correctly classified cat and dog test images.*


![CatsDogs]({{site.url}}/blog/images/cats_and_dogs/montage_incorrect.png)
*Fig. 6.  Representative examples of incorrectly classified test images.*

To gain insight into how CNNs learn to categorize, we need to investigate
more than just test imagery classification results.  We take up this topic
in the next section.

## Visualizing CNN Learning

Neural network feature visualization provides insight into what trained CNN
neurons have learned to recognize [11].  Starting from some node within a
network, we backpropagate to an input image initialized with noise.  The
image's pixel values are subsequently optimized to maximally activate the
selected neuron while the network's weights are held fixed.  The resulting
synthesized image represents an input preferred by the trained neuron.  The
detailed appearances of such preferred inputs vary each time they are
generated from random noise.  But their qualitative patterns are generally
stable for entire layers in a CNN.

Figure 7 displays representative examples of inputs that maximally activate
neurons within each of the convolutional layers in one particular trained
architecture I CNN.  These results were generated via a minor variant of
TensorFlow code developed by Alex Mordvintsev [12].  Nodes in the network's
first layer respond to input image colors.  Neurons in deeper layers fire
when they observe increasingly complicated textures and patterns.  Zooming
into the convolution 6 layer of the figure, we see filters responding to
cat ears and dog eyes.  In the network's final convolutional layer, entire
animal faces become recognizable among preferred inputs for some nodes.


![CatsDogs]({{site.url}}/blog/images/cats_and_dogs/preferred_filters.png)
*Fig. 7.  Examples of preferred inputs for trained filters within
convolution layers of CNN architecture I.*

It is interesting to compare preferred input visualizations for different
networks trained on the same imagery corpus.  Figure 8 presents such a
comparison for four of the eight CNNs within our ensemble used to classify
cat and dog test images.  The final convolutional layer results in this
figure were intentionally chosen based upon their animal face contents.
Qualitatively different cat and dog faces are clearly visible within the
preferred inputs for the four CNNs.  Such qualitative variation among
filter responses holds even for models frozen from a single network at
different training epochs.  Since an ensemble of trained CNNs can respond
to a wider variety of imagery inputs than any of its individual members, it
makes sense that classification performance for the former should be
superior to that of the latter.

![CatsDogs]({{site.url}}/blog/images/cats_and_dogs/multiexpt_preferred_final_conv_filters.png)
*Fig. 8.  Examples of preferred inputs for trained filters within the final 
convolution layers of four different CNNs with various architectures.*

Another interesting visualization of network classifiers centers upon
category clustering.  Nonlinear layers in CNNs generally transform input
images into representations which are easier for their final classifier
layer to handle.  For example, our architecture I CNN converts 148 $
\times$ 148 images into 12-dimensional descriptors which feed into its
final softmax classifier.  Before training starts, the CNN's weights are
randomly initialized.  Descriptors corresponding to input cat and dog
images are then intermingled within the output 12-dimensional space.  But
as the CNN's weights evolve during training, we expect distinct cat and dog
clusters to develop.  Visualizing the formation of such image descriptor
clusters provides an instructive view into CNN learning.

The movie in figure 9 illustrates PCA decompositions of image descriptors
from the final fully connected nonlinear layer of an architecture I CNN.
Starting with the network after it has been trained for 250 epochs, we
first compute descriptor eigendirections for a distribution of O$($2.4K$)$
validation images. Twelve-dimensional validation descriptors at training
epochs 0, 4, 8 and 16 are subsequently projected onto the dominant three
asymptotic eigendirections.  Cat and dog PCA descriptors are depicted by
red and blue points in figure 9.

<a
href="http://www.youtube.com/watch?feature=player_embedded&v=Si2sxR5Pm_w"
target="_blank"><center><img
src="http://img.youtube.com/vi/Si2sxR5Pm_w/0.jpg" width="600"
height="400"/></center></a>

*Fig. 9. Movie showing PCA decompositions of 12-dimensional image
descriptors of cat and dog test images at various training epochs.  Click
on the image above to start the YouTube video.*

Watching the movie, we clearly observe greater separation between projected
cat and dog descriptors as training epoch number increases.  The relatively
small overlap between red and blue dots within the last quadrant's point
cloud would presumably diminish if more PCA eigendirections could be
plotted.  But some intermingling among descriptors would persist since CNN
classification is never perfect.


In closing, we note the emergence of projected descriptor subclusters in
figure 9 particularly among dog images.  It would be interesting to examine
photos corresponding to identifiable descriptor subgroups to see if they
are semantically meaningful.  But we leave such exploration for future
work.

## References

1.  [See the 2013 Kaggle "Dogs vs Cats" competition.](https://www.kaggle.com/c/dogs-vs-cats)

2.  [P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Ferbus and Y. LeCun,
"OverFeat: Integrated Recognition, Localization and Detection using
Convolutional Networks," ICLR $\small{(2014)}$,
16.](https://arxiv.org/abs/1312.6229)

3.  [S. Ioffe and C. Szegedy, "Batch Normalization: Accelerating Deep
Network Training by Reducing Internal Covariate Shift," 32nd ICML 
$\small{(2015)}$.](https://arxiv.org/abs/1502.03167)

4.  [D.P. Kingma and J. Ba, "Adam: A method for Stochastic Optimization,"
3rd ICLR $\small{(2014)}$.](https://arxiv.org/abs/1412.6980)

5.  [See TensorFlow: An open-source software library for Machine
Intelligence.](https://www.tensorflow.org/)

6.  [G. Huang, Z. Liu, K.Q. Weinberger, L. van der Maaten, "Densely
Connected Convolutional Networks," CVPR $\small{(2017)}$.](https://arxiv.org/abs/https://arxiv.org/abs/1608.06993)

7.  [A partial list of labeling errors within the Kaggle dogs vs cats
training corpus has been compiled by Fabian
Linzberger.](https://gist.github.com/lefant/bb9c304cfcf5da0f9f652570cc54893a)

8.  [A.L. Maas, A.Y. Hannun and A.Y. Ng, "Rectifier Nonlinearities Improve
Neural Network Acoustic Models", 50th ICML $\small{(2013)}$.](https://web.stanford.edu/~awni/papers/relu_hybrid_icml2013_final.pdf)

9.  [D.H.T Hien, "A guide to receptive field arithmetic for Convolutional
Neural Networks," $\small{(2017)}$.](https://medium.com/@nikasa1889/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807)

10.  [T.G. Dietterich, "Ensemble Methods in Machine Learning," International
Workshop on Mutiple Classifier Systems $\small{(2000)}$ 1.](http://web.engr.oregonstate.edu/~tgd/publications/mcs-ensembles.pdf)

11.  [C. Olah, A. Mordvintsev and L. Schubert, "Feature Visualization,"
Distill $\small{(2017)}$.](https://distill.pub/2017/feature-visualization/)

12.  [See DeepDreaming with TensorFlow.](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/deepdream/deepdream.ipynb)


