---
layout: post
title:  "Classifying Cat and Dog Images via an Ensemble of CNNs"
date:   2017-11-25 12:00:00
categories: Deep Learning
use_math: true
---

## Revisiting a Kaggle competition

In September 2013, Kaggle sponsored a competition to create an algorithm
which determines whether images contain either a dog or a cat [1].  This
challenge provided a training archive of 25K labeled images of dogs and
cats.  It further supplied a testing corpus of 10K unlabeled images of dogs
and cats.  A total of 215 teams participated in the competition.  The first
place winner in 2014, Pierre Sermanet, achieved a classification accuracy
of 98.9% using his OverFeat deep learning library [2].  The second place
winner attained 98.3% accuracy.

In this blog post, we revisit this fun problem and apply Convolutional
Neural Network $\small($CNN$\small)$ techniques $\small($e.g. batch
normalization [3]$\small)$, optimizers $\small($e.g. Adam [4]$\small)$,
frameworks $\small($e.g. TensorFlow [5]$\small)$ and architectures
$\small($e.g. DenseNet [6]$\small)$ which were developed after the original
challenge ended.  Like many Kaggle competition participants, we work with
an ensemble of trained CNNs in order to maximize classification accuracy.
But rather than finetune networks pre-trained on ImageNet data, we are more
interested in training CNNs from scratch.  Moreover, we are willing to
sacrifice some classification accuracy for training speed.  Such tradeoffs
are commonly made for "real-world" classification problems.

A relatively small number of the Kaggle training images were mislabeled,
and none of the testing images were annotated.  So we first manually fixed
the training data and labeled the testing data.  We also discarded Kaggle
pictures containing no cat or dog as well as images containing both cats
and dogs [7].  Our cleaned training and testing sets respectively contain
24922 and 9970 images.

At the start of each training session, we randomly split 85% of the input
24922 images into a training subset and the remaining 15% into validation
data.  To decrease training time, we subsample all 35K Kaggle images down
to 160 $\times$ 160 pixel size without preserving aspect ratio $\small($see
figure 1$\small)$.  Dynamic image augmentation of the training subset is
subsequently performed to increase its diversity and regularize neural
network training.  Following common practice, we randomly crop a 148
$\times$ 148 chip from each input 160 $\times$ 160 image and horizontally
flip with 50% probability.  Since cats and dogs are deformable objects that
may appear in highly variable poses, we also randomly rotate 12% of
training images by 90, 180 or 270 degrees.  Finally, we globally alter the
hue, saturation and value color contents for a maximum of 20% of training
inputs.  Examples of augmented versions of the 160 $\times$ 160 image in
figure 1 are displayed in figure 2.  We note that random cropping is also
performed on validation and testing samples so that CNN input is always 148
$\times$ 148 in size.

![CatsDogs]({{site.url}}/blog/images/cats_and_dogs/cat_orig_downsampled.png)
*Fig. 1.  An original 499 $\times$ 427 Kaggle image of a cat is downsampled
to 160 $\times$ 160.*

![CatsDogs]({{site.url}}/blog/images/cats_and_dogs/montage_augment.png)
*Fig. 2.  Examples of training data augmentation include random
combinations of image crops, 90 degree rotations, horizontal flips and
global color alterations.*

In this Kaggle competition redux, we work within the popular TensorFlow
machine learning framework [5].  The TensorBoard feature of TensorFlow may
be used to display input images as training proceeds.  Figure 3 presents
dynamically augmented cat and dog training samples visualized via
TensorBoard.

![CatsDogs]({{site.url}}/blog/images/cats_and_dogs/training_images.png)
*Fig. 3.  Tensorboard display of dynamically augmented training images.*


## CNN architectures

Over the course of 100+ experiments, we investigated a variety of CNN
architectures and different training strategies in search of those yielding
maximal validation classification accuracies.  While running these
experiments on the cats and dogs imagery corpus, we noted several
"rules-of-thumb" which may hold for other classification problems:

*  Deeper networks based upon 3x3 convolution kernels did not yield better
validation results than networks with fewer 5x5 convolution kernels layers.

*  Networks with 2 pooling layers did not yield better validation results
than those with 3 pooling layers.

*  Networks with extra convolution layers interspersed among the 3 pooling
layers did not yield better validation results than networks with 3
up-front pooling layers.  The latter aggressively reduce image size and
consequently train more quickly than the former.

*  Adding final dropout layers in addition to batch normalization did not
help reduce data overfitting.

* A final fully connected layer did not benefit from having more than 6
times as many nodes as the number of classification states.

We found the following combinations of convolutions with 5x5 kernels, batch
normalization, leaky ReLU activation [8], 2x2 pooling and fully connected
layers to be useful building blocks:

CBLP: Conv5, Batch Norm, Leaky ReLU, Pool

CBL: Conv5, Batch Norm, Leaky ReLU

BLD: Batch Norm, Leaky ReLU, Dense Conv5 with no bottleneck

FBL: Fully connected, Batch Norm, Leaky ReLU.

We choose these super-layers' parameters and their numbers in deep network
configurations so that total CNN receptive field and training image pixel
sizes are comparable.  The best three architectures we found for Kaggle
cats and dogs classification all have receptive field size R=157 [9]:

Architecture I with 9 super-layers: 

$\qquad$ 3 $\times$ [CBLP], 4 $\times$ [CBL], 2 $\times$ [FBL]

Architecture II with 10 super-layers: 

$\qquad$ 2 $\times$ [CBLP], 2 $\times$ [CBL], CBLP, 3 $\times$ [CBL], 2 $\times$ [FBL]

Architecture III with 10 super-layers including 5 dense blocks:
    
$\qquad$ 2$\times$ [CBLP], 2$\times$ [BLD], CBLP, 3$\times$ [BLD], 2$\times$ [FBL]

As we shall see, each of these CNN architectures can yield binary
classification accuracies on validation imagery greater than 97%.

## CNN training and ensemble testing

Deep learning often involves setting several hyperparameters whose values
must be empirically determined.  Some hyperparameter choices are discrete
such as optimizer type while others are continuous like base learning rate.
Table 1 lists hyperparameter values which we found worked well with CNN
architectures I, II and III for classifying cats and dogs.  It is worth
noting that this table does not include any dropout probability or weight
decay coefficient.  Instead, we observed that batch normalization plus data
augmentation provided sufficient regularization to prevent significant
overfitting.  

<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: left">Hyperparameter description</th>
      <th style="text-align: center">Value    </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Number of training chips per epoch</td>
      <td style="text-align: center">22434</td>
    </tr>
    <tr>
      <td style="text-align: left">Number of validation chips per epoch</td>
      <td style="text-align: center">2493</td>
    </tr>
    <tr>
      <td style="text-align: left">Minibatch size</td>
      <td style="text-align: center">100</td>
    </tr>
    <tr>
      <td style="text-align: left">Optimizer type</td>
      <td style="text-align: center">Adam</td>
    </tr>
    <tr>
      <td style="text-align: left">Base learning rate</td>
      <td style="text-align: center">0.01</td>
    </tr>
    <tr>
      <td style="text-align: left">Leaky ReLU coefficient</td>
      <td style="text-align: center">0.1</td>
    </tr>
  </tbody>
</table>

*Table 1.  List of hyperparameters employed for CNN architectures I, II and
III.*



As training proceeds, we maintain a list $L_{wrong \; samples}$ of
incorrectly classified training samples whose size is capped at the number
of training chips per epoch.  We randomly replace entries in training
batches with samples from $L_{wrong \; samples}$.  The probability of
reusing a previous incorrectly-classified training samples linearly
increases from 0 to 0.9 during a training session.  Once some previously
incorrect training sample is reused, the probability of deleting it from
$L_{wrong \; samples}$ decreases from from 1.0 to 0.25 during a training
session.  The classifier is consequently exposed to increasingly difficult
samples as training proceeds.




![CatsDogs]({{site.url}}/blog/images/cats_and_dogs/class_accur.png)
*Fig. 4.  Tensorboard display of fractional classification accuracy for
training $\small($green$\small)$ and validation $\small($purple$\small)$
data sets for one particular CNN experiment.*


![CatsDogs]({{site.url}}/blog/images/cats_and_dogs/montage_correct_cats.png)
![CatsDogs]({{site.url}}/blog/images/cats_and_dogs/montage_correct_dogs.png)
*Fig. 5.  Representative examples of correctly classified cat and dog test images.*

![CatsDogs]({{site.url}}/blog/images/cats_and_dogs/montage_incorrect.png)
*Fig. 6.  Representative examples of incorrectly classified test images.*


## Visualizing CNN Learning


![CatsDogs]({{site.url}}/blog/images/cats_and_dogs/preferred_filters.png)
*Fig. 7.  Examples of preferred inputs for trained filters within all
convolution layers of CNN architecture I.* 

![CatsDogs]({{site.url}}/blog/images/cats_and_dogs/multiexpt_preferred_final_conv_filters.png)
*Fig. 8.  Examples of preferred inputs for trained filters within final 
convolution layers of four different CNNs with various architectures.*





## References

1.  [See the 2013 Kaggle "Dogs vs Cats" competition.](https://www.kaggle.com/c/dogs-vs-cats)

2.  [P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Ferbus and Y. LeCun,
"OverFeat: Integrated Recognition, Localization and Detection using
Convolutional Networks," ICLR $\small{(2014)}$,
16.](https://arxiv.org/abs/1312.6229)

3.  [S. Ioffe and C. Szegedy, "Batch Normalization: Accelerating Deep
Network Training by Reducing Internal Covariate Shift," 32nd ICML 
$\small{(2015)}$.](https://arxiv.org/abs/1502.03167)

4.  [D.P. Kingma and J. Ba, "Adam: A method for Stochastic Optimization,"
3rd ICLR $\small{(2014)}$.](https://arxiv.org/abs/1412.6980)

5.  [See TensorFlow: An open-source software library for Machine
Intelligence.](https://www.tensorflow.org/)

6.  [G. Huang, Z. Liu, K.Q. Weinberger, L. van der Maaten, "Densely
Connected Convolutional Networks," CVPR $\small{(2017)}$.](https://arxiv.org/abs/https://arxiv.org/abs/1608.06993)

7.  [A partial list of labeling errors within the Kaggle dogs vs cats
training corpus has been compiled by Fabian
Linzberger.](https://gist.github.com/lefant/bb9c304cfcf5da0f9f652570cc54893a)

8.  [A.L. Maas, A.Y. Hannun and A.Y. Ng, "Rectifier Nonlinearities Improve
Neural Network Acoustic Models", 50th ICML $\small{(2013)}$.](https://web.stanford.edu/~awni/papers/relu_hybrid_icml2013_final.pdf)

9.  [D.H.T Hien, "A guide to receptive field arithmetic for Convolutional
Neural Networks," $\small{(2017)}$.](https://medium.com/@nikasa1889/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807)

10.  [T.G. Dietterich, "Ensemble Methods in Machine Learning," International
Workshop on Mutiple Classifier Systems $\small{(2000)}$ 1.](http://web.engr.oregonstate.edu/~tgd/publications/mcs-ensembles.pdf)

11.  [See DeepDreaming with TensorFlow.](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/deepdream/deepdream.ipynb)

12.  [C. Olah, A. Mordvintsev and L. Schubert, "Feature Visualization,"
Distill $\small{(2017)}$.](https://distill.pub/2017/feature-visualization/)

