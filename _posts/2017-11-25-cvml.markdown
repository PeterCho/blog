---
layout: post
title:  "Classifying Cat and Dog Imagery via a CNN Ensemble"
date:   2017-11-24 12:00:00
categories: Deep Learning
use_math: true
---

## A Kaggle competition redux

In September 2013, Kaggle sponsored a competition to create an algorithm
which determines whether images contain either a dog or a cat [1].  This
challenge provided a training archive of 25K labeled images of dogs and
cats.  It further supplied a testing corpus of 10K unlabeled images of dogs
and cats.  A total of 215 teams participated in the competition.  The first
place winner in 2014, Pierre Sermanet, achieved a classification accuracy
of 98.9% using his OverFeat deep learning library [2].  The second place
winner achieved 98.3% accuracy.

We revisit this fun problem and apply CNN techniques $($ e.g. batch
normalization [3] $)$, optimizers $($ e.g. ADAM [4] $)$, frameworks $($
e.g. TensorFlow [5] $)$ and architectures $($ e.g. DenseNet [6] $)$ which
were developed since the original challenge ended.  Like many Kaggle
competition participants, we work with an ensemble of trained CNNs in order
to maximize classification accuracy.  But rather than finetune networks
pre-trained on ImageNet data, we are more interested in training from
scratch.  Moreover, we are willing to sacrifice some classification
accuracy for training speed.  We believe such tradeoffs are common for
"real-world" classification problems.

To decrease training time, we subsample all 35K Kaggle images down to 160
$\times$ 160 pixel size without preserving aspect ratio $($ see figure 1
$)$.  We also corrected mislabeled training images and manually labeled all
testing images.  We further discarded samples containing no cat or dog as
well as images containing both cats and dogs [7].  Our cleaned training and
testing sets respectively contain 24922 and 9970 images.

![CatsDogs]({{site.url}}/blog/images/cats_and_dogs/cat_orig_downsampled.png)
*Fig. 1.  Original 499 $\times$ 427 image of a cat is downsampled to a 160
$\times$ 160 version that does not preserve the original's aspect ratio.*

## Training data augmentation

At the start of each training session, we randomly split 85% of the input
24922 images into a training subset and the remaining 15% into validation
data.  Dynamic image augmentation of the training subset is subsequently
performed to increase its diversity and regularize neural network training.
Following common practice, we randomly crop a 148 $\times$ 148 chip from
each input 160 $\times$ 160 image and horizontally flip with 50%
probability.  Since cats and dogs are deformable objects that may appear in
highly variable poses, we also randomly rotate 12% of training images by
90, 180 or 270 degrees.  Finally, we globally alter the hue, saturation and
value color contents for a maximum of 20% of training inputs.  Examples of
augmented versions of the 160 $\times$ 160 image in figure 1 are displayed
in figure 2.  We note that random cropping is also performed on validation
and testing samples so that CNNs always ingest 148 $\times$ 148 pixel size
inputs.

![CatsDogs]({{site.url}}/blog/images/cats_and_dogs/montage_augment.png)
*Fig. 2.  Examples of training data augmentation include random
combinations of image crops, 90 degree rotations, horizontal flips and
global color alterations.*

As training proceeds, we maintain a list $L_{wrong}$ of incorrectly
classified training samples whose size is capped at the number of training
chips per epoch.  We randomly replace entries in training batches with
samples from $L_{wrong}$.  The probability of reusing a previous
incorrectly-classified training samples linearly increases from 0 to 0.9
during a training session.  Once some previously incorrect training sample
is reused, the probability of deleting it from $L_{wrong}$ decreases from
from 1.0 to 0.25 during a training session.  The classifier is consequently
exposed to increasingly difficult samples as training proceeds.

## CNN architectures

Over the course of 100+ experiments, we investigated a variety of CNN
architectures and training strategies in search of those yielding maximal
validation classification accuracies.  While running these experiments on
the cats and dogs imagery corpus, we noted several "rules-of-thumb" which
may hold for other classification problems:

*  Deeper networks based upon 3x3 convolution kernels did not yield better
validation results than networks with fewer 5x5 convolution kernels layers.

*  Networks with 2 pooling layers did not yield better validation results
than those with 3 pooling layers.

*  Networks with extra convolution layers interspersed among the 3 pooling
layers did not yield better validation results than networks with 3
up-front pooling layers.  The latter aggressively reduce image size and
consequently train more quickly than the former.

*  Adding final dropout layers in addition to batch normalization did not
help reduce data overfitting.

*  A final fully connected layer does not benefit from having more than 6
times as many nodes as the number of classification states.

In the end, we empirically found particular "super-layer" combinations of
2D convolution with 5x5 kernels, batch normalization, leaky ReLU activation
[8], 2x2 pooling and fully connected layers to be most useful:

CBLP: Conv, Batch Norm, Leaky Relu, Pool

CBL: Conv, Batch Norm, Leaky Relu

Dense: Batch Norm, Leaky Relu, Dense Conv with no bottleneck

FC: Fully connected, Batch Norm, Leaky Relu

We choose these super-layers' parameters along with their number in network
configurations so that total receptive field and training image pixel sizes
are comparable [9].  Our top three CNN architectures all have receptive
field size R=157:

9 super-layer architecture: 
    3 x [CBLP], 4 x [CBL], 2 x [FC]

10 super-layer architecture: 
    2 x [CBLP], 2 x [CBL], CBLP, 3 x [CBL], 2 x [FC]

10 super-layer densenet architecture:
    2 x [CBLP], 2 x [Dense], CBLP, 3 x [Dense], 2 x [FC]

In the end, we empirically found particular "super-layer" combinations of
2D convolution with 5x5 kernels, batch normalization, leaky ReLU
activation, 2x2 pooling and fully connected layers to be most useful:

CBLP: Conv, Batch Norm, Leaky Relu, Pool

CBL: Conv, Batch Norm, Leaky Relu

Dense: Batch Norm, Leaky Relu, Dense Conv with no bottleneck

FC: Fully connected, Batch Norm, Leaky Relu

We choose these super-layers' parameters along with their number in network
configurations so that total receptive field and training image pixel sizes
are comparable [ref].  Our top three CNN architectures all have receptive
field size R=157:

9 super-layer architecture: 
    3 x [CBLP], 4 x [CBL], 2 x [FC]

10 super-layer architecture: 
    2 x [CBLP], 2 x [CBL], CBLP, 3 x [CBL], 2 x [FC]

10 super-layer densenet architecture:
    2 x [CBLP], 2 x [Dense], CBLP, 3 x [Dense], 2 x [FC]



<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: left">Hyperparameter description</th>
      <th style="text-align: center">Value    </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Number of training chips per epoch</td>
      <td style="text-align: center">22434</td>
    </tr>
    <tr>
      <td style="text-align: left">Number of validation chips per epoch</td>
      <td style="text-align: center">2493</td>
    </tr>
    <tr>
      <td style="text-align: left">Minibatch size</td>
      <td style="text-align: center">100</td>
    </tr>
    <tr>
      <td style="text-align: left">Optimizer</td>
      <td style="text-align: center">ADAM</td>
    </tr>

    <tr>
      <td style="text-align: left">Base learning rate</td>
      <td style="text-align: center">0.01</td>
    </tr>
    <tr>
      <td style="text-align: left">Leaky ReLU alpha</td>
      <td style="text-align: center">0.1</td>
    </tr>
  </tbody>
</table>

*Table 1.  List of common hyperparameters used to train a CNN ensemble.*



## Visualizing CNN Learning

## References

1.  [See the 2013 Kaggle dogs vs cats
competition.](https://www.kaggle.com/c/dogs-vs-cats)

2.  [P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Ferbus and Y. LeCun,
"OverFeat: Integrated Recognition, Localization and Detection using
Convolutional Networks," ICLR $\small{(2014)}$,
16.](https://arxiv.org/abs/1312.6229)

3.  [S. Ioffe and C. Szegedy, "Batch Normalization: Accelerating Deep
Network Training by Reducing Internal Covariate Shift," 32nd ICML 
$\small{(2015)}$.](https://arxiv.org/abs/1502.03167)

4.  [D.P. Kingma and J. Ba, "Adam: A method for Stochastic Optimization,"
3rd ICLR $\small{(2014)}$.](https://arxiv.org/abs/1412.6980)

5.  [See TensorFlow: An open-source software library for Machine
Intelligence.](https://www.tensorflow.org/)

6.  [G. Huang, Z. Liu, K.Q. Weinberger, L. van der Maaten, "Densely
Connected Convolutional Networks," CVPR $\small{(2017)}$.](https://arxiv.org/abs/https://arxiv.org/abs/1608.06993)

7.  [A partial list of labeling errors within the Kaggle dogs vs cats
training corpus has been compiled by Fabian
Linzberger.](https://gist.github.com/lefant/bb9c304cfcf5da0f9f652570cc54893a)

8.  [A.L. Maas, A.Y. Hannun and A.Y. Ng, "Rectifier Nonlinearities Improve
Neural Network Acoustic Models", 50th ICML $\small{(2013)}$.](https://web.stanford.edu/~awni/papers/relu_hybrid_icml2013_final.pdf)

9.  [D.H.T Hien, "A guide to receptive field arithmetic for Convolutional
Neural Networks," $\small{(2017)}$.](https://medium.com/@nikasa1889/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807)


10.  [T.G. Dietterich, "Ensemble Methods in Machine Learning," International
Workshop on Mutiple Classifier Systems $\small{(2000)}$ 1.](http://web.engr.oregonstate.edu/~tgd/publications/mcs-ensembles.pdf)

11.  [C. Olah, A. Mordvintsev and L. Schubert, "Feature Visualization,"
Distill $\small{(2017)}$.](https://distill.pub/2017/feature-visualization/)


