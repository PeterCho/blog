---
layout: post
title:  "Solving Mazes via Deep Reinforcement Learning"
date:   2016-11-19 12:00:00
categories: Deep Learning
use_math: true
---

## Reinforcement learning

In a standard reinforcement learning setting, an agent sequentially
interacts with its environment and learns a mapping from states to actions
by trial-and-error [1,2].  The agent observes the state of the environment
and then chooses some action to perform.  The action generally changes the
environment's state, and the environment scores the change via a reward
signal.  Over time, the agent learns to perform actions which maximize its
cumulative reward when starting from some initial state and proceeding
towards a terminal state.

Reinforcement learning (({\rm RL})) has been applied to a wide array of
practical problems which include robot control, autonomous flight, resource
allocation, route planning and financial trading [3,4].  Game playing
represents another RL application which is perhaps impractical but
pedagogically instructive.  In 2015, Deep Mind researchers developed an RL
system which achieved super-human performance on multiple Atari games [5].
Even more impressively, another RL system made newspaper headlines when it
recently beat a world champion in the game of Go [6].  Reinforcement
learning arguably demonstrates the first signs of true artificial
intelligence.

In this blog entry, we dip our toes into reinforcement learning by
constructing a system which solves mazes.  We intentionally focus here upon
maze path finding which is nontrivial yet sufficiently simple to be
solvable via non-RL techniques.  We can thus monitor the progress of an RL
agent as it iteratively converges towards a unique solution.

We start by building an RL system which implements a Q value function by
table lookup.  We then replace the table over environment states and agent
actions with a neural network that performs function approximation.  Though
deep Q learning is overkill for maze solving, this approach can be applied
to a much larger and more difficult set of problems in the future.

## Tabular Q learning

In order to generate a random maze of specified size, we first implement a
standard depth-first search algorithm [7]. The maze starts as a 2D lattice
of cells which are each bounded by four walls.  We initially take the cell
located in the bottom right corner of the lattice as the current cell.
After it is marked as visited, one of the current cell's unvisited
neighbors is randomly chosen, and the current cell is pushed onto a stack.
The walls between the current and neighbor cells are then removed.  The
neighbor cell is reset as the current cell, and it is marked as visited.
If all of the current cell's neighbors have previously been visited, we pop
a cell from the stack and take it as the current cell.  This recursive
process continues until every cell in the maze has been visited.

An example of a 10x10 maze generated via this depth-first search approach
is illustrated on the left side of figure 1.  The graphics routines used to
render the maze are based upon C++ codes in [8].  The solution to this maze
can readily be found by again initializing the bottom right corner of the
lattice as the current cell.  Looping over up, right, down and left
directions, we ignore any direction vector for which the current cell has a
wall.  We further ignore any direction vector for which the neighbor cell
already has an assigned direction.  Otherwise, the neighbor cell is
assigned a direction vector pointing towards the current cell, and
direction finding is recursively performed on the neighbor cell.  The
resulting flow field is illustrated on the right side of figure 1.

![MazeRL]({{site.url}}/blog/images/maze_rl/nondeepQ/empty_vs_solved_mazes.png)
*Fig. 1. ((Left)) A 10x10 maze generated via a depth-first search
algorithm.  ((Right)) Green arrows indicate directions to follow in
order to reach the blue X in the bottom right corner starting from any cell
within the maze.*

We now want to reproduce this unique solution for the 10x10 maze using
"Q-learning" which represents a model-free form of RL [9].  Q-learning
finds a mapping from state/action pairs to so-called Q-values.  $Q =
Q^\pi(s,a)$ is the expected reward for executing action $a$ at state $s$
and following policy $\pi$ thereafter.  Once an agent learns all possible
Q-values, its optimal policy is simply given by $\pi^* (s) = a^* $ where
$a^* $ is the action which maximizes $Q(s,a)$.

The agent's Q-learning experience consists of a sequence of steps.  In the
$n^{th}$ step, the agent

* observes its current state $s_n$,

* selects and performs an action $a_n$,

* receives a reward $r_n$,

* transitions to subsequent state $s'_{n+1}$,

* adjusts its Q-value according to the off-policy Temporal-Difference
(({\rm TD})) control equation

$$Q(s_n,a_n)  \leftarrow
(1 - \alpha) Q(s_n,a_n) + \alpha \bigl [ r_n + \gamma \, {\rm max}_b
Q(s'_{n+1},b) \bigr ].  $$

Here $0 \le \alpha \le 1$ represents a learning rate, while $0 \le \gamma
\le 1$ denotes a discount factor for future rewards [9].

For our maze problem, lattice cells represent states, and single-cell
movements in up, right, down or left directions correspond to actions.  If
the agent performs an illegal move which causes it to crash into an
internal or external maze wall, the agent immediately loses the game.  On
the other hand, if the agent successfully navigates its way through the
maze to the bottom right corner cell marked by the blue "X" in figure 1b,
it wins the game.  For RL systems learning to play games, it is common that
the reward function returns zero for all moves except at terminal states.
So if the the agent wins [loses] the game, we assign it rewards +1 [-1].
After playing many game episodes, the agent should learn by trial-and-error
which set of states and actions typically lead to the winning end state.

We initialize $Q(s,a)$ with random values uniformly ranging over interval
[-1,1].  At the start of each new episode, we also randomly select the
agent's starting location within the maze or equivalently its initial
state.  For every step in the game, the agent's action is chosen randomly
among the four possible direction vectors.  Working with a learning rate
$\alpha= 0.75$ and reward discount factor $\gamma = 0.95$, we then
iteratively update the Q-values using the off-policy TD control
equation.

$$ \rho  \equiv 
{\rm Number \; of \; maze \; cells \; whose \; Q \; values \;
yield \; correct \; direction \; actions \over Number \; of \; maze \; 
cells} $$

![MazeRL]({{site.url}}/blog/images/maze_rl/nondeepQ/tabular_Q_convergence.png)
*Fig. XX. Fractional correct maze cell direction actions plotted versus
episode number during tabular Q learning.*

<a href="http://www.youtube.com/watch?feature=player_embedded&v=QXr8LJHTpNE"
target="_blank"><img src="http://img.youtube.com/vi/QXr8LJHTpNE/0.jpg"
alt="IMAGE ALT TEXT HERE" width="720" height="480" border="10" /></a>
*Fig. XX.  Movie illustrating iterative solving of the 10x10 maze in figure
XX via tabular Q learning .  Click on image above to play video.*


## Deep Q learning

The numbers of nodes in each layer of a CNN are freely chosen
hyperparameters.  We let fully connected layers 5 and 6 in Model II contain
256 neurons.  FC5 and FC6 activation values thus form 256-dimensional
global descriptors for input test images.  But many of the FC5 and FC6
descriptor coordinates are strongly correlated.  So the genuine dimensions

![MazeRL]({{site.url}}/blog/images/maze_rl/deepQ/log10_losses_history.png)
*Fig. XX. Logarithm base 10 of loss function plotted versus episode number
during deep Q learning.*

![MazeRL]({{site.url}}/blog/images/maze_rl/deepQ/deep_Q_convergence.png)
*Fig. XX. Fractional correct maze direction actions plotted versus episode
number during deep Q learning.*

<a href="http://www.youtube.com/watch?feature=player_embedded&v=KW0X0gB1B2I"
target="_blank"><img src="http://img.youtube.com/vi/KW0X0gB1B2I/0.jpg"
alt="IMAGE ALT TEXT HERE" width="720" height="480" border="10" /></a>
*Fig. XX.  Movie illustrating solving of a 10x10 maze via deep Q learning .  Click on image above to play video.*


![MazeRL]({{site.url}}/blog/images/maze_rl/deepQ/trained_padded_weights.png)
*Fig. XX. Visualization of trained weights in first layer of neural
network.*

## References

1.  [R.S. Sutton and A.G. Barton, "Reinforcement Learning: An
Introduction", 2nd edition
((2016)). ](https://webdocs.cs.ualberta.ca/~sutton/book/bookdraft2016sep.pdf)

2.  [M.E. Harmon and S.S. Harmon, "Reinforcement Learning: A Tutorial"
((1997)). ](http://www.dtic.mil/dtic/tr/fulltext/u2/a323194.pdf)

3.  [S. Singh, "Successes of Reinforcement Learning" ((2009)).](http://umichrl.pbworks.com/w/page/7597597/Successes%20of%20Reinforcement%20Learning)

4.  [J. Schulman, "Deep Reinforcement Learning", Machine Learning
Summer School ((2016)).](http://learning.mpi-sws.org/mlss2016/slides/2016-MLSS-RL.pdf)

5.  [V. Mnih et al, "Human-level control through deep reinforcement
learning", Nature ((2015) 529-533.](
http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html)

6.  [D. Silver et al, "Mastering the game of Go with deep neural networks
and tree search", Nature ((2016)), 484 - 489.](http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html)

7.  [See the "Depth-first search" section in the "Maze generation
algorithm" Wikipedia
entry.](https://en.wikipedia.org/wiki/Maze_generation_algorithm)

8.  [S. Kosarevsky, "Depth-first Seach Random Maze Generator ((2014)). ](https://github.com/corporateshark/random-maze-generator)

9.  [C. Watkins and P. Dayan, "Technical Note: Q-Learning", Machine
Learning ((1992)) 279.](http://www.gatsby.ucl.ac.uk/~dayan/papers/cjch.pdf)



*.  [V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou,
D. Wierstra and M. Riedmiller, "Playing Atari with Deep Reinforcement
Learning" ((2013)).](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)


*.  [T. Matiisen, "Demystifying Deep Reinforcement Learning", ((2016)).](https://www.nervanasys.com/demystifying-deep-reinforcement-learning/)

*.  [D. Silver, "Deep Reinforcement Learning", ICLR ((2015)).](http://www.iclr.cc/lib/exe/fetch.php?media=iclr2015:silver-iclr2015.pdf)

*.  [M. Riedmiller, "Neural Fitted Q Iteration - First Experiences with a
Data Efficient Neural Reinforcement Learning Method" ((2005)).](http://ml.informatik.uni-freiburg.de/_media/publications/rieecml05.pdf)


*.  [A. Kaparthy, "Deep Reinforcement Learning: Pong from Pixels", ((2016)).](
http://karpathy.github.io/2016/05/31/rl/)



$$ a^2+b^2 = c^2 $$

Let's test some inline math $x$, $y$, $x_1$, $y_1$.

Test a display math:
$$
   |\psi_1\rangle = a|0\rangle + b|1\rangle
$$

Test a display math with equation number:
$$
  \begin{align}
    |\psi_1\rangle &= a|0\rangle + b|1\rangle \\
    |\psi_2\rangle &= c|0\rangle + d|1\rangle
  \end{align}
$$





