---
layout: post
title:  "Solving Mazes via Reinforcement Q Learning"
date:   2016-11-19 12:00:00
categories: Deep Learning
use_math: true
---

## Reinforcement learning

In a standard reinforcement learning setting, an agent sequentially
interacts with its environment and learns a mapping from states to actions
by trial-and-error [1,2].  The agent observes the state of the environment
and then chooses some action to perform.  The action generally changes the
environment's state, and the environment scores the change via a reward
signal.  Over time, the agent learns to perform actions which maximize its
cumulative reward when starting from some initial state and proceeding
towards a terminal state.

Reinforcement learning (({\rm RL})) has been applied to a wide array of
practical problems which include robot control, autonomous flight, resource
allocation, route planning and financial trading [3,4].  Game playing
represents another RL application which is perhaps impractical but
pedagogically instructive.  In 2015, Deep Mind researchers developed an RL
system which achieved super-human performance on multiple Atari games [5].
Even more impressively, another RL system made newspaper headlines when it
recently beat a world champion in the game of Go [6].  Reinforcement
learning arguably demonstrates the first signs of true artificial
intelligence.

In this blog entry, we dip our toes into reinforcement learning by
constructing a system which solves mazes.  We intentionally focus here upon
maze path finding which is nontrivial yet sufficiently simple to be
solvable via non-RL techniques.  We can thus monitor the progress of an RL
agent as it iteratively converges towards a unique solution.

We start by building an RL system which implements a Q value function by
table lookup.  We then replace the table over environment states and agent
actions with a neural network that performs function approximation.  Though
deep Q learning is overkill for maze solving, this approach can be applied
to a much larger and more difficult set of problems in the future.

## Tabular Q learning

In order to generate a random maze of specified size, we first implement a
standard depth-first search algorithm [7]. The maze starts as a 2D lattice
of cells which are each bounded by four walls.  We initially take the cell
located in the bottom right corner of the lattice as the current cell.
After it is marked as visited, one of the current cell's unvisited
neighbors is randomly chosen, and the current cell is pushed onto a stack.
The walls between the current and neighbor cells are then removed.  The
neighbor cell is reset as the current cell, and it is marked as visited.
If all of the current cell's neighbors have previously been visited, we pop
a cell from the stack and take it as the current cell.  This recursive
process continues until every cell in the maze has been visited.

An example of a $10 \times 10$ maze generated via this depth-first search
approach is illustrated on the left side of figure 1.  The graphics
routines used to render the maze are based upon C++ codes in [8].  The
solution to this maze can readily be found by again initializing the bottom
right corner of the lattice as the current cell.  Looping over up, right,
down and left directions, we ignore any direction vector for which the
current cell has a wall.  We further ignore any direction vector for which
the neighbor cell already has an assigned direction.  Otherwise, the
neighbor cell is assigned a direction vector pointing towards the current
cell, and direction finding is recursively performed on the neighbor cell.
The resulting flow field is illustrated on the right side of figure 1.

![MazeRL]({{site.url}}/blog/images/maze_rl/nondeepQ/empty_vs_solved_mazes.png)
*Fig. 1. [Left] A 10x10 maze generated via a depth-first search
algorithm.  [Right] Green arrows indicate directions to follow in order
to reach the bottom right corner starting from any cell within the maze.*

We now want to reproduce this unique solution for the $10 \times 10$ maze
using "Q-learning" which represents a model-free form of RL [9].
Q-learning finds a mapping from state/action pairs to so-called Q-values.
$Q = Q^\pi(s,a)$ is the expected reward for executing action $a$ at state
$s$ and following policy $\pi$ thereafter.  Once an agent learns all
possible Q-values, its optimal policy is simply given by $\pi^* (s) = a^* $
where $a^* $ is the action which maximizes $Q(s,a)$.

The agent's Q-learning experience consists of a sequence of steps.  In the
$n^{th}$ step, the agent

* observes its current state $s_n$,

* selects and performs an action $a_n$,

* receives a reward $r_n$,

* transitions to subsequent state $s'_{n+1}$,

* adjusts its Q-value according to the off-policy Temporal-Difference
(({\rm TD})) control equation

$$ \begin{align}
Q(s_n,a_n)  \leftarrow
(1 - \alpha) Q(s_n,a_n) + \alpha \bigl [ r_n + \gamma \, {\rm max}_b
Q(s'_{n+1},b) \bigr ].  \\
  \end{align}
$$

Here $0 \le \alpha \le 1$ represents a learning rate, while $0 \le \gamma
\le 1$ denotes a discount factor for future rewards [9].

For our maze problem, lattice cells represent states, and single-cell
movements in up, right, down or left directions correspond to actions.  If
the agent performs an illegal move which causes it to crash into an
internal or external maze wall, the agent immediately loses the game.  On
the other hand, if the agent successfully navigates its way through the
maze to the bottom right corner cell marked by the blue "X" in figure 1b,
it wins the game.  For RL systems learning to play games, it is common that
the reward function returns zero for all moves except at terminal states.
So if the the agent wins [loses] the game, we assign it rewards +1 [-1].
After playing many game episodes, the agent should learn by trial-and-error
which set of states and actions typically lead to the winning end state.

We initialize $Q(s,a)$ with random values uniformly ranging over the
interval [-1,1].  At the start of a new episode, we also randomly select
the agent's beginning location in the maze or equivalently its initial
state.  For every step in the game, the agent's action is chosen randomly
among up, right, left and down movements.  Working with learning rate
$\alpha= 0.75$ and reward discount factor $\gamma = 0.95$, we then
iteratively update the Q-values using equation ((1)).

As the solution to the $10 \times 10$ maze is already known, we can monitor
the progress of tabular Q-learning towards finding the answer presented in
figure 1b.  After each trial episode, we simply count the number of maze
cells for which $\max_a Q(s,a)$ yields the correct direction.  Normalizing
by the total number of maze cells, we plot the ratio in figure 2 as a
function of episode number.  After approximately 16K episodes,
trial-and-error tabular Q-learning solves the maze.

![MazeRL]({{site.url}}/blog/images/maze_rl/nondeepQ/tabular_Q_convergence.png)
*Fig. 2. Fractional correct maze cell direction actions plotted versus
episode number during tabular Q learning.*

Figure 3 presents a movie which displays the evolution of tabular Q
learning over time.  Since the $Q(s,a)$ is initially filled with noise, its
maximal action directions are randomly oriented at the movie's beginning.
Flow field directions which cause the agent to illegally crash into an
interior or exterior maze wall are colored red.  Legal, though perhaps
incorrect, maze movements are indicated by arrows colored green.
Neighboring pairs of arrows pointing in contradictory directions are
colored orange.  As the movie plays, we observe that the agent rapidly
learns to not crash into walls.  It takes longer for all of the
contradictory direction pairs to be resolved.  But eventually,
reinforcement learning settles upon a self-consistent maze solution which
agrees with the result in figure 1b.

<a href="http://www.youtube.com/watch?feature=player_embedded&v=QXr8LJHTpNE"
target="_blank"><img src="http://img.youtube.com/vi/QXr8LJHTpNE/0.jpg"
alt="IMAGE ALT TEXT HERE" width="720" height="480" border="10" /></a>
*Fig. 3.  Movie illustrating tabular Q learning for the 10x10 maze in
figure 1a.  Click on image above to play video.*

## Deep Q learning

For agents with relatively small numbers of states and actions, it is
feasible to explicitly compute $Q(s,a)$ in table form.  But many
interesting applications of reinforcement learning have huge numbers of
possible states which renders tabular Q-learning totally impractical.  For
such cases, we must approximate $Q(s,a)$ via some function which can be
learned by trial-and-error.  Over the past decade, researchers have found a
set of tricks which enable neural networks to act as function approximators
for Q-learning [10-12].  We apply their techniques to the maze solving
problem.

We first represent a maze with $m \times m$ cells by a $(2m-1) \times
(2m-1)$ occupancy grid.  Grid locations with zero occupancy correspond to
traversable maze cells.  The grid location corresponding to the agent's
current maze position is set to occupancy one.  On the other hand, the
$n_{wall \; cells}$ locations in the occupancy grid representing maze walls
are set equal to $-1 / n_{wall \; cells}$.  The sum of all occupancy grid
values consequently equals zero.  We flatten the rows of the occupancy grid
into a single column vector which becomes the input layer to the neural
network.

The network's output layer contains the number of nodes needed to represent
all possible actions $a$ corresponding to any input state $s$.  So for maze
problems, our network's contains 4 outputs corresponding to up, right, down
and left agent movements.  The values of the output nodes equal $Q(s,a)$.
It is important to note that no a priori constraint is placed upon illegal
actions for state $s$.  Instead, the agent should learn via negative
reinforcement to avoid illegal moves.

As for all other neural networks, we must choose a set of hyperparameters
which govern how our maze network operates and trains.  After running
several experiments, we select a fully-connected network configuration with
two hidden layers that each contain 10 nodes.  Following ref. [13], we
choose not to include bias terms into this network.  It consequently
contains 3750 variables which must be fit.

We adopt ReLU nonlinearities in all nodes except those in the final layer.
The last layer generates activations by an identity operator.  During
network training via conventional back propagation, we initialize the
learning rate to 0.0003 and slowly decrease it to 0.0001.  Rather than
employ conventional stochastic gradient descent to update network weights,
we instead utilize RMS propagation with a 0.9 decay rate [13, 14].

Following references [11, 12], we take action $a_n$ at the $n^{th}$ step in
an episode according to an $\epsilon$-greedy policy.  At the start of
network training, $\epsilon = 1$.  But after every 1000 episodes, $\epsilon
\rightarrow \max(0.9 \, \epsilon, 0.01)$.  We also store up to the last $5
m^2$ transitions $(s_n, a_n, r_n, s'_{n+1})$ within a replay memory ${\cal
D}$.  After every episode has finished, a random mini-batch of 

$$ N_d = 0.1 \times [{\rm replay \; memory \; capacity}]$$ 

transitions is sampled from ${\cal D}$.  Working with such random
transitions helps break temporal correlations in the training data and
allows the network to learn from a wider range of past policies.


All weights within our neural network are Xavier initialized as gaussian
random variables distributed according to ${\cal N}(0, 1/\sqrt{n_{in}})$.
During network training, the weights $\bf w$ are iteratively adjusted so as
to minimize the mean square error loss function

$$ \begin{align}
L({\bf w}) = 
{1 \over N_d} \sum_{d \in {\cal D}} 
{1 \over 2} \bigl[ {\rm target} - Q(s, a, {\bf w}) \bigr]^2 \\
  \end{align}
$$

where

$$ {\rm target} = \begin{cases}
& r + \gamma \max_{a'} Q(s', a')   & {\rm if} \; s' \ne {\rm terminal \;
state} \\
& r \qquad\qquad\qquad\qquad\quad  & {\rm if} \; s' = {\rm terminal \; state. }
\end{cases}
$$

The target term appearing within the loss function plays a role in
reinforcement learning analogous to a class label in supervised learning.
Researchers have previously found that using frequently-updated weights in
a Q-learning target often leads to neural network training oscillations and
instabilities [11, 12].  So they recommend computing the target with a set
of old, fixed ${\bf w^-}$ parameters and periodically updating ${\bf w^-}$.
We consequently copy ${\bf w} \to {\bf w^-}$ only after every tenth
episode.

We train the network using our own C++ codes rather than an off-the-shelf
deep learning framework.  Over time, the loss function in eqn. (2) jumps
around while generally decreasing.  The logarithmic plot in figure 4
demonstrates such loss function behavior for one particular $10 \times 10$
maze.  Actions inferred from neural network Q-value outputs also converge
to the solution for this particular maze after O(({\rm 150K})) training
episodes (({\rm see \; figure \; 5})).  The movie in figure 6 displays the
evolution of the flow-field from random orientations to the self-consistent
answer as Q-learning progresses.

![MazeRL]({{site.url}}/blog/images/maze_rl/deepQ/log10_losses_history.png)
*Fig. 4. Base-10 logarithm of loss function plotted versus episode number
during deep Q learning for a $10 \times 10$ maze.*

![MazeRL]({{site.url}}/blog/images/maze_rl/deepQ/deep_Q_convergence.png)
*Fig. 5. Fractional correct maze direction actions plotted versus episode
number during deep Q learning for a $10 \times 10$ maze.*

<a href="http://www.youtube.com/watch?feature=player_embedded&v=KW0X0gB1B2I"
target="_blank"><img src="http://img.youtube.com/vi/KW0X0gB1B2I/0.jpg"
alt="IMAGE ALT TEXT HERE" width="720" height="480" border="10" /></a>
*Fig. 6.  Movie illustrating deep Q learning for a 10x10 maze.  Click on
image above to play video.*

It is interesting to examine trained weight values in order to gain insight
into how the neural network learns.  For visualization purposes only, we
renormalize the weights linking the input and first hidden layers in our
network so that they all lie inside the interval [0,1].  For each of the 10
nodes in the first hidden layer, its fully-connected weights are reformed
into a $(2 \times 10 - 1) \times (2 \times 10 -1)$ matrix.  Figure 7
displays these matrices whose low [high] rescaled weight values are colored
blue [red].

![MazeRL]({{site.url}}/blog/images/maze_rl/deepQ/trained_padded_weights.png)
*Fig. 7. Visualization of trained weights which connect the neural
network's input and first hidden layers.*

Comparing the coloring patterns for the trained weights in figure 7 with
the structure of the $10 \times 10$ maze in figure 6, we clearly see that
the network learns the wall layout for the maze.  While wall knowledge is
an obvious prerequisite for solving a maze, it is reassuring to confirm
that the agent develops this basic understanding about its environment via
reinforcement learning.  

We had hoped to find other clear clues about network maze solving within
its first set of trained weights.  Some of the warmer-colored occupancy
cells in figure 7 tantalizingly correspond to dead ends or long traversable
stretches inside the maze.  But after inspecting analogous weight matrices
for several other randomly-generated $10 \times 10$ mazes, we did not find
a consistent pattern among larger trained weight values.  

Perhaps more insight into could be gained by visualizing trained weight
matrices linking higher layers in the neural network.  We leave such
exploration for future work.

## References

1.  [R.S. Sutton and A.G. Barton, "Reinforcement Learning: An
Introduction", 2nd edition
((2016)). ](https://webdocs.cs.ualberta.ca/~sutton/book/bookdraft2016sep.pdf)

2.  [M.E. Harmon and S.S. Harmon, "Reinforcement Learning: A Tutorial"
((1997)). ](http://www.dtic.mil/dtic/tr/fulltext/u2/a323194.pdf)

3.  [S. Singh, "Successes of Reinforcement Learning" ((2009)).](http://umichrl.pbworks.com/w/page/7597597/Successes%20of%20Reinforcement%20Learning)

4.  [J. Schulman, "Deep Reinforcement Learning", Machine Learning
Summer School ((2016)).](http://learning.mpi-sws.org/mlss2016/slides/2016-MLSS-RL.pdf)

5.  [V. Mnih et al, "Human-level control through deep reinforcement
learning", Nature ((2015) 529-533.](
http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html)

6.  [D. Silver et al, "Mastering the game of Go with deep neural networks
and tree search", Nature ((2016)), 484 - 489.](http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html)

7.  [See the "Depth-first search" section in the "Maze generation
algorithm" Wikipedia
entry.](https://en.wikipedia.org/wiki/Maze_generation_algorithm)

8.  [S. Kosarevsky, "Depth-first Seach Random Maze Generator ((2014)). ](https://github.com/corporateshark/random-maze-generator)

9.  [C. Watkins and P. Dayan, "Technical Note: Q-Learning", Machine
Learning ((1992)) 279.](http://www.gatsby.ucl.ac.uk/~dayan/papers/cjch.pdf)

10.  [M. Riedmiller, "Neural Fitted Q Iteration - First Experiences with a
Data Efficient Neural Reinforcement Learning Method" ((2005)).](http://ml.informatik.uni-freiburg.de/_media/publications/rieecml05.pdf)

11.  [V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou,
D. Wierstra and M. Riedmiller, "Playing Atari with Deep Reinforcement
Learning" ((2013)).](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)

12.  [D. Silver, "Deep Reinforcement Learning", ICLR ((2015)).](http://www.iclr.cc/lib/exe/fetch.php?media=iclr2015:silver-iclr2015.pdf)

13.  [A. Kaparthy, "Deep Reinforcement Learning: Pong from Pixels", ((2016)).](
http://karpathy.github.io/2016/05/31/rl/)

14. [G. Hinton, "Neural Networks for Machine Learning", Slide 29 from
Lecture 6 Coursera notes,
((2012)).](https://www.coursera.org/learn/neural-networks)

