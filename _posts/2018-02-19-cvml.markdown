---
layout: post
title:  "Synthesizing Face and Flower Images via DCGANs, WGANs and BEGANs"
date:   2018-02-19 12:00:00
categories: Deep Learning
use_math: true
---

## Three members of the GAN zoo 

Generative Adversarial Networks $($GANS$)$ belong to a family of generative
models that learn to sample from specified data distributions.  GANs
involve a game between two competing players [1].  The first is called the
generator which creates samples mimicing those within the training data.
The second player called the discriminator assesses whether samples
genuinely originate from the training corpus.  The discriminator is trained
via conventional supervised learning to classify inputs as either real or
fake.  The generator is simultaneously trained to fool the discriminator.
Both players improve their capabilities when GAN training is successful.

Current applications of GANs often center upon image synthesis.  The
generator $G$ is a differentiable function typically represented by a deep
neural network with parameters $\theta_G$.  $G$ maps a latent variable $z$
drawn from some simple distribution to an output image $x$.  The
discriminator $D$ is another differentiable function represented by a
neural network with parameters $\theta_D$.  $D$ takes an input image $x$
and generically returns the probability of its belonging to the training
dataset.

The discriminator and generator are trained by minimizing two loss
functions $L_D(\theta_D, \theta_G)$ and $L_G(\theta_D, \theta_G)$.  $D$
tries to minimize $L_D$ while controlling only $\theta_D$, and $G$
minimizes $L_G$ while controlling only $\theta_G$.  The solution to this
game is a Nash equilibrium which is a local minimum of $L_D$ with respect
to $\theta_D$ as well as a local minimum of $L_G$ with respect to
$\theta_G$ [2].

Over the past few years, an entire zoo of GANs has been proposed and
studied [3].  Models within this large collection differ depending upon
their deep network designs, loss functions and training procedures.  In
this posting, we study three particular GAN variants of increasing
complexity:

I.  Deep Convolutional GANs $($DCGANs$)$

In 2016, Radford, Metz and Chintala performed an extensive search for CNN
architectures that enable stable GAN training [4].  These investigators
chose to work with all-convolutional networks from which pooling and fully
connected layers were banished.  The authors also incorporated batch
normalization into the generator and discriminator [5], and they selected
ReLU and leaky ReLU [6] activation functions.  Most interestingly, Radford
et al demonstrated nontrivial manipulation of generated image content via
simple vector arithmetic on latent space descriptors.

II.  Wasserstein GANs $($WGANs$)$

In 2017, Arjovsky, Chintala and Bottou demonstrated that the standard GAN
loss function introduced in [1] is based upon minimizing the
Kullback-Leibler (KL) divergence between an underlying data distribution
and a model's approximation to that distribution [7].  These authors
described problems arising from the KL divergence and proposed replacing it
with an Earth Mover distance function.  They also introduced a measure of
convergence that can be monitored during training and empirically
demonstrated it correlates well with generated sample quality.  Refinements
to the original WGAN model's training procedure were reported in ref. [8].

III.  Boundary Equilibrium GANs $($BEGANs$)$

In 2017, Berthelot, Schumm and Metz proposed using proportional control
theory to maintain an equilibrium during training between the discriminator
and generator [9].  These authors employed an autoencoder for the GAN
discriminator with a per-pixel loss.  They further introduced an adjustable
parameter $\gamma$ which trades off between generated image diversity and
discriminator reconstruction quality.  Like WGANs, BEGANs provide an
approximate convergence measure which helps monitoring of training
progress.

We implement these three GAN types and assess their relative performance in
the following section.

## Face and flower image generation

We work with two sets of curated imagery in order to train GANs.  The first
is the "CelebA" dataset containing O$($220K$)$ celebrity faces [10].  The
second is a combination of 17-category and 102-category "Oxford flower"
datasets with O$($9.5K$)$ pictures [11].  We increase the number and
variation of flower images by dynamically augmenting the originals via
random horizontal flips, 90 degree rotations and HSV color modulations.
Representative examples of face and flower training images are presented in
figures 1 and 2.

![BEGAN]({{site.url}}/blog/images/gans_faces_flowers/training_faces_64x64.png)
*Fig. 1.  Tensorboard display of representative face images from the CelebA
training set.*

![BEGAN]({{site.url}}/blog/images/gans_faces_flowers/training_flowers_64x64.png)
*Fig. 2.  Tensorboard display of dynamically augmented images from two
Oxford flower datasets.*

Over a series of 150+ experiments performed within the TensorFlow framework
[12], we used these images to train DCGANs, WGANs and BEGANs with varying
architectures and hyperparameters.  They involved convolutions and
transposed convolutions [13, 14] with 3x3 and 5x5 kernels.  We also tried
incorporating batch normalization [5], layer normalization [15] and no
normalization.  We further experimented with ReLU, leaky ReLU and
exponential linear units [17] as nonlinear activations functions.

The DCGAN architecture we adopted closely follows that of Radford et al
[4].  The particular "super layer" combinations of convolutions,
normalizations and nonlinearities in our DCGAN discriminator and generator
networks for synthesizing 64x64 images are listed in Table 1.  The
receptive fields of these networks equals 61, and they contain O$($36.6M$)$
trainable parameters.  We employed similar discriminator and generator
architectures for WGAN experiments.  But following [8], we utilized a
gradient penalty term within the loss function rather than weight clipping
as in the original WGAN paper [7].  Moreover, we replaced batch
normalization with layer normalization and dispensed with a final sigmoid
activation in the WGAN discriminator.

<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: left">DCGAN/WGAN discriminator super layer</th>
      <th style="text-align: center">Activation volume [width x height x
channels] </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Input image</td>
      <td style="text-align: center">64 x 64 x 3</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv5, Leaky Relu, stride 2</td>
      <td style="text-align: center">32 x 32 x 128</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv5, Norm, Leaky Relu, stride 2</td>
      <td style="text-align: center">16 x 16 x 256</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv5, Norm, Leaky Relu, stride 2</td>
      <td style="text-align: center">8 x 8 x 512</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv5, Norm, Leaky Relu, stride 2</td>
      <td style="text-align: center">4 x 4 x 1024</td>
    </tr>
    <tr>
      <td style="text-align: left">Linear proj, Activation</td>
      <td style="text-align: center">1</td>
    </tr>

    <!---  Add empty line in final table row -->

    <tr>
      <td style="text-align: left">$ $</td>
      <td style="text-align: center">$ $</td>
    </tr>
</tbody>
</table>

<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: left">DCGAN/WGAN generator super layer</th>
      <th style="text-align: center">Activation volume [width x height x
channels] </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Input descriptor</td>
      <td style="text-align: center">$Z_{dim}$</td>
    </tr>
    <tr>
      <td style="text-align: left">Linear proj, Batch norm, Relu</td>
      <td style="text-align: center">4 x 4 x 1024</td>
    </tr>
    <tr>
      <td style="text-align: left">Transposed conv5, Batch norm, ReLU</td>
      <td style="text-align: center">8 x 8 x 512</td>
    </tr>
    <tr>
      <td style="text-align: left">Transposed conv5, Batch norm, ReLU</td>
      <td style="text-align: center">16 x 16 x 256</td>
    </tr>
    <tr>
      <td style="text-align: left">Transposed conv5, Batch norm, ReLU</td>
      <td style="text-align: center">32 x 32 x 128</td>
    </tr>
    <tr>
      <td style="text-align: left">Transposed conv5, Tanh</td>
      <td style="text-align: center">64 x 64 x 3</td>
    </tr>

    <!---  Add empty line in final table row -->

    <tr>  
      <td style="text-align: left">$ $</td>
      <td style="text-align: center">$ $</td>
    </tr>
</tbody>
</table>

*Table 1.  DCGAN/WGAN discriminator and generator architectures.  In the
discriminator, Norm = batch normalization for DGCAN and layer normalization
for WGAN.  Activation = Sigmoid for DCGAN and Identity for WGAN.*

The BEGAN discriminator qualitatively differs from its DCGAN and WGAN
counterparts inasmuch as it assumes an autoencoder form.  The autoencoder
is comprised of an encoder which maps an input image to an intermediate
bottleneck descriptor followed by a decoder that reconstructs the original
image.  The basic architectures for the encoder and decoder are outlined in
Table 2.  Following Berthelot et al [9], we incorporate skip connections in
the decoder network that concatenate the input descriptor with each
upsampling layer.  Moreover, we adopt the authors' suggestion to initialize
the network using vanishing residuals.

<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: left">BEGAN encoder super layer</th>
      <th style="text-align: center">Activation volume [width x height x
channels] </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Input image</td>
      <td style="text-align: center">64 x 64 x 3</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv3, Elu
      $(repeat \> super \> layer \> 3 \> times)$ </td>
      <td style="text-align: center">64 x 64 x 128</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv3, Elu, stride 2</td>
      <td style="text-align: center">32 x 32 x 128</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv3, Elu 
      $(repeat \> super \> layer \> 2 \> times)$ </td>
      <td style="text-align: center">32 x 32 x 256</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv3, Elu, stride 2</td>
      <td style="text-align: center">16 x 16 x 256</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv3, Elu 
      $(repeat \> super \> layer \> 2 \> times)$ </td>
      <td style="text-align: center">16 x 16 x 384</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv3, Elu, stride 2</td>
      <td style="text-align: center">8 x 8 x 384</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv3, Elu 
      $(repeat \> super \> layer \> 2 \> times)$ </td>
      <td style="text-align: center">8 x 8 x 512</td>
    </tr>
    <tr>
      <td style="text-align: left">Fully connected</td>
      <td style="text-align: center">32768 x $Z_{dim}$</td>
    </tr>
    <tr>
      <td style="text-align: left">Output descriptor</td>
      <td style="text-align: center">$Z_{dim}$</td>
    </tr>

    <!---  Add empty line in final table row -->

    <tr>
      <td style="text-align: left">$ $</td>
      <td style="text-align: center">$ $</td>
    </tr>
</tbody>
</table>

<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: left">BEGAN decoder super layer</th>
      <th style="text-align: center">Activation volume [width x height x
channels] </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Input descriptor</td>
      <td style="text-align: center">$Z_{dim}$</td>
    </tr>
    <tr>
      <td style="text-align: left">Fully connected</td>
      <td style="text-align: center">$Z_{dim}$ x 8192</td>
    </tr>

    <tr>
      <td style="text-align: left">Conv3, Elu
      $(repeat \> super \> layer \> 3 \> times)$ </td>
      <td style="text-align: center">8 x 8 x 128</td>
    </tr>
    <tr>
      <td style="text-align: left">Nearest neighbor upscale</td>
      <td style="text-align: center">16 x 16 x 1</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv3, Elu
      $(repeat \> super \> layer \> 3 \> times)$ </td>
      <td style="text-align: center">16 x 16 x 128</td>
    </tr>
    <tr>
      <td style="text-align: left">Nearest neighbor upscale</td>
      <td style="text-align: center">32 x 32 x 1</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv3, Elu
      $(repeat \> super \> layer \> 3 \> times)$ </td>
      <td style="text-align: center">32 x 32 x 128</td>
    </tr>
    <tr>
      <td style="text-align: left">Nearest neighbor upscale</td>
      <td style="text-align: center">64 x 64 x 1</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv3, Elu
      $(repeat \> super \> layer \> 3 \> times)$ </td>
      <td style="text-align: center">64 x 64 x 128</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv3, Elu</td>
      <td style="text-align: center">64 x 64 x 3</td>
    </tr>

    <!---  Add empty line in final table row -->

    <tr>  
      <td style="text-align: left">$ $</td>
      <td style="text-align: center">$ $</td>
    </tr>
</tbody>
</table>

*Table 2.  BEGAN encoder and decoder network architectures.*

We take the architecture for the BEGAN generator to be the same as the
decoder's.  A BEGAN which synthesizes 64x64 images then contains
O$($17.2M$)$ trainable parameters.  The input descriptor to the generator
has the same form as the intermediate bottleneck within the BEGAN
discriminator's autoencoder.  As we process genuine images through the
autoencoder, we iteratively keep track of the mean and covariance matrix
for their intermediate bottleneck descriptors.  Using these evolving
statistics, we sample input descriptors for the BEGAN generator from a
multivariate gaussian distribution.  In contrast, the descriptor inputs for
DCGAN and WGAN generators are completely random noise vectors whose
components are sampled from uniform distributions.

The three types of GANs with which we work depend upon a sizable set of
hyperparameters.  Some hyperparameters are particular to only one of the
GANs.  For example, the diversity ratio $\gamma$ regulates the BEGAN
discriminator's balance between autoencoding real images and distinguishing
synthetic from genuine images.  But others like those listed in Table 3
influence all DCGAN, WGAN and BEGAN training experiments.  We settled upon
the tabulated hyperparameter values via trial and error.

<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: left">Hyperparameter description</th>
      <th style="text-align: center">Value    </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Number of training images</td>
      <td style="text-align: center"> 202599 </td>
    </tr>
    <tr>
      <td style="text-align: left">Minibatch size</td>
      <td style="text-align: center">128 $($DCGAN & WGAN$)$; 48 $($BEGAN$)$</td>
    </tr>
    <tr>
      <td style="text-align: left">Adam optimizer $\beta_1$</td>
      <td style="text-align: center">0.5 $($DCGAN & BEGAN$)$; 0.0 $($WGAN$)$ </td>
    </tr>
    <tr>
      <td style="text-align: left">Adam optimizer $\beta_2$</td>
      <td style="text-align: center">0.999 $($DCGAN & BEGAN$)$; 0.9 $($WGAN$)$ </td>
    </tr>
    <tr>
      <td style="text-align: left">Base learning rate</td>
      <td style="text-align: center">0.0002 $($DCGAN$)$; 0.0001 $($WGAN, BEGAN$)$ </td>
    </tr>
    <tr>
      <td style="text-align: left">Descriptor dimension $Z_{dim}$</td>
      <td style="text-align: center">128 $($DCGAN & WGAN$)$; 64 $($BEGAN$)$ </td>
    </tr>
    <tr>
      <td style="text-align: left">Standard deviation for weight-initializing gaussian</td>
      <td style="text-align: center">0.02 </td>
    <tr>
      <td style="text-align: left">Leaky ReLU slope coefficient</td>
      <td style="text-align: center">0.2</td>
    </tr>
  </tr>

</tbody>
</table>

*Table 3.  Hyperparameter values employed for DCGAN, WGAN and BEGAN face synthesis.*

Face and flower images generated by DCGAN, WGAN and BEGAN architectures are
displayed side-by-side in figures 3 and 4 for comparison.  These 64x64
examples were intentionally selected from particular points during training
experiments based upon their generally good qualitative appearances.
Looking at the results, we observe that the DCGAN and WGAN outputs contain
higher spatial frequency content than their BEGAN counterparts.  But
several of the DCGAN faces exhibit significant distortions.  Similarly,
DCGAN flowers look more like impressionistic paintings than actual
photographs.

![BEGAN]({{site.url}}/blog/images/gans_faces_flowers/dcgan_wgan_began_faces.png)
*Fig. 3.  Example 64x64 faces generated by DCGAN $($left$)$, WGAN
$($center$)$ and BEGAN $($right$)$ architectures.*

![BEGAN]({{site.url}}/blog/images/gans_faces_flowers/dcgan_wgan_began_flowers.png)
*Fig. 4.  Example 64x64 flowers generated by DCGAN $($left$)$, WGAN
$($center$)$ and BEGAN $($right$)$ architectures.*

WGAN results are qualitatively similar to those from DCGAN.  Though the
theoretical arguments for employing the Wasserstein loss function are
attractive [7], we empirically find no practical benefit relative to the
more conventional DCGAN loss which justifies much slower WGAN training
rates.  In contrast, BEGAN outputs are remarkably realistic-looking for our
faces and flowers training data.  We consequently focus primarily upon
BEGAN image generation in the remainder of this posting.

As generative adversarial networks train, we observe that generator outputs
frequently exhibit noticeable correlations which vary over time.  For
instance, skin tones in faces and dominant colorings of flowers randomly
change at varying model checkpoints.  Figure 5 illustrates this phenomenon
where BEGAN training outputs are superposed on a plot of the model's
approximate convergence measure as a function of training iteration.
Abundances of pink, white and purple flowers are produced at different
times during training.  For the particular case of BEGAN models, more
variety in generated output can be requested by increasing the diversity
ratio $\gamma$ at the cost of diminished image quality. $($We typically set
$\gamma = 0.5$.$)$ We observed the same phenomenon of evolving color
correlations in DCGAN and WGAN images as well.

![BEGAN]({{site.url}}/blog/images/gans_faces_flowers/BEGAN_flowers_learning_curve.jpg)
*Fig. 5.  The BEGAN convergence measure generally decreases as training
proceeds.  Abrupt jumps in the measure's value signal training
instabilities.  Insets illustrate 64x64 flower thumbnails generated at
various points during the training session.*

The convergence measure plot in figure 5 further illustrates a training
instability problem that commonly afflicts GANs.  During training,
discriminator and/or generator loss functions can undergo major spikes.  An
example of generated outputs going bad during such unstable training
intervals is apparent in the figure.  Our BEGAN experiments typically
suffered less from such training instabilities than their DCGAN and WGAN
counterparts.  But contrary to the claims of Berthelot et al [9], BEGAN
model training does occasionally become unstable.

Generated images with 64x64 pixel sizes are small.  So it is obviously
desirable to produce larger outputs.  We experimented with deeper networks
architectures containing more convolutional layers, different kernel sizes
and increased number of filter channels.  We again found unacceptable
distortions in 128x128 faces coming from DCGANs and WGANs.  On the other
hand, the BEGAN encoder and decoder superlayer architectures listed in
Table 2 yielded the 128x128 faces shown in figure 6.  The BEGAN faces do
not exhibit the same degree of variety as those in the training corpus
$($see figure 1$)$.  For instance, we never observed a generated BEGAN face
with glasses even though the CelebA training set contains many such
examples [9].  But the overall quality of the faces appearing in figure 6
is noteworthy.  Unfortunately, we could not find a BEGAN architecture which
produced similarly compelling results for 128x128 flowers.


![BEGAN]({{site.url}}/blog/images/gans_faces_flowers/began1_128x128_faces_expt_140_inference.png)
*Fig. 6.  Representative 128x128 faces generated by the BEGAN architecture.*

## Synthetic image manipulation

One of the most impressive results from the original DCGAN paper was the
demonstration that generator inputs can be regarded linear vector space
descriptors [4].  Interpolating between two different descriptors yields a
smooth morphing of the first generated image into the second.  Since DCGANs
take random noise vectors as generator inputs, the preferred interpolation
between starting and ending points in high-dimensional descriptor spaces is
performed along spherical arcs [17].  But we work instead within a BEGAN
descriptor space characterized by a mean vector $\vec{\mu}$ and covariance
matrix $\Sigma$.  Linearly interpolated points between any of its
descriptors lie inside a convex ellipsoidal space.  Representative examples
of interpolated BEGAN flowers are pictured in figure 7.

![BEGAN]({{site.url}}/blog/images/gans_faces_flowers/padded_began1_64x64_flowers_linear_interp.png)
*Fig. 7.  Linear interpolations between random points in BEGAN descriptor
space yield smooth transformations between generated flower images.*

It is interesting to explore the structure of BEGAN descriptor spaces which
our iteratively-derived gaussian statistics enable.  For example, the image
in figure 8 corresponding to the descriptor space mean $\vec{\mu}$ is of a
Caucasian woman's face.  Given that the celebA training corpus contains
O$($118K$)$ female and O$($84K$)$ male faces and that it under-represents
people of color, this average face is consistent with semantic biases in
the training data.

![BEGAN]({{site.url}}/blog/images/gans_faces_flowers/padded_eigencenter_face.png)
*Fig. 8.  Generated face corresponding to the center location in BEGAN
descriptor space.*

Starting from the center point $\vec{\mu}$, we can move along eigenvectors
extracted from covariance matrix $\Sigma$ for the BEGAN descriptor space.
As the generated image examples in figure 9 illustrate, such eigenspace
translations lead to descriptor regions whose corresponding faces exhibit
different genders, ethnicities and facial expressions.  We can thus
systematically map the semantic facial content which the trained BEGAN
model is capable of generating.


![BEGAN]({{site.url}}/blog/images/gans_faces_flowers/eigendirs_09_images_006.png)
*Fig. 9.  Movements along various eigendirections passing through the
BEGAN descriptor space's center yield smooth transitions between semantically
different faces.*

In ref. [4], Radford et al demonstrated amusing vector arithmetic
relationships within descriptor space such as "smiling woman - neutral
woman + neutral man = similing man".  Working with our BEGAN descriptor
space statistics, we can perform similar manipulations on entire eigenspace
subregions.  For instance, we first identify sets of eigendirections
corresponding to semantically distinct facial subcategories such as women
with red lips, men with mustaches and people with wide grins.  Then
starting from random descriptors, we remove their initial projections along
the semantically-meaningful eigendirections.  Next, we translate the
flattened descriptors along desired linear combinations of eigenvectors by
several multiples of their corresponding square root eigenvalues.  As
figure 10 demonstrates, the generated images corresponding to the
engineered descriptors exhibit feminine red lips, manly mustaches and
toothy grins.

![BEGAN]({{site.url}}/blog/images/gans_faces_flowers/redlips_grins_mustaches_montage.png)
*Fig. 10.  Generated faces corresponding to random descriptors offset along
particular eigendirections exhibit engineered traits.  For example,
eigendirection combinations yield faces with red lips $($left$)$, wide
grins $($center$)$ and mustaches $($right$)$.*

We can produce images with more specific characteristics via descriptor
space translations into more than one semantically meaningful
eigen-subspace.  For example, by summing the eigendirections for female red
lips with wide grins, we can induce female faces with red lips.  Similarly,
male faces with mustaches and grins are obtained by feeding descriptors
from a particular descriptor space subregion into the BEGAN generator.  See
figure 11.


![BEGAN]({{site.url}}/blog/images/gans_faces_flowers/grinredlips_grinmustaches_montage.png)
*Fig. 11.  Taking combinations of semantically-meaningful eigendirections
yields female faces with red lips and wide grins $($left$)$ as well as
grinning male faces with mustaches $($right$)$.*

As a final example of synthetic image manipulation, figure 12 demonstrates
controlled modulation of faces from neutral expressions into wide grins.
Starting from neutral expression descriptors, we simply move along a linear
combination of eigendirections which semantically corresponds to grinning.
The sequence of faces in the figure exhibit other relatively minor changes
besides grin development.  Perhaps it is possible to find a more precise
eigendirection in BEGAN descriptor space which only affects grin content in
generated images.  We leave such refined descriptor space investigation for
future work.

![BEGAN]({{site.url}}/blog/images/gans_faces_flowers/neutral_to_grin.png)
*Fig. 12.  Movement between descriptor space points along a particular
linear combination of eigendirections smoothly adds grins to initially
neutral faces.*


## References

1. [I. Goodfellow, J.P.-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
A. Courville and Y. Bengio, "Generative Adversarial Nets," Advances in
Neural Information Processing Systems
$\small{(2014)}$.](https://arxiv.org/pdf/1406.2661.pdf)

2. [I. Goodfellow, "Generative Adversarial Networks," NIPS Tutorial
$\small{(2016)}$.](https://arxiv.org/pdf/1701.00160.pdf)

3. [A. Hindupur, "The GAN Zoo," $\small{(2017)}$.](https://deephunt.in/the-gan-zoo-79597dc8c347)

4. [A. Radford, L. Metz and S. Chintala, "Unsupervised Representation
Learning with Deep Convolutional Generative Adversarial Networks," ICLR
$\small{(2016)}$.](https://arxiv.org/pdf/1511.06434.pdf)

5.  [S. Ioffe and C. Szegedy, "Batch Normalization: Accelerating Deep
Network Training by Reducing Internal Covariate Shift," 32nd ICML 
$\small{(2015)}$.](https://arxiv.org/abs/1502.03167)

6.  [A.L. Maas, A.Y. Hannun and A.Y. Ng, "Rectifier Nonlinearities Improve
Neural Network Acoustic Models", 50th ICML $\small{(2013)}$.](https://web.stanford.edu/~awni/papers/relu_hybrid_icml2013_final.pdf)

7. [M. Arjovsky, S. Chintala and L. Bottou, "Wasserstein GAN,"
arXiv:1701.07875 $\small{(2017})$.](https://arxiv.org/pdf/1701.07875.pdf)

8. [I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin and A.C Courville,
"Improved Training of Wasserstein GANs," NIPS $\small{(2017)}$.](https://arxiv.org/pdf/1704.00028.pdf)

9. [D. Berthelot, T. Schumm and L. Metz, "BEGAN: Boundary Equilibrium
Generative Adversarial Networks,", CoRR $\small{(2017)}$.](https://arxiv.org/pdf/1703.10717.pdf)

10. [Z. Liu, P. Luo, X. Wang and W. Tang, "Large-scale CelebFaces Attributes
$($CelebA$)$ Dataset,"
$\small{(2015)}$.](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)

11. [M.-E. Nilsback and A. Zisserman, "17 and 102 Category Flower
Datasets," $\small{(2008)}$.](http://www.robots.ox.ac.uk/~vgg/data/flowers/)

12.  [See TensorFlow: An open-source software library for Machine Intelligence.](https://www.tensorflow.org/)

13.  [V. Dumoulin and F. Visin, "A Guide to Convolution Arithmetic for Deep
Learning," arXiv:1603.07285 v1 $\small{(2016)}$.](https://ai2-s2-pdfs.s3.amazonaws.com/7918/2aab186f0b68a8432540d8695e1646338479.pdf)

14.  [F.-F. Li, J. Johnson and S. Yeung, "CS-231N course slides, lecture 11,
Detection and Segmentation," slides 28 - 43
$\small{(2017)}$.](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf)

15. [J. L. Ba, J. R. Kiros and G.E. Hinton, "Layer Normalization,"
arXiv:1607.06450 $\small{(2016)}$.](https://arxiv.org/abs/1607.06450.pdf)

16. [D.-A. Clevert, T. Unterthiner and S. Hochreiter, "Fast and Accurate
Deep Network Learning by Exponential Linear Units," arXiv:1511.07289 $\small{(2016)}$.](
https://arxiv.org/pdf/1511.07289.pdf)

17.  [T. White, "Sampling Generative Networks," NIPS $\small{(2016)}$.](https://arxiv.org/pdf/1609.04468.pdf)
