---
layout: post
title:  "Generating and Manipulating Face and Flower Images via DCGANs, WGANs and BEGANs"
date:   2018-02-19 12:00:00
categories: Deep Learning
use_math: true
---

## Three GAN zoo members

Generative Adversarial Networks $($GANS$)$ belong to a family of generative
models that learn to sample from specified data distributions.  GANs
involve a game between two competing players [1].  The first is called the
generator which creates samples mimicing those within the training data.
The second player called the discriminator assesses whether samples
genuinely originate from the training corpus.  The discriminator is trained
via conventional supervised learning to classify inputs as either real or
fake.  The generator is simultaneously trained to fool the discriminator.
Both players improve their capabilities when GAN training is successful.

Current applications of GANs often center upon image synthesis.  The
generator $G$ is a differentiable function typically represented by a deep
neural network with parameters $\theta_G$.  $G$ maps a latent variable $z$
drawn from some simple distribution to an output image $x$.  The
discriminator $D$ is another differentiable function represented by a
neural network with parameters $\theta_D$.  $D$ takes an input image $x$
and generically returns the probability of its belonging to the training
data set.

The discriminator and generator are trained by minimizing two loss
functions $L_D(\theta_D, \theta_G)$ and $L_G(\theta_D, \theta_G)$.  $D$
tries to minimize $L_D$ while controlling only $\theta_D$, while $G$
minimizes $L_G$ while controlling only $\theta_G$.  The solution to this
game is a Nash equilibrium which is a local minimum of $L_D$ with respect
to $\theta_D$ as well as a local minimum of $L_G$ with respect to
$\theta_G$ [2].

Over the past few years, an entire zoo of GANs has been proposed and
studied [3].  Models within this large collection differ depending upon
their deep network designs, loss functions and training procedures.  In
this posting, we study three particular GAN variants.


## Face and flower synthesis

![BEGAN]({{site.url}}/blog/images/gans_faces_flowers/training_faces_64x64.png)
*Fig. X.  Tensorboard display of representative face images drawn
from the CelebA training set.*

![BEGAN]({{site.url}}/blog/images/gans_faces_flowers/training_flowers_64x64.png)
*Fig. X.  Tensorboard display of dynamically augmented images drawn from
the combined 17-flower and 102-flower training sets.*

![BEGAN]({{site.url}}/blog/images/gans_faces_flowers/dcgan_wgan_began_faces.png)
*Fig. X.  Example 64x64 faces generated by the DCGAN $($left$)$, WGAN
$($center$)$ and BEGAN $($right$)$ architectures.*

![BEGAN]({{site.url}}/blog/images/gans_faces_flowers/dcgan_wgan_began_flowers.png)
*Fig. X.  Example 64x64 flowers generated by the DCGAN $($left$)$, WGAN
$($center$)$ and BEGAN $($right$)$ architectures.*

![BEGAN]({{site.url}}/blog/images/gans_faces_flowers/BEGAN_flowers_learning_curve.jpg)
*Fig. X.  BEGAN convergence measure generally decreases as training
proceeds.  But sudden increases in measure value indicates training
instabilities.  Insets illustrate 64x64 flower thumbnails generated at
various points during training.*

![BEGAN]({{site.url}}/blog/images/gans_faces_flowers/began1_128x128_faces_expt_140_inference.png)
*Fig. X.  Representative 128x128 faces generated by the BEGAN architecture.*


## Latent space image manipulation

![BEGAN]({{site.url}}/blog/images/gans_faces_flowers/padded_began1_64x64_flowers_linear_interp.png)
*Fig. X.  Linear interpolations between random points in descriptor space
yields smooth transformations between generated flower images.*


![BEGAN]({{site.url}}/blog/images/gans_faces_flowers/padded_eigencenter_face.png)
*Fig. X.  Generated face corresponding to mean code-vector in descriptor space.*

![BEGAN]({{site.url}}/blog/images/gans_faces_flowers/eigendirs_09_images_006.png)
*Fig. X.  Movements along various eigendirections passing through the
eigencenter face image.*

![BEGAN]({{site.url}}/blog/images/gans_faces_flowers/redlips_grins_mustaches_montage.png)
*Fig. X.  Generated faces corresponding to random descriptors offset along
particular eigendirections exhibit semantically similar traits.  For example,
eigendirection combinations yield faces with red lips and rosy cheeks
$($left$)$, wide grins $($center$)$ and mustaches $($right$)$.*

![BEGAN]({{site.url}}/blog/images/gans_faces_flowers/grinredlips_grinmustaches_montage.png)
*Fig. X.  Taking combinations of semantically-meaningful eigendirections
yields faces with red lips and wide grins $($left$)$ as well as mustaches
and wide grins $($right$)$*

![BEGAN]({{site.url}}/blog/images/gans_faces_flowers/neutral_to_grin.png)
*Fig. X.  Movement between points in descriptor space along a particular
combination of eigendirections smoothly adds grins to initially neutral
faces.*


## References

1. [I. Goodfellow, J.P.-Abadie, M. Mirza, B> Xu, D. Warde-Farley, S. Ozair,
A. Courville and Y. Bengio, "Generative Adversarial Nets," Advances in
Neural Information Processing Systems
$\small{(2014)}$.](https://arxiv.org/pdf/1406.2661.pdf)

2. [I. Goodfellow, "Generative Adversarial Networks," NIPS Tutorial
$\small{(2016)}$.](https://arxiv.org/pdf/1701.00160.pdf)

3. [A. Hindupur, "The GAN Zoo," $\small{(2017)}$.](https://deephunt.in/the-gan-zoo-79597dc8c347)

4. [A. Radford, L. Metz and S. Chintala, "Unsupervised Representation
Learning with Deep Convolutional Generative Adversarial Networks," ICLR
$\small{(2016)}$.](https://arxiv.org/pdf/1511.06434.pdf)

5. [D. Berthelot, T. Schumm and L. Metz, "BEGAN: Boundary Equilibrium
Generative Adversarial Networks,", CoRR $\small{(2017)}$.](https://arxiv.org/pdf/1703.10717.pdf)

6. [Z. Liu, P. Luo, X. Wang and W. Tang, "Large-scale CelebFaces Attributes
$($CelebA$)$ Dataset,"
$\small{(2015)}$.](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)

7. [M.-E. Nilsback and A. Zisserman, "Delving Deeper into the Whorl of
Flower Segmentation," Image and Vision Computing
$\small{(2009)}$.](http://www.robots.ox.ac.uk/~vgg/data/flowers/)
