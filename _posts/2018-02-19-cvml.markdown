---
layout: post
title:  "Synthesizing Face and Flower Images via DCGANs, WGANs and BEGANs"
date:   2018-02-19 12:00:00
categories: Deep Learning
use_math: true
---

## Three GAN zoo members

Generative Adversarial Networks $($GANS$)$ belong to a family of generative
models that learn to sample from specified data distributions.  GANs
involve a game between two competing players [1].  The first is called the
generator which creates samples mimicing those within the training data.
The second player called the discriminator assesses whether samples
genuinely originate from the training corpus.  The discriminator is trained
via conventional supervised learning to classify inputs as either real or
fake.  The generator is simultaneously trained to fool the discriminator.
Both players improve their capabilities when GAN training is successful.

Current applications of GANs often center upon image synthesis.  The
generator $G$ is a differentiable function typically represented by a deep
neural network with parameters $\theta_G$.  $G$ maps a latent variable $z$
drawn from some simple distribution to an output image $x$.  The
discriminator $D$ is another differentiable function represented by a
neural network with parameters $\theta_D$.  $D$ takes an input image $x$
and generically returns the probability of its belonging to the training
dataset.

The discriminator and generator are trained by minimizing two loss
functions $L_D(\theta_D, \theta_G)$ and $L_G(\theta_D, \theta_G)$.  $D$
tries to minimize $L_D$ while controlling only $\theta_D$, while $G$
minimizes $L_G$ while controlling only $\theta_G$.  The solution to this
game is a Nash equilibrium which is a local minimum of $L_D$ with respect
to $\theta_D$ as well as a local minimum of $L_G$ with respect to
$\theta_G$ [2].

Over the past few years, an entire zoo of GANs has been proposed and
studied [3].  Models within this large collection differ depending upon
their deep network designs, loss functions and training procedures.  In
this posting, we study three particular GAN variants of increasing
complexity:

I.  Deep Convolutional GANs $($DCGANs$)$

In 2016, Radford, Metz and Chintala performed an extensive search for CNN
architectures that enable stable GAN training [4].  These investigators
chose to work with all-convolutional networks from which pooling and fully
connected layers are banished.  The authors also incorporated batch
normalization into the generator and discriminator [5], and they worked
with ReLU and leaky ReLU [6] activation functions.  Most interestingly,
Radford et al demonstrated nontrivial manipulation of generated image
content via simple vector arithmetic on latent space descriptors.

II.  Wasserstein GANs $($WGANs$)$

In 2017, Arjovsky, Chintala and Bottou demonstrated that the standard GAN
loss function introduced in [1] is based upon minimizing the
Kullback-Leibler (KL) divergence between an underlying data distribution
and a model's approximation to that distribution [7].  These authors
described problems arising from the KL divergence and proposed replacing it
with an Earth Mover distance function.  They also introduced a measure of
convergence that can be monitored during training and empirically
demonstrated it correlates well with generated sample quality.  Refinements
to the original WGAN model's training procedure were reported in [8].

III.  Boundary Equilibrium GANs $($BEGANs$)$

In 2017, Berthelot, Schumm and Metz proposed using proportional control
theory to maintain an equilibrium during training between the discriminator
and generator [9].  These authors also employed an autoencoder for the GAN
discriminator with a per-pixel loss.  They further introduced an adjustable
parameter $\gamma$ which trades off between generated image diversity and
discriminator reconstruction quality.  Like WGANs, BEGANs provide an
approximate convergence measure which helps monitoring of training
progress.

We implement these three GAN types and assess their performance in the
following section.  

## Face and flower image generation

We work with two sets of curated imagery in order to train GANs.  The first
is the "CelebA" dataset containing O$($220K$)$ celebrity faces [10].  The
second is a combination of 17-category and 102-category "Oxford flower"
datasets with O$($9.5K$)$ pictures [11].  We increase the number and
variation of flower images by dynamically augmenting the originals via
random horizontal flips, 90 degree rotation angles and HSV color
modulations.  Representative examples of face and flower training images
are presented in figures 1 and 2.

![BEGAN]({{site.url}}/blog/images/gans_faces_flowers/training_faces_64x64.png)
*Fig. 1.  Tensorboard display of representative face images from the CelebA
training set.*

![BEGAN]({{site.url}}/blog/images/gans_faces_flowers/training_flowers_64x64.png)
*Fig. 2.  Tensorboard display of dynamically augmented images from two
Oxford flower datasets.*



<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: left">DCGAN/WGAN discriminator super layer</th>
      <th style="text-align: center">Activation volume [width x height x
channels] </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Input image</td>
      <td style="text-align: center">64 x 64 x 3</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv3, Norm, Leaky Relu, stride 2</td>
      <td style="text-align: center">32 x 32 x 128</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv3, Norm, Leaky Relu, stride 2</td>
      <td style="text-align: center">16 x 16 x 256</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv3, Norm, Leaky Relu, stride 2</td>
      <td style="text-align: center">8 x 8 x 512</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv3, Norm, Leaky Relu, stride 2</td>
      <td style="text-align: center">4 x 4 x 1024</td>
    </tr>
    <tr>
      <td style="text-align: left">Linear proj, Activation</td>
      <td style="text-align: center">1</td>
    </tr>

    <!---  Add empty line in final table row -->

    <tr>
      <td style="text-align: left">$ $</td>
      <td style="text-align: center">$ $</td>
    </tr>
</tbody>
</table>

<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: left">DCGAN/WGAN generator super layer</th>
      <th style="text-align: center">Activation volume [width x height x
channels] </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Input descriptor</td>
      <td style="text-align: center">$Z_{dim}$</td>
    </tr>
    <tr>
      <td style="text-align: left">Linear proj, Batch norm, Relu</td>
      <td style="text-align: center">4 x 4 x 1024</td>
    </tr>
    <tr>
      <td style="text-align: left">Transposed conv3, Batch norm, ReLU</td>
      <td style="text-align: center">8 x 8 x 512</td>
    </tr>
    <tr>
      <td style="text-align: left">Transposed conv3, Batch norm, ReLU</td>
      <td style="text-align: center">16 x 16 x 256</td>
    </tr>
    <tr>
      <td style="text-align: left">Transposed conv3, Batch norm, ReLU</td>
      <td style="text-align: center">32 x 32 x 128</td>
    </tr>
    <tr>
      <td style="text-align: left">Transposed conv3, Tanh</td>
      <td style="text-align: center">64 x 64 x 3</td>
    </tr>

    <!---  Add empty line in final table row -->

    <tr>  
      <td style="text-align: left">$ $</td>
      <td style="text-align: center">$ $</td>
    </tr>
</tbody>
</table>

*Table 1.  DCGAN/WGAN discriminator and generator architectures.  In the
discriminator, Norm = batch normalization for DGCAN and layer normalization
for WGAN.  Activation = Sigmoid for DCGAN and Identity for WGAN.*

<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: left">BEGAN encoder super layer</th>
      <th style="text-align: center">Activation volume [width x height x
channels] </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Input image</td>
      <td style="text-align: center">64 x 64 x 3</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv3, Elu
      $(repeat \> super \> layer \> 3 \> times)$ </td>
      <td style="text-align: center">64 x 64 x 128</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv3, Elu, stride 2</td>
      <td style="text-align: center">32 x 32 x 128</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv3, Elu 
      $(repeat \> super \> layer \> 2 \> times)$ </td>
      <td style="text-align: center">32 x 32 x 256</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv3, Elu, stride 2</td>
      <td style="text-align: center">16 x 16 x 256</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv3, Elu 
      $(repeat \> super \> layer \> 2 \> times)$ </td>
      <td style="text-align: center">16 x 16 x 384</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv3, Elu, stride 2</td>
      <td style="text-align: center">8 x 8 x 384</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv3, Elu 
      $(repeat \> super \> layer \> 2 \> times)$ </td>
      <td style="text-align: center">8 x 8 x 512</td>
    </tr>
    <tr>
      <td style="text-align: left">Fully connected</td>
      <td style="text-align: center">32768 x $Z_{dim}$</td>
    </tr>
    <tr>
      <td style="text-align: left">Output descriptor</td>
      <td style="text-align: center">$Z_{dim}$</td>
    </tr>

    <!---  Add empty line in final table row -->

    <tr>
      <td style="text-align: left">$ $</td>
      <td style="text-align: center">$ $</td>
    </tr>
</tbody>
</table>




<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: left">BEGAN decoder super layer</th>
      <th style="text-align: center">Activation volume [width x height x
channels] </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Input descriptor</td>
      <td style="text-align: center">$Z_{dim}$</td>
    </tr>
    <tr>
      <td style="text-align: left">Fully connected</td>
      <td style="text-align: center">$Z_{dim}$ x 8192</td>
    </tr>

    <tr>
      <td style="text-align: left">Conv3, Elu
      $(repeat \> super \> layer \> 3 \> times)$ </td>
      <td style="text-align: center">8 x 8 x 128</td>
    </tr>
    <tr>
      <td style="text-align: left">Nearest neighbor upscale</td>
      <td style="text-align: center">16 x 16 x 1</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv3, Elu
      $(repeat \> super \> layer \> 3 \> times)$ </td>
      <td style="text-align: center">16 x 16 x 128</td>
    </tr>
    <tr>
      <td style="text-align: left">Nearest neighbor upscale</td>
      <td style="text-align: center">32 x 32 x 1</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv3, Elu
      $(repeat \> super \> layer \> 3 \> times)$ </td>
      <td style="text-align: center">32 x 32 x 128</td>
    </tr>
    <tr>
      <td style="text-align: left">Nearest neighbor upscale</td>
      <td style="text-align: center">64 x 64 x 1</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv3, Elu
      $(repeat \> super \> layer \> 3 \> times)$ </td>
      <td style="text-align: center">64 x 64 x 128</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv3, Elu</td>
      <td style="text-align: center">64 x 64 x 3</td>
    </tr>

    <!---  Add empty line in final table row -->

    <tr>  
      <td style="text-align: left">$ $</td>
      <td style="text-align: center">$ $</td>
    </tr>
</tbody>
</table>

*Table 2.  BEGAN encoder and decoder network architectures.*







<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: left">Hyperparameter description</th>
      <th style="text-align: center">Value    </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Number of training images</td>
      <td style="text-align: center"> 202599 </td>
    </tr>
    <tr>
      <td style="text-align: left">Minibatch size</td>
      <td style="text-align: center">128 $($DCGAN & WGAN$)$; 48 $($BEGAN$)$</td>
    </tr>
    <tr>
      <td style="text-align: left">Adam optimizer $\beta_1$</td>
      <td style="text-align: center">0.5 $($DCGAN & BEGAN$)$; 0.0 $($WGAN$)$ </td>
    </tr>
    <tr>
      <td style="text-align: left">Adam optimizer $\beta_2$</td>
      <td style="text-align: center">0.999 $($DCGAN & BEGAN$)$; 0.9 $($WGAN$)$ </td>
    </tr>
    <tr>
      <td style="text-align: left">Base learning rate</td>
      <td style="text-align: center">0.0002 $($DCGAN$)$; 0.0001 $($WGAN, BEGAN$)$ </td>
    </tr>
    <tr>
      <td style="text-align: left">Descriptor dimension $Z_{dim}$</td>
      <td style="text-align: center">100 $($DCGAN$)$; 128 $($WGAN$)$; 64 $($BEGAN$)$ </td>
    </tr>
    <tr>
      <td style="text-align: left">Standard deviation for weight-initializing gaussian</td>
      <td style="text-align: center">0.02 </td>
    <tr>
      <td style="text-align: left">Leaky ReLU slope coefficient</td>
      <td style="text-align: center">0.2</td>
    </tr>
  </tr>

</tbody>
</table>

*Table 3.  Hyperparameter values employed for DCGAN, WGAN and BEGAN face synthesis.*



![BEGAN]({{site.url}}/blog/images/gans_faces_flowers/dcgan_wgan_began_faces.png)
*Fig. 3.  Example 64x64 faces generated by DCGAN $($left$)$, WGAN
$($center$)$ and BEGAN $($right$)$ architectures.*

![BEGAN]({{site.url}}/blog/images/gans_faces_flowers/dcgan_wgan_began_flowers.png)
*Fig. 4.  Example 64x64 flowers generated by DCGAN $($left$)$, WGAN
$($center$)$ and BEGAN $($right$)$ architectures.*

![BEGAN]({{site.url}}/blog/images/gans_faces_flowers/BEGAN_flowers_learning_curve.jpg)
*Fig. X.  BEGAN convergence measure generally decreases as training
proceeds.  But sudden increases in measure value indicates training
instabilities.  Insets illustrate 64x64 flower thumbnails generated at
various stages of training.*

![BEGAN]({{site.url}}/blog/images/gans_faces_flowers/began1_128x128_faces_expt_140_inference.png)
*Fig. X.  Representative 128x128 faces generated by the BEGAN architecture.*


## Synthetic image manipulation

![BEGAN]({{site.url}}/blog/images/gans_faces_flowers/padded_began1_64x64_flowers_linear_interp.png)
*Fig. X.  Linear interpolations between random points in descriptor space
yields smooth transformations between generated flower images.*


![BEGAN]({{site.url}}/blog/images/gans_faces_flowers/padded_eigencenter_face.png)
*Fig. X.  Generated face corresponding to mean code-vector in descriptor space.*

![BEGAN]({{site.url}}/blog/images/gans_faces_flowers/eigendirs_09_images_006.png)
*Fig. X.  Movements along various eigendirections passing through the
eigencenter face image.*

![BEGAN]({{site.url}}/blog/images/gans_faces_flowers/redlips_grins_mustaches_montage.png)
*Fig. X.  Generated faces corresponding to random descriptors offset along
particular eigendirections exhibit semantically similar traits.  For example,
eigendirection combinations yield faces with red lips and rosy cheeks
$($left$)$, wide grins $($center$)$ and mustaches $($right$)$.*

![BEGAN]({{site.url}}/blog/images/gans_faces_flowers/grinredlips_grinmustaches_montage.png)
*Fig. X.  Taking combinations of semantically-meaningful eigendirections
yields faces with red lips and wide grins $($left$)$ as well as mustaches
and wide grins $($right$)$*

![BEGAN]({{site.url}}/blog/images/gans_faces_flowers/neutral_to_grin.png)
*Fig. X.  Movement between points in descriptor space along a particular
combination of eigendirections smoothly adds grins to initially neutral
faces.*


## References

1. [I. Goodfellow, J.P.-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
A. Courville and Y. Bengio, "Generative Adversarial Nets," Advances in
Neural Information Processing Systems
$\small{(2014)}$.](https://arxiv.org/pdf/1406.2661.pdf)

2. [I. Goodfellow, "Generative Adversarial Networks," NIPS Tutorial
$\small{(2016)}$.](https://arxiv.org/pdf/1701.00160.pdf)

3. [A. Hindupur, "The GAN Zoo," $\small{(2017)}$.](https://deephunt.in/the-gan-zoo-79597dc8c347)

4. [A. Radford, L. Metz and S. Chintala, "Unsupervised Representation
Learning with Deep Convolutional Generative Adversarial Networks," ICLR
$\small{(2016)}$.](https://arxiv.org/pdf/1511.06434.pdf)

5.  [S. Ioffe and C. Szegedy, "Batch Normalization: Accelerating Deep
Network Training by Reducing Internal Covariate Shift," 32nd ICML 
$\small{(2015)}$.](https://arxiv.org/abs/1502.03167)

6.  [A.L. Maas, A.Y. Hannun and A.Y. Ng, "Rectifier Nonlinearities Improve
Neural Network Acoustic Models", 50th ICML $\small{(2013)}$.](https://web.stanford.edu/~awni/papers/relu_hybrid_icml2013_final.pdf)

7. [M. Arjovsky, S. Chintala and L. Bottou, "Wasserstein GAN,"
arXiv:1701.07875 $\small{(2017})$.](https://arxiv.org/pdf/1701.07875.pdf)

8. [I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin and A.C Courville,
"Improved Training of Wasserstein GANs," NIPS $\small{(2017)}$.](https://arxiv.org/pdf/1704.00028.pdf)

9. [D. Berthelot, T. Schumm and L. Metz, "BEGAN: Boundary Equilibrium
Generative Adversarial Networks,", CoRR $\small{(2017)}$.](https://arxiv.org/pdf/1703.10717.pdf)

10. [Z. Liu, P. Luo, X. Wang and W. Tang, "Large-scale CelebFaces Attributes
$($CelebA$)$ Dataset,"
$\small{(2015)}$.](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)

11. [M.-E. Nilsback and A. Zisserman, "17 and 102 Category Flower
Datasets," $\small{(2008)}$.](http://www.robots.ox.ac.uk/~vgg/data/flowers/)

12. [J. L. Ba, J. R. Kiros and G.E. Hinton, "Layer Normalization,"
arXiv:1607.06450 $\small{(2016)}$.](https://arxiv.org/abs/1607.06450.pdf)

13. [D.-A. Clevert, T. Unterthiner and S. Hochreiter, "Fast and Accurate
Deep Network Learning by Exponential Linear Units," arXiv:1511.07289 $\small{(2016)}$.](
https://arxiv.org/pdf/1511.07289.pdf)
