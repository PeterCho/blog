---
layout: post
title:  "Localizing Text-in-the-Wild via Synthetic Phrase Generation and
Semantic Segmentation"
date:   2016-04-25 12:00:0
categories: Deep Learning
---

## Synthetic text imagery generation

Optical Character Recognition (OCR) in cooperative settings such as
book-scanning machines is generally regarded as a solved problem within the
computer vision community.  In contrast, teaching machines to read
"text-in-the-wild" remains an outstanding challenge.  Unlike benign OCR
setups, text-in-the-wild presents many *a priori* unknown variables which
an AI system must handle.  These include wide ranges of text fonts, colors,
illumination conditions, 3D orientations, intrinsic character sizes,
distance ranges, foreground occlusions and background surface materials.
Moreover, gross spatial layouts for text appearing in the physical world
are highly variable.  Single characters, multiple abutted business names
and parking sign paragraphs can all be seen within complex urban
environments.  So the text-in-the-wild problem has drawn significant
interest among computer vision and machine learning researchers for many
years [refs].

In this blog entry, we focus upon just one aspect of reading
text-in-the-wild.  In particular, we are interested in localizing text in
arbitrary input images which represents a pre-requisite to text
recognition.  Given recent successes in supervised image classification,
one obvious attempt to solve this first stage of text-in-the-wild
processing would be to amass a training collection containing thousands of
manually labeled examples.  This brute force approach is often adopted by
big companies with huge budgets and sizable human resources.  But we are
more interested in exploring alternative sources of training data which do
not require massive human effort.

Text falls within a special category of objects seen in the physical world
inasmuch as it is often computer-generated.  Nearly all text appearing in
business advertisements, roadway street signs, and TV commercials
originated from some computer system.  And given the wide range of freely
available computer fonts, machines can now do a remarkably good job at
emulating human handwriting and painted words.  So we are interested in
synthesizing text imagery and seeing how well classifiers trained on it can
localize text-in-the-wild [Jaderberg refs].

We start by developing a corpus of English phrases which become inputs to
our synthetic image engine.  Since names frequently appear in the physical
world, we initially downloaded lists of male and female first names along
with common surnames.  We then randomly combined these parts to form
reasonable-sounding full names.  Some of the output names were exported in
capital letters.  Others had their first and/or middle names replaced by
initials.  In addition to synthesized people names, we also included
company names found via internet searches to our text source corpus.

Numerals also frequently appear in the physical world.  So we randomly
generated integers containing 1-6 digits as well as decimal numerals
containing 1 or 2 digits after a decimal point.  For 20% of the decimal
numbers, we prepended a "$" in order to simulate price tags often posted by
stores and businesses.  Telephone numbers containing 10 or 7 digits were
also randomly generated.  Some of the 3-digit area codes were separated by
parentheses, and some of the 7-digit telephone numbers were separated by
hyphens.

Text-in-the-wild often contains many words spanning multiple lines.  So we
harvested relatively long phrases from random news reports, computer blogs,
science articles and creative writing stories posted on the internet over
the past two decades.  White space was removed from these input sources,
but punctuation marks were intentionally retained.  Phrases containing 1 to
15 words were extracted from these text files.

After combining together and shuffling these different sources, our text
corpus contained O(215K) phrases.  We scanned through every corpus
character and rejected any whose ascii value did not lie within the
printable interval [32, 126].  No attempt was made to harvest words in
languages other than English.  So we believe the overwhelming majority of
our input phrases are in English.

We next downloaded and inspected hundreds of different truetype and
opentype fonts freely available on the internet.  We used our judgement to
retain "reasonable" looking fonts which mimics those that plausibly could
be seen in the real world.  Any font which could not render at least 10
digits and 26 uppercase Latin letters over character widths ranging from 5
- 10 pixels was rejected.  Fonts corresponding to cursive handwriting were
also intentionally included.  We ended up with 700+ fonts to use for
synthesizing text.  

Our synthetic phrase generator is based upon ImageMagick's text to image
capabilities [ref].  The generator first randomly selects some particular
text label from the large set of phrases along with a font.  A small subset
of the text labels have extra spaces inserted between each non-white
character.  Others have carriage returns inserted after each letter in
order to generate vertically aligned text.

RGB colors for the text foreground and possible stroke are next randomly
chosen.  If an underbox for the text is randomly selected, its background
color is required to be reasonably different from the text foreground's
color.  Otherwise, the foreground text is rendered against a transparent
background.

The overall width and height for the entire text phrase is quasi-randomly
selected so that a significant percentage of strings are rendered onto two
or more text lines.  For example, figure 1 illustrates a single phrase
which wraps onto 5 text lines:

![TextLocalization]({{site.url}}/blog/images/text_segmentation/syntext/padded_syn_phrase.png)
*Fig. 1. A synthetically generated image containing 15 words and spanning 5
text lines.  The foreground is rendered against a transparent background.*

After a text phrase is rendered, we compute reasonable estimates for all
the rendered characters' widths.  A very wide, temporary image is virtually
created to hold the entire input string within a single textline.
Individual character widths are found by subtracting lengths of virtually
rendered substrings containing c+1 and c characters within the wide,
temporary image.  (Index c ranges from 0 to one less than the number of
characters in the input text phrase.)  Once rendered character widths are
known and the beginnings of each text line within the actual synthetic
image are determined via horizontal and vertical profiling, pixel bounding
boxes around each character are readily derived.

Once character bounding boxes are known, masks indicating various text
properties may be generated.  For instance, the pixel bounding boxes
corresponding to letters starting individual words in a phrase may be
filled with one mask value.  Bounding boxes for characters located in the
middle or at the ends of words may similarly be filled with different mask
values.  Such character-within-word relationships for the text phrase in
figure 1 are illustrated in figure 2a.  White spaces as well as
single-character words are also assigned their own mask values.  The
resulting mask in figure 2a horizontally parses the multi-line, multi-word
text phrase rendering.  In contrast, a vertical parsing of the multi-line
text phrase is depicted in figure 2b.  In this second mask example, the top
and bottom halves of each character bounding box are assigned distinct
values.  The resulting mask pattern is useful for separating different
text lines within the rendered phrase.

![TextLocalization]({{site.url}}/blog/images/text_segmentation/syntext/padded_char_word_masks_b.png)
*Fig. 2a. Horizontal character masks are color-coded according to letter
spatial relationships.  Starting [stopping] characters in words are colored
blue [green].  Middle characters inside words are colored red.  Single
character words are colored orange.  Spaces between words are colored pink.
White character outlines in this figure are for visualization only.*

![TextLocalization]({{site.url}}/blog/images/text_segmentation/syntext/padded_char_word_masks_c.png)
*Fig. 2b. Text line masks are color-coded according to vertical
spatial relationships.  Upper [lower] half of text characters are colored
blue [red].*

A random fraction of the synthetic text images have drop shadow and
character shine effects added to make them look somewhat three-dimensional.
Another random fraction are intentionally corrupted via the addition of
simulated solar shadows and foreground occlusions.  The text image chips
along with their masks are then rotated by random azimuth, elevation and
roll angles.  After perspectively projecting the string image and masks
into the virtual camera, we use ImageMagick to generate and export their 2D
imageplane renderings.  

Following this synthesis procedure, we generated 131K+ foreground text
phrases plus accompanying image masks.  Some examples of these synthesized
text images are displayed in figure 3.

![TextLocalization]({{ site.url }}/blog/images/text_segmentation/syntext/syn_phrases.png)
*Fig. 3.  Representative examples of synthetically generated images
exhibiting a variety of text contents, fonts, colors, rotations, shadings,
shadowings and occlusions.*

Text in the physical world appears against a wide variety of backgrounds.
In some cases such as TV broadcasts and many roadsigns, text foreground is
sharply distinct from the solid, plain backgrounds on which it resides.  In
other instances such as store names on awnings or advertisements painted
onto vehicle sides, text foreground may be strongy shadowed or irregularly
illuminated.  Text-in-the-wild also frequently appears on complex
background surfaces like glass windows or non-rigid cloth flags.  Ideally,
a machine should learn to recognize text phrases independent of their
surrounding backgrounds.

We consequently quasi-randomly downloaded over 1000 pictures from the
internet to serve as backgrounds for our synthesized text image
foregrounds.  The pixel size for each background photo was required to be
larger than 1Kx1K.  Many of the backgrounds views are of real-world outdoor
or indoor scenes.  Others are more abstract computer graphics,
line-drawing or oil painting art.  In all cases, we inspected each
background picture and manually blackened out any text content it
originally contained.  

Given a text-free internet image as a backdrop, foreground text chips are
quasi-randomly superposed onto the background texture.  At each candidate
foreground text location, entropy density is calculated within the
background image.  The candidate text location is rejected if the
background entropy density is too high.  Moreover, average foreground and
bacground RGB values are calculated at nominated text sites.  If their
color contents do not sufficiently different, the candidate text location
is rejected.  The bounding box for any foreground chip is also not allowed
to overlap any existing text bounding boxes.  The number of foreground
overlays within each composite varies randomly over the interval [9,27].

Once a composite image is formed, each of its pixel's RGB values is
fluctuated by a nonzero amount of random gaussian noise.  A sizable
fraction of the composites are also randomly blurred.  To our surprise, we
observed that such composite degradation significantly improves subsequent
semantic segmentation classifier performance on test images containing
genuine text content.

For every composite image, we generate two 8-bit greyscale masks which
encode the locations of the synthetic text foregrounds.  One of the masks
encodes horizontal word-character spatial relationships while the other
contains vertical text-line spacing information.  Examples of these two
types of masks are displayed in figures 4 and 5 versus their corresponding
composite images.

![TextLocalization]({{site.url}}/blog/images/text_segmentation/syntext/montage_composite_image_00028___char_bytearray_00028.jpg)
*Fig. 4.  (a) Synthetically generated text phrases superposed on a
background image of an outdoor scene.  Note that text inside the roadsign
located near the lower right corner has been manually blackened. (b)
Corresponding 8-bit mask indicates character locations within words via
greyscale colorings.*

![TextLocalization]({{site.url}}/blog/images/text_segmentation/syntext/montage_composite_image_00187___word_bytearray_00187.jpg)
*Fig. 5.  (a) Synthetically generated text phrases superposed on a
background, text-free image of an indoor scene. (b) Corresponding 8-bit
mask indicates vertical text-line locations via greyscale colorings.*

Following this compositing procedure, we formed O(28K) composites along
with their two accompanying masks.  We subsequently decomposed each
composite into 3x3 tiles of pixel size 321x321.  So we produced over a
quarter-million synthetic image tiles.  As we shall see in the next
section, these tiles plus masks can be used to train deep semantic
segmentation networks to localize text-in-the-wild in arbitrary input
images.

## Semantic segmentation of text-in-the-wild

To perform pixel-level image segmentation, we utilize the DeepLab framework
which is built on top of Caffe [refs].  Deeplab combines deep neural
network (DNN) responses with densely connected conditional random fields
(CRF).  But we choose to not employ DeeplLab's CRF functionality as it is
computationally expensive.

Our synthetically generated 321x321 pixel image tiles become inputs to
Deeplab's network, while their corresponding masks become inputs to
Deeplab's loss function.  We also supply DeepLab with a set of
randomly-shuffled associations betwee image tiles and masks.  8K of the
250K+ tile and masks pairs are reserved for network validation, while the
rest are used for network training.

Using Stochastic Gradient Descent, DeepLab fine-tunes the weight and bias
parameters of a pre-trained DNN which is a variant of the VGG-16 model
[ref].  We adopt a batch size of 30, retain DeepLab's default 0.9 momentum
and 0.0005 weight decay parameter values, and utilize a step learning rate
policy.  We found that training and validation results were relatively
insensitive to modifications of the learning rate parameter.  So we adopted
a learning rate of 0.0045 [0.005] when fine-tuning the DNN for horizontal
word-character [vertical text-line] segmentation.  The loss functions for
these two DNN models are displayed in figure 6 over 3 training epochs.

![TextLocalization]({{ site.url}}/blog/images/text_segmentation/training/montage_horiz_lossfunc___vert_lossfunc.jpg)
*Fig. 6.  Horizontal word-character and vertical text-line segmentation
losses plotted as functions of DNN model training.*

After network fine-tuning has asymptotically finished, we can apply the
trained DNNs to localize genuine text in arbitrary input images.  In order
to quantitatively assess the performance of our classifiers on genuine
text-in-the-wild, we work with a challenging set of ground-truthed images
[ref].  It consists of 307 color images of outdoor urban scenes with
significant text content appearing on store fronts, billboard
advertisements, street signs, trucks and vans, etc.  Text observed in these
pictures occurs exhibit a wide variety of fonts, colors, sizes, distance
ranges, angular orientations, physical locations and illumination
conditions.  Horizontal text segmentation results for 5
representative images from this data set are presented in figure 7.
Individual words are identified by their starting and stopping letters as
well as spaces between words.  

![TextLocalization]({{ site.url }}/blog/images/text_segmentation/fullsized_segs/montage_text_img0055___segmented_image_00055.jpg)

![TextLocalization]({{ site.url }}/blog/images/text_segmentation/fullsized_segs/montage_text_img0062___segmented_image_00062.jpg)

![TextLocalization]({{ site.url }}/blog/images/text_segmentation/fullsized_segs/montage_text_img0101___segmented_image_00101.jpg)

![TextLocalization]({{ site.url }}/blog/images/text_segmentation/fullsized_segs/montage_text_img0116___segmented_image_00116.jpg)

![TextLocalization]({{ site.url}}/blog/images/text_segmentation/fullsized_segs/montage_text_img0149___segmented_image_00149.jpg)
*Fig. 7.  Horizontal text segmentation results for 5 different test images.
Starting [stopping] letters within words are tinted yellow [blue].  Middle
letters inside words are tinted green.  Spaces between words are colored orange.*


![TextLocalization]({{ site.url}}/blog/images/text_segmentation/vert_full_segs/montage_horiz_segmented_image_00076___vert_segmented_image_00076.jpg)

![TextLocalization]({{ site.url}}/blog/images/text_segmentation/vert_full_segs/montage_horiz_segmented_image_00201___vert_segmented_image_00201.jpg)
*Fig. 8.  Individual word versus vertical text line segmentation results
for 2 different images.  Colored tinting for horizontal characters appearing on
the left hand-side of these image pairs follows the same pattern as in
figure 7.  The top [bottom] half of text lines are tinted blue [yellow]
within the right hand-side of the image pairs.*



![TextLocalization]({{ site.url }}/blog/images/text_segmentation/img_pyramid/cropped_montage_imgpyr_1.jpg)
*Fig. 9a.  Individual word segmentation results for one particular test
image at its original pixel resolution.*

![TextLocalization]({{ site.url }}/blog/images/text_segmentation/img_pyramid/cropped_montage_imgpyr_2.jpg)
*Fig. 9b.  Word segmentation results for the test image at half-sized and double-sized pixel resolutions.*

## Text segmentation results vs ground truth

![TextLocalization]({{ site.url }}/blog/images/text_segmentation/total_segs/montage_text_img0305___bboxes_0305.jpg)
*Fig. 10.  Text pixel segmentation results for a test image aggregated
from original, half-sized and double-sized pixel resolutions.  Ground truth
bounding boxes are colored yellow in the image on the right hand-side.*

![TextLocalization]({{ site.url}}/blog/images/text_segmentation/total_segs/montage_text_img0294___bboxes_0294.jpg)
*Fig. 11a.  Text pixel segmentation results in this test image actually
cover more genuine word content than that contained within the "ground
truth".  The false positives are not in fact really false.*

![TextLocalization]({{ site.url}}/blog/images/text_segmentation/total_segs/montage_text_img0006___bboxes_0006.jpg)
*Fig. 11b.  Text pixel segmentation results for in this test image more
accurately cover genuine word content than the axis-aligned bounding boxes.
False negative pixels around "BUTLER GARAGE" are not really false.*



![TextLocalization]({{ site.url}}/blog/images/text_segmentation/total_segs/montage_text_img0243___bboxes_0243.jpg)
*Fig. 12a.  Examples of genuine false positives in this test image include
building facade ornamentations which are incorrectly classified as text.*


![TextLocalization]({{ site.url }}/blog/images/text_segmentation/total_segs/montage_text_img0290___bboxes_0290.jpg)
*Fig. 12b.  Examples of genuine false negatives in this test image include
entire words outlined by yellow truth boxes which were missed by the classifier.*


![TextLocalization]({{ site.url }}/blog/images/text_segmentation/recall_prec/montage_recall_dens___precision_dens.jpg)
*Fig. 13.  Bounding box recall and text pixel precision distributions for 306 test images.*

## Segmentation of non-English text

![TextLocalization]({{ site.url}}/blog/images/text_segmentation/non-English/montage_segmented_image_00044_chinese___segmented_image_00066_chinese.jpg)

![TextLocalization]({{ site.url }}/blog/images/text_segmentation/non-English/montage_segmented_image_00052_greek___segmented_image_00068_arabic.jpg)
*Fig. 14.  Semantic segmentation of Chinese, Greek and Arabic text from
classifier trained on synthetic English phrases.*



## References

*.  General text-in-wild references 

*.  Jaderberg articles about synthetic text generation

1.  Y. LeCun, L. Bottou, Y. Bengio and P. Haffner, *Gradient-based learning
applied to document recognition*, Proc of IEEE (1998).

*.  Microsoft truth data reference

*.  MSRA for chinese text 

*.  NewsHour broadcasts for Greek and Arabic text

*.  See www.imagemagick.org/Usage/text/

*.  See http://research.microsoft.com/enus/um/people/eyalofek/text_detection_database.zip
