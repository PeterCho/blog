---
layout: post
title:  "Localizing Text-in-the-Wild via Synthetic Phrase Generation and
Semantic Segmentation"
date:   2016-04-25 12:00:0
categories: Deep Learning
---

## Synthetic text imagery generation

Optical Character Recognition (OCR) in cooperative settings such as
book-scanning machines is generally regarded as a solved problem within the
computer vision community.  In contrast, teaching machines to read
"text-in-the-wild" remains an outstanding challenge.  Unlike benign OCR
setups, text-in-the-wild presents many *a priori* unknown variables which
an AI system must handle.  These include wide ranges of text fonts, colors,
illumination conditions, 3D orientations, intrinsic character sizes,
distance ranges, foreground occlusions and background surface materials.
Moreover, gross spatial layouts for text appearing in the physical world
are highly variable.  Single characters, multiple abutted business names
and parking sign paragraphs can all be seen within complex urban
environments.  So the text-in-the-wild problem has drawn significant
interest among computer vision and machine learning researchers for many
years [refs].

In this blog entry, we focus upon just one aspect of reading
text-in-the-wild.  In particular, we are interested in localizing text in
arbitrary input images which represents a pre-requisite to text
recognition.  Given recent successes in supervised image classification,
one obvious attempt to solve this first stage of text-in-the-wild
processing would be to amass a training collection containing thousands of
manually labeled examples.  This brute force approach is often adopted by
big companies with huge budgets and sizable human resources.  But we are
more interested in exploring alternative sources of training data which do
not require massive human effort.

Text falls within a special category of objects seen in the physical world
inasmuch as it is often computer-generated.  Nearly all text appearing in
business advertisements, roadway street signs, and TV commercials
originated from some computer system.  And given the wide range of freely
available computer fonts, machines can now do a remarkably good job at
emulating human handwriting and painted words.  So we are interested in
synthesizing text imagery and seeing how well classifiers trained on it can
localize text-in-the-wild [Jaderberg refs].

We start by developing a corpus of English phrases which become inputs to
our synthetic image engine.  Since names frequently appear in the physical
world, we initially downloaded lists of male and female first names along
with common surnames.  We then randomly combined these parts to form
reasonable-sounding full names.  Some of the output names were exported in
capital letters.  Others had their first and/or middle names replaced by
initials.  In addition to synthesized people names, we also included
company names found via internet searches to our text source corpus.

Numerals also frequently appear in the physical world.  So we randomly
generated integers containing 1-6 digits as well as decimal numerals
containing 1 or 2 digits after a decimal point.  For 20% of the decimal
numbers, we prepended a "$" in order to simulate price tags often posted by
stores and businesses.  Telephone numbers containing 10 or 7 digits were
also randomly generated.  Some of the 3-digit area codes were separated by
parentheses, and some of the 7-digit telephone numbers were separated by
hyphens.

Text-in-the-wild often contains many words spanning multiple lines.  So we
harvested relatively long phrases from random news reports, computer blogs,
science articles and creative writing stories posted on the internet over
the past two decades.  White space was removed from these input sources,
but punctuation marks were intentionally retained.  Phrases containing 1 to
15 words were extracted from these text files.

After combining together and shuffling these different sources, our text
corpus contained O(215K) phrases.  We scanned through every corpus
character and rejected any whose ascii value did not lie within the
printable interval [32, 126].  No attempt was made to harvest words in
languages other than English.  So we believe the overwhelming majority of
our input phrases are in English.

We next downloaded and inspected hundreds of different truetype and
opentype fonts freely available on the internet.  We used our judgement to
retain "reasonable" looking fonts which mimics those that plausibly could
be seen in the real world.  Any font which could not render at least 10
digits and 26 uppercase Latin letters over character widths ranging from 5
- 10 pixels was rejected.  Fonts corresponding to cursive handwriting were
also intentionally included.  We ended up with 700+ fonts to use for
synthesizing text.  

Our synthetic phrase generator is based upon ImageMagick's text to image
capabilities [ref].  The generator first randomly selects some particular
text label from the large set of phrases along with a font.  A small subset
of the text labels have extra spaces inserted between each non-white
character.  Others have carriage returns inserted after each letter in
order to generate vertically aligned text.

RGB colors for the text foreground and possible stroke are next randomly
chosen.  If an underbox for the text is randomly selected, its background
color is required to be reasonably different from the text foreground's
color.  Otherwise, the foreground text is rendered against a transparent
background.

The overall width and height for the entire text phrase is quasi-randomly
selected so that a significant percentage of strings are rendered onto two
or more text lines.  For example, figure 1 illustrates a single phrase
which wraps onto 5 text lines:

![TextLocalization]({{site.url}}/blog/images/text_segmentation/syntext/padded_syn_phrase.png)
*Fig. 1. A synthetically generated image containing 15 words and spanning 5
text lines.  The foreground is rendered against a transparent background.*

After a text phrase is rendered, we compute reasonable estimates for all
the rendered characters' widths.  A very wide, temporary image is virtually
created to hold the entire input string within a single textline.
Individual character widths are found by subtracting lengths of virtually
rendered substrings containing c+1 and c characters within the wide,
temporary image.  (Index c ranges from 0 to one less than the number of
characters in the input text phrase.)  Once rendered character widths are
known and the beginnings of each text line within the actual synthetic
image are determined via horizontal and vertical profiling, pixel bounding
boxes around each character are readily derived.

Once character bounding boxes are known, masks indicating various text
properties may be generated.  For instance, the pixel bounding boxes
corresponding to letters starting individual words in a phrase may be
filled with one mask value.  Bounding boxes for characters located in the
middle or at the ends of words may similarly be filled with different mask
values.  Such character-within-word relationships for the text phrase in
figure 1 are illustrated in figure 2a.  White spaces as well as
single-character words are also assigned their own mask values.  The
resulting mask in figure 2a horizontally parses the multi-line, multi-word
text phrase rendering.  In contrast, a vertical parsing of the multi-line
text phrase is depicted in figure 2b.  In this second mask example, the top
and bottom halves of each character bounding box are assigned distinct
values.  The resulting mask pattern is useful for separating different
text lines within the rendered phrase.

![TextLocalization]({{site.url}}/blog/images/text_segmentation/syntext/padded_char_word_masks_b.png)
*Fig. 2a. Horizontal character masks are color-coded according to letter
spatial relationships.  Starting [stopping] characters in words are colored
blue [green].  Middle characters inside words are colored red.  Single
character words are colored orange.  Spaces between words are colored pink.
White character outlines in this figure are for visualization only.*

![TextLocalization]({{site.url}}/blog/images/text_segmentation/syntext/padded_char_word_masks_c.png)
*Fig. 2b. Text line masks are color-coded according to vertical
spatial relationships.  Upper [lower] half of text characters are colored
blue [red].*

A random fraction of the synthetic text images have drop shadow and
character shine effects added to make them look somewhat three-dimensional.
Another random fraction are intentionally corrupted via the addition of
simulated solar shadows and foreground occlusions.  The text image chips
along with their masks are then rotated by random azimuth, elevation and
roll angles.  After perspectively projecting the string image and masks
into the virtual camera, we use ImageMagick to generate and export their 2D
imageplane renderings.  

Following this synthesis procedure, we generated 131K+ foreground text
phrases plus accompanying image masks.  Some examples of these synthesized
text images are displayed in figure 3.

![TextLocalization]({{ site.url }}/blog/images/text_segmentation/syntext/syn_phrases.png)
*Fig. 3.  Representative examples of synthetically generated images
exhibiting a variety of text contents, fonts, colors, rotations, shadings,
shadowings and occlusions.*

Text in the physical world appears against a wide variety of backgrounds.
In some cases such as TV broadcasts and many roadsigns, text foreground is
sharply distinct from the solid, plain backgrounds on which it resides.  In
other instances such as store names on awnings or advertisements painted
onto vehicle sides, text foreground may be strongy shadowed or irregularly
illuminated.  Text-in-the-wild also frequently appears on complex
background surfaces like glass windows or non-rigid cloth flags.  Ideally,
a machine should learn to recognize text phrases independent of their
surrounding backgrounds.

We consequently quasi-randomly downloaded over 1000 pictures from the
internet to serve as backgrounds for our synthesized text image
foregrounds.  The pixel size for each background photo was required to be
larger than 1Kx1K.  Many of the backgrounds views are of real-world outdoor
or indoor scenes.  Others are more abstract computer graphics,
line-drawing or oil painting art.  In all cases, we inspected each
background picture and manually blackened out any text content it
originally contained.  

Given a text-free internet image as a backdrop, foreground text chips are
quasi-randomly superposed onto the background texture.  At each candidate
foreground text location, entropy density is calculated within the
background image.  The candidate text location is rejected if the
background entropy density is too high.  Moreover, average foreground and
bacground RGB values are calculated at nominated text sites.  If their
color contents do not sufficiently different, the candidate text location
is rejected.  The bounding box for any foreground chip is also not allowed
to overlap any existing text bounding boxes.  The number of foreground
overlays within each composite varies randomly over the interval [9,27].

Once a composite image is formed, each of its pixel's RGB values is
fluctuated by a nonzero amount of random gaussian noise.  A sizable
fraction of the composites are also randomly blurred.  To our surprise, we
observed that such composite degradation significantly improves subsequent
semantic segmentation classifier performance on test images containing
genuine text content.

For every composite image, we generate two 8-bit greyscale masks which
encode the locations of the synthetic text foregrounds.  One of the masks
encodes horizontal word-character spatial relationships while the other
contains vertical text-line spacing information.  Examples of these two
types of masks are displayed in figures 4 and 5 versus their corresponding
composite images.

![TextLocalization]({{site.url}}/blog/images/text_segmentation/syntext/montage_composite_image_00028___char_bytearray_00028.jpg)
*Fig. 4.  (a) Synthetically generated text phrases superposed on a
background image of an outdoor scene.  Note that text inside the roadsign
located near the lower right corner has been manually blackened. (b)
Corresponding 8-bit mask indicates character locations within words via
greyscale colorings.*

![TextLocalization]({{site.url}}/blog/images/text_segmentation/syntext/montage_composite_image_00187___word_bytearray_00187.jpg)
*Fig. 5.  (a) Synthetically generated text phrases superposed on a
background, text-free image of an indoor scene. (b) Corresponding 8-bit
mask indicates vertical text-line locations via greyscale colorings.*

Following this compositing procedure, we formed O(28K) composites along
with their two accompanying masks.  We subsequently decomposed each
composite into 3x3 tiles of pixel size 321x321.  So we produced over a
quarter-million synthetic image tiles.  As we shall see in the next
section, these tiles plus masks can be used to train deep semantic
segmentation networks to localize text-in-the-wild in arbitrary input
images.

## Semantic segmentation of text-in-the-wild

To perform pixel-level image segmentation, we utilize the DeepLab framework
which is built on top of Caffe [refs].  Deeplab combines deep neural
network (DNN) responses with densely connected conditional random fields
(CRF).  But we choose to not employ DeeplLab's CRF functionality as it is
computationally expensive.

Our synthetically generated 321x321 pixel image tiles become inputs to
Deeplab's network, while their corresponding masks become inputs to
Deeplab's loss function.  We also supply DeepLab with a set of
randomly-shuffled associations betwee image tiles and masks.  8K of the
250K+ tile and masks pairs are reserved for network validation, while the
rest are used for network training.

Using Stochastic Gradient Descent, DeepLab fine-tunes the weight and bias
parameters of a pre-trained DNN which is a variant of the VGG-16 model
[ref].  We adopt a batch size of 30, retain DeepLab's default 0.9 momentum
and 0.0005 weight decay parameter values, and utilize a step learning rate
policy.  We found that training and validation results were relatively
insensitive to modifications of the learning rate parameter.  So we adopted
a learning rate of 0.0045 [0.005] when fine-tuning the DNN for horizontal
word-character [vertical text-line] segmentation.  The loss functions for
these two DNN models are displayed in figure 6 over 3 training epochs.

![TextLocalization]({{ site.url}}/blog/images/text_segmentation/training/montage_horiz_lossfunc___vert_lossfunc.jpg)
*Fig. 6.  Horizontal word-character and vertical text-line segmentation
losses plotted as functions of DNN model training.*

After network fine-tuning has asymptotically finished, we can apply the
trained DNNs to localize genuine text in arbitrary input images.  In order
to quantitatively assess the performance of our classifiers on genuine
text-in-the-wild, we work with a challenging set of ground-truthed images
[ref].  It consists of 307 color images of outdoor urban scenes with
significant text content appearing on store fronts, billboard
advertisements, street signs, trucks and vans, etc.  Text observed in these
pictures exhibit a wide variety of fonts, colors, sizes, distance ranges,
angular orientations, physical locations and illumination conditions.

Horizontal text segmentation results for 5 representative images from this
data set are presented in figure 7.  Individual words may be isolated via
their starting and stopping letters as well as spaces between words.  But
in cases such as those illustrated in figure 8 where multiple text lines
are closely packed together, horizontal character-in-word pixel
classifications may bleed from one line into another.  Vertical
segmentation then becomes valuable for separating different lines of text
and the words contained within them.

![TextLocalization]({{ site.url }}/blog/images/text_segmentation/fullsized_segs/montage_text_img0055___segmented_image_00055.jpg)

![TextLocalization]({{ site.url }}/blog/images/text_segmentation/fullsized_segs/montage_text_img0062___segmented_image_00062.jpg)

![TextLocalization]({{ site.url }}/blog/images/text_segmentation/fullsized_segs/montage_text_img0101___segmented_image_00101.jpg)

![TextLocalization]({{ site.url }}/blog/images/text_segmentation/fullsized_segs/montage_text_img0116___segmented_image_00116.jpg)

![TextLocalization]({{ site.url}}/blog/images/text_segmentation/fullsized_segs/montage_text_img0149___segmented_image_00149.jpg)
*Fig. 7.  Horizontal text segmentation results for 5 different test images.
Starting [stopping] letters within words are tinted yellow [blue].  Middle
letters inside words are tinted green.  Spaces between words are colored orange.*

![TextLocalization]({{ site.url}}/blog/images/text_segmentation/vert_full_segs/montage_horiz_segmented_image_00076___vert_segmented_image_00076.jpg)

![TextLocalization]({{ site.url}}/blog/images/text_segmentation/vert_full_segs/montage_horiz_segmented_image_00201___vert_segmented_image_00201.jpg)
*Fig. 8.  Individual word versus vertical text line segmentation results
for 2 different images.  Colored tinting for horizontal characters appearing on
the left hand-side of these image pairs follows the same pattern as in
figure 7.  The top [bottom] half of text lines are tinted blue [yellow]
within the right hand-side of the image pairs.*

It is important to note that DeepLab's semantic segmentation kernel has a
maximum radius of O(100) pixels.  Moreover, we restricted the maximum width
of our synthesized characters to 100 pixels.  So our fine-tuned DNNs
struggle when segmenting letters or digits whose scales exceed 100 pixels.
Similarly, their performance deteriorates on strings with very small pixel
sizes.  Both of these maladies can be seen in figure 9a.

![TextLocalization]({{ site.url }}/blog/images/text_segmentation/img_pyramid/cropped_montage_imgpyr_1.jpg)
*Fig. 9a.  Individual word segmentation results for one particular test
image with 1024x1360 pixel size.*

We consequently feed half-sized and double-sized versions of each picture
along with the original-sized image into the trained DNNs when performing
inference.  As figure 9b demonstrates, the classifiers are then able to
localize a larger range of character sizes when they operate on resized
versions of the original picture.

![TextLocalization]({{ site.url }}/blog/images/text_segmentation/img_pyramid/cropped_montage_imgpyr_2.jpg)
*Fig. 9b.  Word segmentation results for the test image in fig. 9a 
at half-sized (512 x 680) and double-sized (2048 x 2720) pixel 
resolutions.*

Multi-class segmentation labels corresponding to different levels of the
input pyramid can straightforwardly be interpolated back to the original
image's pixel size.  They could then be aggregated into a single mask via
max pooling, average pooling or more sophisticated weighted pooling [ref].
But for purposes of comparing our text localization results with ground
truth, we simply convert all character classes into binary text versus
non-text pixel labels.  After taking the union of text labels across the 3
pyramid levels, we form a single binary mask that represents the
consolidated output from our detection system.  We quantitatively compare
these masks with ground truth bounding boxes in the next section.
  
## Text segmentation results vs ground truth

The 307 test images available for download at the website in ref [] are
accompanied by axis-aligned bounding boxes surrounding text strings which
were manually generated by human annotators (see figure 10).  These
bounding box metadata can be used to evaluate the precision and recall of
our text localization system. We compute a precision value for each test
image from the ratio of pixels classified as text that lie inside the
truthed bounding boxes to the total number of pixels classified as text.
Adopting an analogous measure for recall based solely upon pixel counts
would bias performance assessment towards text strings with large pixel
sizes.  So we instead compute the fractional coverage bounding boxes by
pixels classified as text within every test image.  We thus form precision
and recall distributions over the set of 307 test images.

![TextLocalization]({{ site.url }}/blog/images/text_segmentation/total_segs/montage_text_img0305___bboxes_0305.jpg)
*Fig. 10.  Text pixel segmentation results for a test image aggregated
from original, half-sized and double-sized pixel resolutions.  Ground truth
bounding boxes are colored yellow in the image on the right hand-side.*

We note that the ground-truth metadata is not in fact perfectly true.  For
example, genuine text-in-the-wild was sometimes missed by human annotators
as figure 11a illustrates.  In these cases, some of our system's valid
detections are erroneously scored as false positives.  In other instances,
axis-aligned bounding boxes do not snuggly fit around text strings
appearing in the test images as can be seen in figure 11b.  Consequently,
some of our system's pixels marked with non-text labels are incorrectly
scored as false negatives.  Of course, the truth data do reveal genuine
false positive and false negative mistakes made by the trained DNN
classifiers in other examples such as those listed in figure 12.


![TextLocalization]({{ site.url}}/blog/images/text_segmentation/total_segs/montage_text_img0294___bboxes_0294.jpg)
*Fig. 11a.  Text pixel segmentation results in this test image actually
cover more genuine word content than that contained within the "ground
truth".  The false positives are not in fact really false.*

![TextLocalization]({{ site.url}}/blog/images/text_segmentation/total_segs/montage_text_img0006___bboxes_0006.jpg)
*Fig. 11b.  Text pixel segmentation results for in this test image more
accurately cover genuine word content than the axis-aligned bounding boxes.
False negative pixels around "BUTLER GARAGE" are not really false.*

The precision and recall distributions presented in figure 13 represent
approximate but useful quantifications of our synthetically-trained
system's performance on a challenging text-in-the-wild data set.  Both
distributions exhibit long tails.  So we quote their median rather than
average values in order to condense them to single numbers: Precision =
75ish%, Recall = 86%.

![TextLocalization]({{ site.url}}/blog/images/text_segmentation/total_segs/montage_text_img0243___bboxes_0243.jpg)
*Fig. 12a.  Examples of genuine false positives in this test image include
building facade ornamentations which are incorrectly classified as text.*

![TextLocalization]({{ site.url }}/blog/images/text_segmentation/total_segs/montage_text_img0290___bboxes_0290.jpg)
*Fig. 12b.  Examples of genuine false negatives in this test image include
entire words outlined by yellow truth boxes which were missed by the classifier.*

![TextLocalization]({{ site.url }}/blog/images/text_segmentation/recall_prec/montage_recall_dens___precision_dens.jpg)
*Fig. 13.  Bounding box recall and text pixel precision distributions for 306 test images.*

Since localization would most likely represent the first step in most
end-to-end text recognition systems, it is practically more important for
recall to be as close to unity as possible even if precision must suffer.
Presumably, false positive text detections could be elimiminated by more
specialized classifiers of letters, digits, words and/or phrases at
subsequent stages in an end-to-end pipeline.  In contrast, it would likely
be much more difficult to later find words if they are completely missed at
the nomination stage.  While 86% recall is still far from perfection, we
note that this value is significantly larger than the XX% recall achieved
by the researchers who developed the data set in ref [].

## Segmentation of non-English text

We close this blog entry with an intriguing observation with regards to
detecting text-in-the-wild written in languages other than English.  As far
as we know, nearly all of the non-numnerical synthesized phrases used to
train the DNNs contained English words.  And none of them contained
characters from outside the Latin alphabet.  Yet gross spatial
relationships among characters within and between words such as starting,
stopping, middle and space transcend the English language.

It is consequently interesting to see our classifiers' results in figure 14
which contain text in Chinese, Greek and Arabic.  I cannot read any of
these languages.  But the color-coded segmentations in this figure look
reasonable to my eye.

![TextLocalization]({{ site.url}}/blog/images/text_segmentation/non-English/montage_segmented_image_00044_chinese___segmented_image_00066_chinese.jpg)

![TextLocalization]({{ site.url }}/blog/images/text_segmentation/non-English/montage_segmented_image_00052_greek___segmented_image_00068_arabic.jpg)
*Fig. 14.  Semantic segmentation of Chinese, Greek and Arabic text from
classifier trained on synthetic English phrases.*



## References

*.  General text-in-wild references 

*.  Jaderberg articles about synthetic text generation

1.  Y. LeCun, L. Bottou, Y. Bengio and P. Haffner, *Gradient-based learning
applied to document recognition*, Proc of IEEE (1998).

*.  See http://research.microsoft.com/enus/um/people/eyalofek/text_detection_database.zip


*.  C. Yao, X. Bai, W. Liu, Y. Ma and Z. Tu, *Detecting Texts of Arbitrary
Orientations in Natural Images*, CVPR (1083).

*.  NewsHour broadcasts for Greek and Arabic text

*.  See www.imagemagick.org/Usage/text/

