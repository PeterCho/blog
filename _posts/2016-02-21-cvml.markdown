---
layout: post
title:  "Recognizing Digits via Synthetic Character Generation and Deep
Network Finetuning"
date:   2016-02-21 12:00:0
categories: Deep Learning
---

## Digit Chip Data

Automatic digit recognition is a classic problem which lies at the
intersection of machine learning and computer vision.  One of the earliest
applications of Convolutional Neural Networks (CNNs) was to recognizing
hand-written digits on envelopes [1].  Lecun and collaborators assembled
their well-known MNIST data set consisting of 60K examples of greyscale
digits superposed against 28x28 white pixel backgrounds.  (A few examples
of MNIST image chips are displayed in figure 1.)  In 1998, these authors
developed CNNs which could classify input chips as corresponding to digits
0, 1, 2, ..., 9 with accuracies around 99.3%.  Over the past few years,
researchers have refined CNNs to classify MNIST digits with accuracies
close to 99.8% [2].

![Digits]({{ site.url }}/blog/images/digit_recog/mnist_digit_examples.png)
*Fig. 1.  Representative examples of MNIST digit image chips.*

In 2011, a team of Stanford researchers harvested a set of digits from
Google Street View imagery which they dubbed the Street View House Numbers
(SVHN) data set [3,4].  As figure 2a illustrates, SVHN digits exhibit
significantly more variation and complexity compared to their MNIST
predecessors.  Consequently, classifying the former is much more
challenging than the latter.  SVHN digit recognition accuracies around 98%
have been reported in the recent literature [2].

![Digits]({{ site.url }}/blog/images/digit_recog/svhn_digit_examples.png)
*Fig. 2.  Representative examples of SVHN digit image chips.*

Unfortunately, we have found a non-negligible number of labeling errors
within the SVHN data set.  In some cases, not enough genuine text content
exists within chips for them to be meaningfully assigned digit labels.  In
other instances, the assigned labels are obviously wrong.  Figure 3
presents examples of both sorts of problems.  In order to use SVHN image
chips for classifier testing purposes, we introduce a "non-character" label
in addition to the 0 - 9 digit labels.  We have also carefully
doublechecked the label assignments for 3000 SVHN image chips and corrected
a number of labeling errors.  We work only with this cleaned subset of 3000
SVHN chips in our digit classification experiments.

![Digits]({{ site.url }}/blog/images/digit_recog/bad_svhn_labels.png)
*Fig. 3.  Examples of SVHN image chips which either are unrecognizable or
incorrectly labeled.  The originally assigned label for each chip appears
as the first digit in its JPG image filename.*

## Synthetic Digit Chip Generation

Digit recognition represents a special case of the more general problem of
recognizing "text in-the-wild".  Many academics regard standard optical
character recognition in highly-cooperative settings such as book scanning
to be a solved problem.  But automatically reading real-world text
appearing street signs, store fronts, posters, roadway surfaces, etc
remains an outstanding technical challenge.

The currently most popular and successful approach to object recognition in
computer vision involves supervised training of CNNs.  Since CNNs typically
involve millions of free parameters, vast numbers of labeled samples are
needed to train these data-hungry models.  Unfortunately, generating large
training sets is laborious and error-prone.  So finding alternatives to
brute-force labeling of training examples is of significant practical
interest.

The source for much text-in-the wild can often be traced to computers.  For
instance, commercial print shops use computers to virtually design and then
physically print placards for their customers.  So for the particular case
of text, it is reasonable to ask if synthetically generated characters and
strings could serve as a source of CNN training data.  If this question has
an affirmative answer, vast quantities of text training examples could be
generated with much less effort than by cropping and labeling images of
real-world text.

Other authors have previously trained CNNs via synthetically generated text
and reported encouraging recognition results [Jaderberg].  We build upon
this earlier work and extend it in some new directions.  In particular,
Jaderberg et al describe in some detail their approach to synthesizing text
strings [Jaderberg].  They choose to work with greyscale foregrounds, and
they intentionally introduce potentially confusing background content.  We
will instead work with colored foregrounds superposed against a larger
variety of backgrounds.

To begin, we choose 113 true type and open type fonts which we regard as
"reasonable" approximations to text that frequently appears in the wild.
32 of these fonts mimic human handwriting, while the regularity of the
remaining 81 fonts looks like they could only have been written by a
machine.  These two broad categories of writing styles roughly correspond
to those encountered among the MNIST and SVHN digits.  Figures 4a and 4b
exhibit examples of "handwriting" and "computer" fonts that we adopt for
digit synthesis.

![Digits]({{ site.url }}/blog/images/digit_recog/handwriting_fonts.png) 
*Fig. 4a.  Representative examples of "handwriting" fonts used for synthetic
digit generation.*

![Digits]({{ site.url }}/blog/images/digit_recog/computer_fonts.png) 
*Fig. 4b.  Representative examples of "computer" fonts used for synthetic
digit generation.*

Looping over a large set of digit characters, we select an image chip's
width and aspect ratio according to exponential and gaussian
distributions. The chip's background is then either extracted from 375
internet images of different textures (e.g. brick, concrete, glass, marble,
metal, stone, stucco, wood, etc), or it is set to a random color which may
contain a few vertical and/or horizontal lines.  We next quasi-randomly
pick text and stroke colors.  The foreground colors are required to be
acceptable with the previously-selected background color according to a
pre-trained SVM decision rule.  

The text character is initially synthesized against black or white
backgrounds depending upon the foreground color's brightness.  The
character is next rotated in three dimensions.  Azimuth, elevation and roll
angles are random variables selected from gaussian distributions chosen to
simulate camera views of text in the wild.  The rotated synthetic character
is subsequently projected back into a 2D image plane.  At least 60% of the
text charater's foreground content is required to actually appear inside
the rotated image chip.  If the foreground fraction is less than 25%, the
image chip's classification label is reset to "non-character".  Rotated
characters with foreground content between 25% - 60% are discarded.
Non-text image chips are also intentionally generated with frequency
comparable to 1/n_classes.

Surviving text character foregrounds are superposed onto nontrivial
backgrounds.  Separate color channel averaging is performed near the text
character's border.  The RGB content of the superposed image is compared
with that of progenitor background chip.  If the two RGB contents do not
significantly differ, the candidate text chip is rejected.

In the final steps of the synthetic digit generation pipeline, image chips
are intentionally corrupted.  For example, we pick a random line passing
through some small fraction of the chips.  Intensities for all pixels lying
below this line are then decreased by 50% relative to their original
values.  The resulting chips appear to include simulated solar shadows.
Random gaussian noise is also added to all image chips.  And random
blurring is performed on a percentage of the final image chips.

This entire synthetic text character formation procedure runs fairly
quickly.  Our linux computer generates 100K+ simulated digits in less than
30 minutes.  Figure 5 illustrates representative examples of synthetic
digit as well as non-character image chips.  Although we cannot expect to
capture all possible variations exhibited by text in the real world, the
synthetic chips in figure 5 look qualitatively similar to the SVHN chips in
figure 2.

![Digits]({{ site.url }}/blog/images/digit_recog/synthesized_digits.png) 
*Fig. 5.  Representative examples of synthetically generated digits.*

## DNN Model Finetuning

Starting with the classic paper of Lecun et al [1], CNN approaches to
digit recognition have usually involved architecting and training
customized networks.  We are interested in asking how well simply
finetuning a previously trained deep neural network (DNN) can work.  



![Digits]({{ site.url }}/blog/images/digit_recog/training_performance.jpg)    
*Fig. 6a.  Loss function for synthetically generated digits.*

![Digits]({{ site.url}}/blog/images/digit_recog/validation_performance.jpg)   
*Fig. 6b.  Validation accuracy for synthetically generated digits.*

![Digits]({{ site.url}}/blog/images/digit_recog/correctly_classified_svhn.png) 
*Fig. 7a.  Representative examples of correctly-classified SVHN digits.*

![Digits]({{ site.url}}/blog/images/digit_recog/incorrectly_classified_svhn.png) 

*Fig. 7b.  Representative examples of incorrectly-classified SVHN digits.
The output label for each chip appears as the last digit in its JPG image
filename.*


## Ensemble Averaging

|  number   	| SVHN    	| Correct    	| MNIST  	| Correct 	|
|  models  	| errors   	| percentage   	| errors 	| percentage  	|
|:-------------:|:-------------:|:-------------:|:-------------:|:-------------:|
|   1		|  828 		| 90.80  	| 708  		| 90.56	|
|   2		|  771 		| 91.43  	| 664  		| 91.14	|
|   3 		|  752 		| 91.64  	| 644  		| 91.36	|
|   4 		|  729 		| 91.90  	| 641  		| 91.41	|
|   5		|  717 		| 92.03  	| 641		| 91.45	|
|   6		|  723 		| 91.96  	| 644  		| 91.41	|
|   7		|  704 		| 92.17  	| 626  		| 91.65	|
|   8		|  706 		| 92.15  	| 606  		| 91.92	|

*Fig. 8.  Ensemble performance statistics*


|       | non	| 0 	| 1	|  2 	|  3	|  4 	|  5 	|  6 	|7  	|  8 	| 9 	| n_incorrect | percent incorrect |
|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----------:|:-----------------:|
|**non**| 262   |  30  	| 15  	|  5  	|  0  	|  4   	| 15  	|  6  	| 2	| 12  	|  18 	| 107         |  29.00 |
|**0**	|  6  	| 780 	|  6  	|  0  	|  0  	|  0   	|  0  	|  0  	| 0	|  0  	|  0  	| 12          | 1.52   |
|**1** 	|  2  	|  0  	| 771 	|  0  	|  0  	|  48  	|  6  	|  0  	| 0	|  0  	|  1	| 57 	      |  6.88  |
|**2**	|  2  	|  0 	|  12	|1325 	|  0  	|  13  	|  0	|  0  	| 0	|  9  	|  25 	| 61          | 4.40 |
|**3**	|  9	|  1  	|  3	|  3  	| 954 	|  7  	| 54	|  20 	| 9	| 15  	|   8  	| 129         | 1.19 |
|**4**  |  6  	|  0  	|  27	|  22 	|  0  	| 629  	|  0	|  3  	| 3	|  2  	|   4  	| 67 	      | 9.63 |
|**5**	|  9  	|  3  	|  0	|  0  	|  0  	|  6   	| 984 	|  30 	| 0	|  0  	|   0  	| 48 	      | 4.65 |
|**6**  |  7  	|  15 	|  0	|  0  	|  3  	|  0   	|  3	| 764 	| 0	|  21 	|   0  	| 49          | 6.03 |
|**7**  |  9 	|  3  	|  93	|  9 	|  3  	|  9   	|  0	|  0	|575 	|  0  	|   4  	| 130 	      | 18.44|
|**8**	|  0  	|  12 	|  0	|  0  	|  3  	|  0   	|  0	|  6  	| 0	| 681 	|   9  	| 30 	      | 4.22 |
|**9**  |  0  	|  6 	|  3	|  0  	|  0  	|  5  	|  0	|  0  	| 0	|  2 	|  569 	| 16          | 2.73 |


*Fig. 9.  Classified SVHN digits' confusion matrix for 8-model ensemble.*


## References

1.  Y. LeCun, L. Bottou, Y. Bengio and P. Haffner, *Gradient-based learning
applied to document recognition*, Proc of IEEE (1998).

2.  See
http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html .

3.  See http://ufldl.stanford.edu/housenumbers/ .

4.  Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu and A. Y. Ng,
*Reading digits in natural images with unsupervised feature learning*, NIPS
Workshop on Deep Learning and Unsupervised Feature Learning, (2011).

5.  M. Jaderberg, K. Simonyan, A. Vedaldi and A. Zisserman, *Synthetic data
and artificial neural networks for natiural scene text recognition*, arXiv
1406.2227v4 (2014).

6.  K. Simonyan and A. Zisserman, *Very Deep Convolutional Networks for
Large-Scale Image Recognition*, arXiv 1409.1556, (2015).

7.  T. G. Dietterich, *Ensemble methods in machine learning,* In Multiple
classifier systems, pages 1–15. Springer, (2000).
