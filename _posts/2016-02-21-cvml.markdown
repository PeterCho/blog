---
layout: post
title:  "Recognizing Digits via Synthetic Character Generation and Deep
Network Finetuning"
date:   2016-02-21 12:00:0
categories: Deep Learning
---

## Digit Image Chips

Automatic digit recognition is a classic problem which lies at the
intersection of machine learning and computer vision.  One of the earliest
applications of Convolutional Neural Networks (CNNs) was to recognizing
hand-written digits on envelopes [1].  Lecun and collaborators assembled
their well-known MNIST data set consisting of 60K examples of greyscale
digits superposed against 28x28 white pixel backgrounds.  (A few examples
of MNIST image chips are displayed in figure 1.)  In 1998, these authors
developed CNNs which could classify input chips as corresponding to digits
0, 1, 2, ..., 9 with accuracies around 99.3%.  Over the past few years,
researchers have refined CNNs to classify MNIST digits with accuracies
close to 99.8% [2].

![Digits]({{ site.url }}/blog/images/digit_recog/mnist_digit_examples.png)
*Fig. 1.  Representative examples of MNIST digit image chips.*

In 2011, a team of Stanford researchers harvested a set of digits from
Google Street View imagery which they dubbed the Street View House Numbers
(SVHN) data set [3,4].  As figure 2a illustrates, SVHN digits exhibit
significantly more variation and complexity compared to their MNIST
predecessors.  Consequently, classifying the former is much more
challenging than the latter.  SVHN digit recognition accuracies around 98%
have been reported in the recent literature [2].

![Digits]({{ site.url }}/blog/images/digit_recog/svhn_digit_examples.png)
*Fig. 2.  Representative examples of SVHN digit image chips.*

Unfortunately, we have found a non-negligible number of labeling errors
within the SVHN data set.  In some cases, not enough genuine text content
exists within chips for them to be meaningfully assigned digit labels.  In
other instances, the assigned labels are obviously wrong.  Figure 3
presents examples of both sorts of problems.  In order to use SVHN image
chips for classifier testing purposes, we introduce a "non-character" label
in addition to the 0 - 9 digit labels.  We have also carefully
doublechecked the label assignments for 3000 SVHN image chips and corrected
a number of labeling errors.  We work only with this cleaned subset of 3000
SVHN chips in our digit classification experiments.

![Digits]({{ site.url }}/blog/images/digit_recog/bad_svhn_labels.png)
*Fig. 3.  Examples of SVHN image chips which either are unrecognizable or
incorrectly labeled.  The originally assigned label for each chip appears
as the first digit in its JPG image filename.*

## Synthetic Digit Chip Generation

Digit recognition represents a special case of the more general problem of
recognizing "text in-the-wild".  Many academics regard standard optical
character recognition in highly-cooperative settings such as book scanning
to be a solved problem.  But automatically reading real-world text
appearing street signs, store fronts, posters, roadway surfaces, etc
remains an outstanding technical challenge.

The currently most popular and successful approach to object recognition in
computer vision involves supervised training of CNNs.  Since CNNs typically
involve millions of free parameters, vast numbers of labeled samples are
needed to train these data-hungry models.  Unfortunately, generating large
training sets is laborious and error-prone.  So finding alternatives to
brute-force labeling of training examples is of significant practical
interest.

The source for much text-in-the wild can often be traced to computers.  For
instance, commercial print shops use computers to virtually design and then
physically print placards for their customers.  So for the particular case
of text, it is reasonable to ask if synthetically generated characters and
strings could serve as a source of CNN training data.  If this question has
an affirmative answer, vast quantities of text training examples could be
generated with much less effort than by cropping and labeling images of
real-world text.

Other authors have previously trained CNNs via synthetically generated text
and reported encouraging recognition results [Jaderberg].  We build upon
this earlier work and extend it in some new directions.  In particular,
Jaderberg et al describe in some detail their approach to synthesizing text
strings [Jaderberg].  They choose to work with greyscale foregrounds, and
they intentionally introduce potentially confusing background content.  We
will instead work with colored foregrounds superposed against a larger
variety of backgrounds.

To begin, we choose 113 true type and open type fonts which we regard as
"reasonable" approximations to text that frequently appears in the wild.
32 of these fonts mimic human handwriting, while the regularity of the
remaining 81 fonts looks like they could only have been written by a
machine.  These two broad categories of writing styles roughly correspond
to those encountered among the MNIST and SVHN digits.  Figures 4a and 4b
exhibit examples of "handwriting" and "computer" fonts that we adopt for
digit synthesis.

![Digits]({{ site.url }}/blog/images/digit_recog/handwriting_fonts.png) 
*Fig. 4a.  Representative examples of "handwriting" fonts used for synthetic
digit generation.*

![Digits]({{ site.url }}/blog/images/digit_recog/computer_fonts.png) 
*Fig. 4b.  Representative examples of "computer" fonts used for synthetic
digit generation.*

Looping over a large set of digit characters, we next select an image
chip's width and aspect ratio according to exponential and gaussian
distributions. The chip's background is either extracted from 375 internet
images of different textures (e.g. brick, concrete, glass, marble, metal,
stone, stucco, wood, etc), or it is set to a random color with possible
vertical and/or horizontal lines.  We next pick text and stroke colors
quasi-randomly.  The foregorund colors are forced to be acceptable with the
previously-selected background color according to a pre-trained SVM
decision rule.  The text character is then synthesized against black or
white backgrounds.


![Digits]({{ site.url }}/blog/images/digit_recog/synthesized_digits.png) 
*Fig. 5.  Representative examples of synthetically generated digits.*

![Digits]({{ site.url }}/blog/images/digit_recog/training_performance.jpg)    
*Fig. 6a.  Loss function for synthetically generated digits.*

![Digits]({{ site.url}}/blog/images/digit_recog/validation_performance.jpg)   
*Fig. 6b.  Validation accuracy for synthetically generated digits.*

![Digits]({{ site.url}}/blog/images/digit_recog/correctly_classified_svhn.png) 
*Fig. 7a.  Representative examples of correctly-classified SVHN digits.*

![Digits]({{ site.url}}/blog/images/digit_recog/incorrectly_classified_svhn.png) 

*Fig. 7b.  Representative examples of incorrectly-classified SVHN digits.
The output label for each chip appears as the last digit in its JPG image
filename.*


|       | non	| 0 	| 1	|  2 	|  3	|  4 	|  5 	|  6 	|  7  	|  8 	| 9 	|
|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|
|**non**| 251   |  28  	| 12  	|  4  	|  2  	|  9   	| 16  	|  7  	|  2  	| 10  	|  28 	|
|**0**	|  7  	| 775 	|  6  	|  0  	|  0  	|  2   	|  0  	|  1  	|  0  	|  1  	|  0  	|
|**1** 	|  3  	|  1  	| 760 	|  2  	|  1  	|  52  	|  5  	|  0  	|  2  	|  0  	|  2	|
|**2**	|  6  	|  2 	|  11	|1317 	|  0  	|  10  	|  0	|  0  	|  2  	|  6  	|  32 	|
|**3**	|  6	|  1  	|  4	|  3  	| 922 	|  9  	| 85	|  17 	|  11 	| 19  	|   6  	|
|**4**  |  2  	|  1  	|  24	|  24 	|  0  	| 632  	|  0	|  6  	|  3  	|  0  	|   4  	|
|**5**	|  8  	|  3  	|  0	|  0  	|  1  	|  6   	| 970 	|  43 	|  0  	|  0  	|   1  	|
|**6**  |  8  	|  15 	|  0	|  2  	|  4  	|  0   	|  3	| 748 	|  0  	|  31 	|   2  	|
|**7**  |  10 	|  1  	| 103	|  17 	|  2  	|  9   	|  0	|  0  	| 556 	|  1  	|   6  	|
|**8**	|  0  	|  16 	|  0	|  0  	|  4  	|  0   	|  3	|  3  	|  0  	| 676 	|   9  	|
|**9**  |  0  	|  3  	|  0	|  0  	|  1  	|  11  	|  1	|  0  	|  0  	|  4  	|  565 	|


*Fig. 8.  Classified digits' confusion matrix*


## Ensemble Averaging

|  number   	| SVHN    	| Correct    	| MNIST  	| Correct 	|
|  models  	| errors   	| fraction   	| errors 	| fraction  	|
|:-------------:|:-------------:|:-------------:|:-------------:|:-------------:|
|   1		|  828 		| 0.9080  	| 708  		| 0.9056	|
|   2		|  771 		| 0.9143  	| 664  		| 0.9114	|
|   3 		|  752 		| 0.9164  	| 644  		| 0.9136	|
|   4 		|  729 		| 0.9190  	| 641  		| 0.9141	|
|   5		|  717 		| 0.9203  	| 641		| 0.9145	|
|   6		|  723 		| 0.9196  	| 644  		| 0.9141	|
|   7		|  704 		| 0.9217  	| 626  		| 0.9165	|
|   8		|  706 		| 0.9215  	| 606  		| 0.9192	|

*Fig. 9.  Ensemble performance statistics*


## References

1.  Y. LeCun, L. Bottou, Y. Bengio and P. Haffner, *Gradient-based learning
applied to document recognition*, Proc of IEEE (1998).

2.  See
http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html .

3.  See http://ufldl.stanford.edu/housenumbers/ .

4.  Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu and A. Y. Ng,
*Reading digits in natural images with unsupervised feature learning*, NIPS
Workshop on Deep Learning and Unsupervised Feature Learning, (2011).

5.  M. Jaderberg, K. Simonyan, A. Vedaldi and A. Zisserman, *Synthetic data
and artificial neural networks for natiural scene text recognition*, arXiv
1406.2227v4 (2014).

6.  K. Simonyan and A. Zisserman, *Very Deep Convolutional Networks for
Large-Scale Image Recognition*, arXiv 1409.1556, (2015).

7.  T. G. Dietterich, *Ensemble methods in machine learning,* In Multiple
classifier systems, pages 1â€“15. Springer, (2000).
