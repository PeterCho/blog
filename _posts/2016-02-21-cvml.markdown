---
layout: post
title:  "Recognizing Digits via Synthetic Character Generation and Deep
Network Finetuning"
date:   2016-02-21 12:00:0
categories: Deep Learning
---

## Digit Chip Data

Automatic digit recognition is a classic problem at the intersection of
machine learning and computer vision.  One of the earliest applications of
Convolutional Neural Networks (CNNs) was to recognizing hand-written digits
[1].  Lecun and collaborators assembled their well-known "Modified NIST
(MNIST)" data set consisting of 60K examples of greyscale digits superposed
against 28x28 white pixel backgrounds.  (A few examples of MNIST image
chips are displayed in figure 1.)  In 1998, these authors developed CNNs
which could classify input chips as corresponding to digits 0, 1, 2, ..., 9
with accuracies around 99.3%.  Over the past few years, researchers have
refined CNNs to classify MNIST digits with accuracies approaching 99.8%
[2].

![Digits]({{ site.url }}/blog/images/digit_recog/mnist_digit_examples.png)
*Fig. 1.  Representative examples of MNIST digit image chips.*

In 2011, a team of Stanford researchers harvested a set of digits from
Google Street View imagery which they dubbed the Street View House Numbers
(SVHN) data set [3,4].  As figure 2 illustrates, SVHN digits exhibit
significantly more variation and complexity compared to their MNIST
predecessors.  Consequently, classifying the former is much more
challenging than the latter.  SVHN digit recognition accuracies around 98%
have been reported in the recent literature [2].

![Digits]({{ site.url }}/blog/images/digit_recog/svhn_digit_examples.png)
*Fig. 2.  Representative examples of SVHN digit image chips.*

Unfortunately, we have found a non-negligible number of labeling errors
within the SVHN data set.  In some cases, not enough genuine text content
exists within chips for them to be meaningfully assigned digit labels.  In
other instances, the assigned labels are obviously wrong.  Figure 3
presents examples of both problems:

![Digits]({{ site.url }}/blog/images/digit_recog/bad_svhn_labels.png)
*Fig. 3.  Examples of SVHN image chips which either are unrecognizable or
incorrectly labeled.  The originally assigned label for each chip appears
as the first digit in its JPG image filename.*


In order to use SVHN image chips for classifier testing purposes, we
introduce a "non-character" label in addition to the original 0 - 9 digit
labels.  We have also carefully doublechecked the label assignments for
3000 SVHN image chips and corrected a number of labeling errors.  We only
work with this cleaned subset of 3000 SVHN chips in our digit
classification experiments.


## Synthetic Digit Chip Generation

Digit recognition represents a special case of the more general problem of
recognizing "text in-the-wild".  Many academics regard standard optical
character recognition in highly-cooperative settings such as book scanning
to be a solved problem.  But automatically reading real-world text
appearing street signs, store fronts, posters, roadway surfaces, etc
remains an outstanding technical challenge.

The currently most popular and successful approach to object recognition in
computer vision involves supervised training of CNNs.  Since CNNs typically
involve millions of free parameters, vast numbers of labeled samples are
needed to train these data-hungry models.  Unfortunately, generating large
training sets is laborious and error-prone.  So finding alternatives to
brute-force labeling of training examples is of great practical interest.

The source for much text-in-the wild can often be traced to computers.  For
instance, commercial print shops use computers to virtually design and then
physically print placards for their customers.  So for the particular case
of text, it is reasonable to ask if synthetically generated characters and
strings could serve as a source of CNN training data.  If this question has
an affirmative answer, vast quantities of text training examples could be
generated with much less effort than by cropping and labeling images of
real-world text.

Other authors have previously trained CNNs via synthetically generated text
and reported encouraging recognition results [5].  We build upon this
earlier work and extend it in new directions.  In particular, Jaderberg et
al describe in some detail their approach to synthesizing text strings [5].
They choose to work with greyscale foregrounds, and they intentionally
introduce potentially confusing background content.  We will instead work
with colored foregrounds superposed against a larger variety of
backgrounds.

To begin, we choose 113 true type and open type fonts which we regard as
"reasonable" approximations to text that frequently appears in the wild.
Among the 113 fonts, 32 mimic human handwriting, while the regularity of
the remaining 81 looks like they could only have been written by a machine.
These two broad categories of writing styles roughly correspond to those
encountered among the MNIST and SVHN digits.  Figures 4a and 4b exhibit
examples of "handwriting" and "computer" fonts that we adopt for digit
synthesis.

![Digits]({{ site.url }}/blog/images/digit_recog/handwriting_fonts.png) 
*Fig. 4a.  Representative examples of "handwriting" fonts used for synthetic
digit generation.*

![Digits]({{ site.url }}/blog/images/digit_recog/computer_fonts.png) 
*Fig. 4b.  Representative examples of "computer" fonts used for synthetic
digit generation.*

Looping over a large set of digit characters, we select an image chip's
width and aspect ratio according to exponential and gaussian distributions.
The chip's pixel width is required to lie within the interval [27,80],
while its aspect ratio is confined to the interval [0.35, 0.85].  The
chip's background is then either extracted from 375 internet images of
different textures (e.g. brick, concrete, glass, marble, metal, stone,
stucco, wood, etc), or it is set to a random color which may contain a few
vertical and/or horizontal lines.  We next quasi-randomly pick text and
stroke colors.  The foreground colors are required to be visible against
the previously-selected background color according to a pre-trained SVM
decision rule.

The text character is initially synthesized against black or white
backgrounds depending upon the foreground color's brightness.  The
character is next rotated in three dimensions.  Azimuth, elevation and roll
angles are random variables selected from gaussian distributions chosen to
simulate camera views of text in the wild.  The rotated synthetic character
is subsequently projected back into a 2D image plane.  At least 60% of the
text charater's foreground content is required to actually appear inside
the rotated image chip.  If the foreground fraction is less than 25%, the
image chip's classification label is reset to "non-character".  Rotated
characters with foreground content between 25% - 60% are discarded.
Non-text image chips are also intentionally generated with frequency
comparable to 1/n_classes = 1/10. 

Surviving text character foregrounds are superposed onto nontrivial
backgrounds.  Individual color channel averaging is performed near the text
character's border.  The RGB content of the superposed image is compared
with that of progenitor background chip.  If the two RGB contents do not
significantly differ so that the foreground digit can at least barely be
seen against its background, the candidate text chip is rejected.

In the final steps of the synthetic digit generation pipeline, image chips
are intentionally corrupted.  For example, we pick a random line passing
through some small fraction of the chips.  Intensities for all pixels lying
below this line are then decreased by 50% relative to their original
values.  The resulting chips thus include simulated solar shadows.  Random
gaussian noise is also added to all image chips.  And random blurring is
performed on a percentage of the final chips.

This entire synthetic text character formation procedure runs fairly
quickly.  Our linux computer generates 100K+ simulated digits in less than
30 minutes.  Figure 5 illustrates representative examples of synthetic
digit as well as non-character image chips.  Although we cannot expect to
capture all possible variations exhibited by text in the real world, the
synthetic chips in figure 5 look qualitatively similar to the SVHN chips in
figure 2.

![Digits]({{ site.url }}/blog/images/digit_recog/synthesized_digits.png)
*Fig. 5.  Representative examples of synthetically generated digit and
non-character chips.*

## DNN Model Finetuning

Starting with the 1998 paper of Lecun et al [1], CNN approaches to digit
recognition have usually involved architecting and training customized
networks.  We are interested in asking how well simply finetuning a
previously trained deep neural network (DNN) can work.  So we begin with
the VGG-16 model on ImageNet photos [6] and replace its last layer holding
1000 classification categories with a new layer corresponding to the 11
labels "non-character", "0", "1", ..., "9".

Working within the caffe framework [7], we retrain the modified VGG-16
model using O(140K) synthesized image chips.  We reserve 20% of the
synthetic data for model validation.  Consequently, each of our 11 labeled
categories, including the non-character class, has O(10K) synthetically
generated training samples.

We synthesize a dozen independent sets of O(140K) digit image chips and
finetune the VGG-16 model on each of them.  Figure 6 displays the training
loss function and validation accuracy over 3 epochs for one representative
run of model finetuning:

![Digits]({{ site.url }}/blog/images/digit_recog/training_performance.jpg)    
![Digits]({{ site.url}}/blog/images/digit_recog/validation_performance.jpg)   
*Fig. 6.  Loss function and validation accuracy for synthetically
generated digits plotted against fine tuning epoch.*

After finetuning the dozen VGG-16 models, we performed inference using the
retrained models on our cleaned set of 3000 SVHN digit chips.  We could
then evaluate the synthetically-trained models' accuracy on real-world
data.  For example, one of the finetuned models correctly classified 2724
of the 3000 SVHN digits which implies a precision accuracy of 90.80%.
(Representative examples of SVHN chips correctly and incorrectly classified
by a synthetically-trained VGG-16 model are presented in figures 7a and
7b.)  This same model correctly classified 2249 of 2500 MNIST digits to
yield a 89.96% recognition rate on a different real-world data set.

![Digits]({{ site.url}}/blog/images/digit_recog/correctly_classified_svhn.png) 
*Fig. 7a.  Representative examples of SVHN digits correctly classified by a
VGG-16 model finetuned on synthetic data.*

![Digits]({{ site.url}}/blog/images/digit_recog/incorrectly_classified_svhn.png) 

*Fig. 7b.  Representative examples of SVHN digits incorrectly classified by
a VGG-16 model finetuned on synthetic data.  The model's inference label
for each chip appears as the last digit in its JPG image filename.*

It is interesting to compare these recognition rates from one
synthetically-trained model with those corresponding to VGG-16 models
finetuned on genuine SVHN and MNIST image chips.  We therefore finetuned
VGG-16 on a set of O(60K) SVHN digits which contained no non-character
chips.  Its recognition performance on the cleaned set of 3000 SVHN chips
which were held-out from training was 96.63%.  But this same model only
correctly classified 59.44% of our 2500 test MNIST digits.  The analogous
results for a VGG-16 model finetuned on 50K genuine MNIST digits are even
more striking.  This last fine-tuned model's 99.33% recognition rate on
held-out MNIST digits is comparable to customized MNIST CNNs' performances.
But it only correctly classified 19.43% of our cleaned 3000 test SVHN
digits!

These comparisons demonstrate an important potential for synthetic data to
yield a **_universal_** text classifier.  Classifiers trained on particular
imagery populations (e.g. SVHN digits, MNIST digits, etc ) will presumably
always yield higher recognition rates at test time on those same
populations than classifiers trained on synthetic data.  But synthetically
trained models offer the hope for recognizing text across all populations
at reasonably high precision and recall.

## Ensemble Averaging

Averaging predictions from multiple trained models has long been known to
improve performance relative to any single classifier [8].  For digit
recognition, we can readily synthesize different sets of training data and
finetune models on each one.  We then perform inference using an ensemble
of models with some method for combining their predictions.  We have
experimented with simple voting, arithmetic and geometric averaging of
probability distributions, and winner-takes-all schemes.  To our surprise,
we empirically found winner-takes-all works best.  In this approach, the
classification label is set based on the highest-valued classification
score from all ensemble model predictions.

Figure 8 tabulates digit recognition performance for ensembles containing 1
through 8 models randomly selected from 12 finetuned VGG-16 models.  For
each test sample in our sets of 3000 cleaned SVHN images and 2500 MNIST
images, we randomly select models without replacement 3 times to form
ensembles of sizes specified in the table's rows.  The total number of
classification errors made on 9000 SVHN and 7500 MNIST tests are presented
in the table's second and fourth columns.  As the number of models within
the ensembles increases from 1 to 8, the recognition performance of our
synthetically trained VGG-16 models increases by 1.3% on both SVHN and
MNIST digit chips.  We did not observe any statistically significant
performance improvement when the number of ensemble models was increased
beyond 8.

|  number   	| SVHN    	| Correct    	| MNIST  	| Correct 	|
|  models  	| errors   	| percentage   	| errors 	| percentage  	|
|:-------------:|:-------------:|:-------------:|:-------------:|:-------------:|
|   1		|  828 		| 90.80  	| 708  		| 90.56	|
|   2		|  771 		| 91.43  	| 664  		| 91.14	|
|   3 		|  752 		| 91.64  	| 644  		| 91.36	|
|   4 		|  729 		| 91.90  	| 641  		| 91.41	|
|   5		|  717 		| 92.03  	| 641		| 91.45	|
|   6		|  723 		| 91.96  	| 644  		| 91.41	|
|   7		|  704 		| 92.17  	| 626  		| 91.65	|
|   8		|  706 		| 92.15  	| 606  		| 91.92	|

*Fig. 8.  Digit recognition performance for ensembles of finetuned VGG-16
models.*

## Future Work

In order to further improve digit recognition via synthetic character
generation, there are several avenues worth exploring in the future.
Firstly, the confusion matrix presented in figure 9 provides some clues.
Each row in the matrix corresponds to some SVHN digit type or
non-character, while its columns show how those types were classified by
the 8-model ensemble.  The final column in figure 9 clearly indicates that
much more work is needed in order to minimize the number of non-character
image chips which are incorrectly classified as digit chips.  Presumably,
the false positive error can be brought down by including more
non-character chips in the training sets.

|       | non	| 0 	| 1	|  2 	|  3	|  4 	|  5 	|  6 	|7	|  8 	| 9 	| # incorrect | % incorrect |
|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----------:|:-----------------:|
|**non**| 262   |  30  	| 15  	|  5  	|  0  	|  4   	| 15  	|  6  	| 2	| 12  	|  18 	| 107         |  29.00 |
|**0**	|  6  	| 780 	|  6  	|  0  	|  0  	|  0   	|  0  	|  0  	| 0	|  0  	|  0  	| 12          | 1.52   |
|**1** 	|  2  	|  0  	| 771 	|  0  	|  0  	|  48  	|  6  	|  0  	| 0	|  0  	|  1	| 57 	      |  6.88  |
|**2**	|  2  	|  0 	|  12	|1325 	|  0  	|  13  	|  0	|  0  	| 0	|  9  	|  25 	| 61          | 4.40 |
|**3**	|  9	|  1  	|  3	|  3  	| 954 	|  7  	| 54	|  20 	| 9	| 15  	|   8  	| 129         | 1.19 |
|**4**  |  6  	|  0  	|  27	|  22 	|  0  	| 629  	|  0	|  3  	| 3	|  2  	|   4  	| 67 	      | 9.63 |
|**5**	|  9  	|  3  	|  0	|  0  	|  0  	|  6   	| 984 	|  30 	| 0	|  0  	|   0  	| 48 	      | 4.65 |
|**6**  |  7  	|  15 	|  0	|  0  	|  3  	|  0   	|  3	| 764 	| 0	|  21 	|   0  	| 49          | 6.03 |
|**7**  |  9 	|  3  	|  93	|  9 	|  3  	|  9   	|  0	|  0	|575 	|  0  	|   4  	| 130 	      | 18.44|
|**8**	|  0  	|  12 	|  0	|  0  	|  3  	|  0   	|  0	|  6  	| 0	| 681 	|   9  	| 30 	      | 4.22 |
|**9**  |  0  	|  6 	|  3	|  0  	|  0  	|  5  	|  0	|  0  	| 0	|  2 	|  569 	| 16          | 2.73 |

*Fig. 9.  Classified SVHN digits' confusion matrix for 8-model ensemble.*

We further observe from the confusion matrix that "7" is the most
problematic digit, for it is frequently misidentified as "1".  Two examples
of such erroneous "7" misclassification are displayed in figure 7b.  In
these cases, the "7" digits belong to numerals which are slanted upwards
rather than aligned with the horizontal axis.  If the numerals' nontrivial
rotation were first detected and removed, we suspect our trained ensemble
would much less frequently misclassify "7" as "1".  So to make further
progress, it makes more sense to investigate detection and extraction of
entire numerals from images in the wild rather than continue concentrating
upon individual digit recognition.  Semantic image segmentation techniques
like those we discussed in our previous blog post potentially offer an
attractive mechanism for detecting text in the wild.

Finally, we have focused upon DNN model finetuning in this blog post
primarily for convenience.  But it would not be surprising if specialized
network architectures were found to yield superior performance compared to
VGG-16 for text recognition applications.  Since arbitrarily large volumes
of synthetic text can be generated, training such specialized networks from
scratch should be straightforward.

We look forward to investigating these interesting avenues for further
research in the future.

## References

1.  Y. LeCun, L. Bottou, Y. Bengio and P. Haffner, *Gradient-based learning
applied to document recognition*, Proc of IEEE (1998).

2.  See
http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html .

3.  See http://ufldl.stanford.edu/housenumbers/ .

4.  Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu and A. Y. Ng,
*Reading digits in natural images with unsupervised feature learning*, NIPS
Workshop on Deep Learning and Unsupervised Feature Learning, (2011).

5.  M. Jaderberg, K. Simonyan, A. Vedaldi and A. Zisserman, *Synthetic data
and artificial neural networks for natiural scene text recognition*, arXiv
1406.2227v4 (2014).

6.  K. Simonyan and A. Zisserman, *Very Deep Convolutional Networks for
Large-Scale Image Recognition*, arXiv 1409.1556, (2015).

7.  See http://caffe.berkeleyvision.orig.

8.  T. G. Dietterich, *Ensemble methods in machine learning,* In Multiple
classifier systems, pages 1â€“15. Springer, (2000).
