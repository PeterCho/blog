---
layout: post
title:  "Improving Flower Image Classification via Conditional GAN Data Augmentation"
date:   2018-05-18 12:00:00
categories: Deep Learning
use_math: true
---

## Flower classification

Deep neural networks employed for supervised learning problems are data
hungry.  They commonly involve millions of parameters whose values must be
fit to training data.  Obtaining high quality labels to feed deep networks
is generally labor-intensive and expensive.  In the absence of vast
quantities of labels, neural nets are prone to overfitting.

Data augmentation represents a standard technique for readily enlarging a
training set's volume [1].  For example, input images may be geometrically
modified via translations, rotations, scalings and flips which leave class
labels invariant.  Alternatively, imagery color contents and/or noise
levels can be altered to some extent.  But these simple augmentations are
constrained by the distribution characteristics of inference imagery which
the training corpus should match.

In this blog posting, we focus upon flower classification as an instructive
laboratory in which to investigate different forms of data augmentation.
We start with an imagery corpus of daisy, dandelion, rose, sunflower and
tulip pictures available from a TensorFlow tutorial [2].  We are primarily
interested in classifying views of individual flowers that fill most of the
image plane.  So photos of flower bunches, flower fields and flower artwork
are discarded from the original corpus.  We next gather additional internet
pictures to form a varied training set containing 800 images of each flower
category.  Moreover, we withold an additional 500+ photos to serve as an
independent test set.  Representative examples of daisy, dandelion, rose,
sunflower and tulip pictures with which we work are presented in figure 1.

![FLOWERCGAN]({{site.url}}/blog/images/flower_cgan/five_flower_classes_montage.png)
*Fig. 1.  Examples of 208 $\times$ 208 images for five classes of flowers.
The different flower types are grouped by column.*

Looking at the flowers in the figure, we observe that gravity's direction
vector often points vertically downward.  Since significant 2D rotations or
horizontal flips would yield pictures with gravity oriented in odd
directions unlikely to be matched in the test set, we avoid these
particular forms of data augmentation.  Similarly, we choose not to perform
color or noise modulations since they would likely disrupt useful features
for distinguishing between the five flower classes.  The only significant
remaining data augmentation is vertical flipping.  We randomly flip every
input training flower image with 50% probability.

The training set of 5 $\times$ 800 = 4000 flower images becomes input to a
Convolutional Neural Network $($CNN$)$ comprised of multiple
"super-layers".  Combinations of convolutions with 5 $\times$ 5 kernels,
batch normalization [3], leaky ReLU activation [4], 2 $\times$ 2 pooling,
Dropout [5], fully connected and softmax layers form useful CNN building
blocks:

$\qquad$ CBLPD: Conv5, Batch norm, Leaky ReLU, Pool, Dropout

$\qquad$ CBLD: Conv5, Batch norm, Leaky ReLU, Dropout

$\qquad$ FBLD: Fully connected, Batch norm, Leaky ReLU, Dropout

$\qquad$ SOFT:  Fully connected, Softmax.

We choose these super-layers' parameters and numbers in a deep network
configuration so that the CNN's receptive field size R=221 [ref] is
comparable to input image 208 $\times$ 208 pixel dimensions.

$\qquad$ 3 $\times$ [CBLPD], 6 $\times$ [CBLD], 2 $\times$ [FBLD], SOFT

We work within the TensorFlow framework [ref] to train this particular
network's 3.2M parameters.  The CNN is subsequently used to perform
inference on the withheld test set containing 81 daisy, 110 dandelion, 120
rose, 83 sunflower and 110 tulip pictures.  Since the networks' weights are
initialized with random values, inference accuracy fluctuates depending
upon training experiment and terminal epoch number.  But on average, the
CNN classifies the five flower types with 87.7% accuracy for the test set.


## Conditional GAN

A number of authors have recently started investigating whether more
substantial data augmentation via Generative Adversarial Network (GAN)
imagery synthesis can quantitatively improve supervised learning.  


We conducted an empirical search over GAN designs and training techniques for
those which optimize bird picture creation.

<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: left">GAN discriminator superlayer</th>
      <th style="text-align: center">Output shape [width $\times$ height $\times$ channels] </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Input image</td>
      <td style="text-align: center">192 $\times$ 192 $\times$ 3</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu, stride 2</td>
      <td style="text-align: center">96 $\times$ 96 $\times$ 32</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu, stride 2</td>
      <td style="text-align: center">48 $\times$ 48 $\times$ 64</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu, stride 2</td>
      <td style="text-align: center">24 $\times$ 24 $\times$ 128</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv3, Pixelwise norm, Leaky Relu, stride 1</td>
      <td style="text-align: center">24 $\times$ 24 $\times$ 256</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv3, Pixelwise norm, Leaky Relu, stride 2</td>
      <td style="text-align: center">12 $\times$ 12 $\times$ 256</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv3, Pixelwise norm, Leaky Relu, stride 1</td>
      <td style="text-align: center">12 $\times$ 12 $\times$ 512</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv3, Pixelwise norm, Leaky Relu, stride 2</td>
      <td style="text-align: center">6 $\times$ 6 $\times$ 512</td>
    </tr>
    <tr>
      <td style="text-align: left">Linear proj</td>
      <td style="text-align: center">$Z_{dim}$</td>
    </tr>
    <tr>
      <td style="text-align: left">Linear proj, Sigmoid</td>
      <td style="text-align: center">1</td>
    </tr>

    <!---  Add empty line in final table row -->

    <tr>
      <td style="text-align: left">$ $</td>
      <td style="text-align: center">$ $</td>
    </tr>
</tbody>
</table>

<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: left">GAN generator superlayer</th>
      <th style="text-align: center">Output shape [width $\times$ height $\times$ channels] </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Input descriptor</td>
      <td style="text-align: center">$Z_{dim}$</td>
    </tr>
    <tr>
      <td style="text-align: left">Linear proj, Pixelwise norm, Leaky Relu</td>
      <td style="text-align: center">6 $\times$ 6 $\times$ 512</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv3, Pixelwise norm, Leaky Relu</td>
      <td style="text-align: center">6 $\times$ 6 $\times$ 256</td>
    </tr>
    <tr>
      <td style="text-align: left">Transposed conv3, Pixelwise norm, Leaky Relu</td>
      <td style="text-align: center">12 $\times$ 12 $\times$ 256</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv3, Pixelwise norm, Leaky Relu</td>
      <td style="text-align: center">12 $\times$ 12 $\times$ 128</td>
    </tr>
    <tr>
      <td style="text-align: left">Transposed conv3, Pixelwise norm, Leaky Relu</td>
      <td style="text-align: center">24 $\times$ 24 $\times$ 128</td>
    </tr>
    <tr>
      <td style="text-align: left">Transposed conv5, Pixelwise norm, Leaky Relu</td>
      <td style="text-align: center">48 $\times$ 48 $\times$ 64</td>
    </tr>
    <tr>
      <td style="text-align: left">Transposed conv5, Pixelwise norm, Leaky Relu</td>
      <td style="text-align: center">96 $\times$ 96 $\times$ 32</td>
    </tr>
    <tr>
      <td style="text-align: left">Transposed conv5, No norm, Tanh</td>
      <td style="text-align: center">192 $\times$ 192 $\times$ 3</td>
    </tr>

    <!---  Add empty line in final table row -->

    <tr>  
      <td style="text-align: left">$ $</td>
      <td style="text-align: center">$ $</td>
    </tr>
</tbody>
</table>

*Table 1.  Discriminator and generator network architectures used to
synthesize 192 $\times$ 192 images.*

![FLOWERCGAN]({{site.url}}/blog/images/flower_cgan/learning_curves.png)
*Fig. 2.  Tensorboard plots of Conditional GAN's discriminator and generator losses as
functions of training iteration.*

<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: left">Hyperparameter description</th>
      <th style="text-align: center">Value    </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Number of training images</td>
      <td style="text-align: center"> 11013 </td>
    </tr>
    <tr>
      <td style="text-align: left">Training image augmentation</td>
      <td style="text-align: center"> Random horizontal flipping </td>
    </tr>
    <tr>
      <td style="text-align: left">Number of training epochs</td>
      <td style="text-align: center"> 500 </td>
    </tr>
    <tr>
      <td style="text-align: left">Minibatch size</td>
      <td style="text-align: center">64</td>
    </tr>
    <tr>
      <td style="text-align: left">Image pixel size</td>
      <td style="text-align: center">192 $\times$ 192</td>
    </tr>
    <tr>
      <td style="text-align: left">Initial blur kernel radius in pixels</td>
      <td style="text-align: center">19.2</td>
    </tr>
    <tr>
      <td style="text-align: left">Adam optimizer $\beta_1$</td>
      <td style="text-align: center">0.0 </td>
    </tr>
    <tr>
      <td style="text-align: left">Adam optimizer $\beta_2$</td>
      <td style="text-align: center">0.99</td>
    </tr>
    <tr>
      <td style="text-align: left">Base learning rate</td>
      <td style="text-align: center">0.0001 </td>
    </tr>
    <tr>
      <td style="text-align: left">Descriptor dimension $Z_{dim}$</td>
      <td style="text-align: center">128 </td>
    </tr>
    <tr>
      <td style="text-align: left">Standard deviation for weight-initializing gaussian</td>
      <td style="text-align: center">0.02 </td>
    </tr>
    <tr>
      <td style="text-align: left">Leaky Relu slope coefficient</td>
      <td style="text-align: center">0.2</td>
    </tr>
  </tbody> 
</table>

*Table 2.  GAN hyperparameters.*

![FLOWERCGAN]({{site.url}}/blog/images/flower_cgan/blur_schedule.png)
*Fig. 3.  Discriminator's blurring kernel radius linearly
decreases from its maximal starting value to zero during the first 250
training epochs.  No low-pass blurring is subsequently performed on input
discriminator images.*

![FLOWERCGAN]({{site.url}}/blog/images/flower_cgan/discrim_gen_io.png)

*Fig. 4. Representative examples of discriminator inputs at equally spaced
intervals during training are pictured within the top row.  Representative
examples of generator outputs conditioned upon different flower classes appear
within the bottom 5 rows.  Columns in this figure correspond to training epochs
0, 170, 340, 510 and 680.*

![FLOWERCGAN]({{site.url}}/blog/images/flower_cgan/gen_1class.png)

*Fig. 5.  Representative examples of discriminator inputs and generator outputs
at equally-spaced training intervals are presented in the top and bottom rows.
No conditioning upon flower classes was performed for this experiment's results.*


![FLOWERCGAN]({{site.url}}/blog/images/flower_cgan/binary_classified_5flowers_montage.png)
*Fig. 6.  Representative examples of conditionally generated flowers with binary
classification softmax scores exceeding 0.99.  Each row in this figure
corresponds to a single descriptor input to the CGAN generator.*

## Data augmentation

Automated systems which make mistakes can be useful if they provide
reasonable confidence measures of their performance.  For our bird image
synthesizer, we explored a few different options for quantitatively
evaluating its renderings.  We began with discriminator probabilities which
the GAN itself outputs.  But surprisingly, no correlation between generator
image quality and corresponding discriminator score was evident.  We next
tried passing generated pictures through a trained autoencoder with the
hope that it could distinguish between highly distorted versus reasonable
bird renderings.  This autoencoder approach to GAN quality assessment also
did not pan out.

![FLOWERCGAN]({{site.url}}/blog/images/flower_cgan/cnn_accur_vs_gan_augmentation.png)
*Fig. 7.  Flower CNN classification accuracy plotted as a function of
Conditional GAN data augmentation.*


## References


1.  [A. Krizhevsky, I. Sutskever and G.E. Hinton, "Imagenet Classification
with Deep Convolutional Neural Networks," NIPS
$\small{(2012)}$.](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)

2. [See TensorFlow tutorial "How to Retrain an Image Classifier for New Categories"](https://www.tensorflow.org/tutorials/image_retraining)

3.  [S. Ioffe and C. Szegedy, "Batch Normalization: Accelerating Deep
Network Training by Reducing Internal Covariate Shift," 32nd ICML 
$\small{(2015)}$.](https://arxiv.org/abs/1502.03167)

4.  [A.L. Maas, A.Y. Hannun and A.Y. Ng, "Rectifier Nonlinearities Improve
Neural Network Acoustic Models", 30th ICML $\small{(2013)}$.](https://web.stanford.edu/~awni/papers/relu_hybrid_icml2013_final.pdf)

5.  [N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever and
R. Salakhutdinov, "Dropout: A Simple Way to Prevent Neural Networks from
Overfitting," Journal of Machine Learning Research {\bf 15} 
$\small{(2014)}$.](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)






2. [I. Goodfellow, J.P.-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
A. Courville and Y. Bengio, "Generative Adversarial Nets," NIPS
$\small{(2014)}$.](https://arxiv.org/pdf/1406.2661.pdf)

6.  [T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford and X.
Chen, "Improved Techniques for Training GANs", NIPS $\small{(2016)}$.](https://arxiv.org/pdf/1606.03498.pdf)

8. [A. Radford, L. Metz and S. Chintala, "Unsupervised Representation
Learning with Deep Convolutional Generative Adversarial Networks," ICLR
$\small{(2016)}$.](https://arxiv.org/pdf/1511.06434.pdf)

9.  [TensorFlow: An open-source software library for Machine Intelligence.](https://www.tensorflow.org/)

14.  [See "Classifying Cat and Dog Images via an Ensemble of CNNs" post in
this blog series $\small{(2017)}$.](https://petercho.github.io/blog//deep/learning/2017/11/25/cvml.html)

*.  [M. Mirza and S. Osindero, "Conditional Generative Adversarial Nets,"
arXiv:1411.1784v1 $\small{(2014)}$.](https://arxiv.org/pdf/1411.1784.pdf)

*.  [I. Goodfellow, "NIPS 2016 Tutorial: Generative Adversarial Networks,"
arXiv:1701.001604v4 $\small{(2017)}$.](https://arxiv.org/pdf/1701.00160.pdf)

*.  [A. Odena, C. Olah and J. Shlens, "Conditional Image Synthesis with
Auxiliary Classifier GANs,"
arXiv:1610.09585v4 $\small{(2017)}$.](https://arxiv.org/pdf/1610.09585.pdf)

*.  [A. Antoniou, A. Storkey and H. Edwards, "Data Augmentation Generative
Adversarial Networks," 
arXiv:1711.043040v3 $\small{(2018)}$.](https://arxiv.org/pdf/1711.04340.pdf)

*.  [G. Mariani, F. Scheidegger, R. Istrate, C. Bekas and C. Malossi,
"BAGAN: Data Augmentation with Balancing GAN,"
arXiv:1803.09655v1 $\small{(2018)}$.](https://arxiv.org/pdf/1803.09655.pdf)

*.  [M. Frid-Adar, E. Klang, M. Amitai, J. Goldberger and H. Greenspan,
"Synthetic Data Augmentation using GAN for Improved Liver Lesion
Classification," arXiv:1801.02385v1
$\small{(2018)}$.](https://arxiv.org/pdf/1801.02385.pdf)
