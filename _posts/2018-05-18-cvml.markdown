---
layout: post
title:  "Improving Flower Image Classification via Conditional GAN Data Augmentation"
date:   2018-05-18 12:00:00
categories: Deep Learning
use_math: true
---

## Flower classification based on genuine imagery

Deep neural networks employed for supervised learning problems are data
hungry.  They commonly involve millions of parameters whose values must be
fit to training data.  Obtaining high quality labels to feed deep networks
is generally labor-intensive and expensive.  In the absence of vast
quantities of labels, neural nets are prone to overfitting.

Data augmentation represents a standard technique for readily enlarging a
training set's volume [1].  For example, input images may be geometrically
modified via translations, rotations, scalings and flips which leave class
labels invariant.  Alternatively, imagery color contents and/or noise
levels can be altered to some extent.  But these simple augmentations are
constrained by the distribution characteristics of inference imagery which
the training corpus should match.

In this blog posting, we focus upon flower classification as an instructive
laboratory in which to investigate different forms of data augmentation.
We start with an imagery corpus of daisy, dandelion, rose, sunflower and
tulip pictures available from a TensorFlow tutorial [2].  We are primarily
interested in classifying views of individual flowers that fill most of the
image plane.  So photos of flower bunches, flower fields and flower artwork
are discarded from the original corpus.  We next gather additional internet
pictures to form a varied training set containing 800 images of each flower
category.  Moreover, we withold an additional 500+ photos to serve as an
independent test set.  Representative examples of daisy, dandelion, rose,
sunflower and tulip pictures with which we work are presented in figure 1.

![FLOWERCGAN]({{site.url}}/blog/images/flower_cgan/five_flower_classes_montage.png)
*Fig. 1.  Examples of 208 $\times$ 208 images for five classes of flowers.
The different flower types are grouped by column.*

Looking at the flowers in the figure, we observe that gravity's direction
vector often points vertically downward.  Since significant 2D rotations or
horizontal flips would yield pictures with gravity oriented in odd
directions unlikely to be matched in the test set, we avoid these
particular forms of data augmentation.  Similarly, we choose not to perform
color or noise modulations since they would likely disrupt useful features
for distinguishing between the five flower classes.  The only significant
remaining data augmentation is vertical flipping.  We randomly flip every
input training flower image with 50% probability.

The training set of 5 $\times$ 800 = 4000 flower images becomes input to a
Convolutional Neural Network $($CNN$)$ comprised of multiple
"super-layers".  Combinations of convolutions with 5 $\times$ 5 kernels,
batch normalization [3], leaky ReLU activation [4], 2 $\times$ 2 pooling,
Dropout [5], fully connected and softmax layers form useful CNN building
blocks:

$\qquad$ CBLPD: Conv5, Batch norm, Leaky ReLU, Pool, Dropout

$\qquad$ CBLD: Conv5, Batch norm, Leaky ReLU, Dropout

$\qquad$ FBLD: Fully connected, Batch norm, Leaky ReLU, Dropout

$\qquad$ SOFT:  Fully connected, Softmax.

We choose these super-layers' parameters and numbers in a deep network
configuration so that the CNN's receptive field size R=221 [ref] is
comparable to input image 208 $\times$ 208 pixel dimensions.

$\qquad$ 3 $\times$ [CBLPD], 6 $\times$ [CBLD], 2 $\times$ [FBLD], SOFT

We work within the TensorFlow framework [6] to train this particular
network's 3.2M parameters.  The CNN is subsequently used to perform
inference on the withheld test set containing 81 daisy, 110 dandelion, 120
rose, 83 sunflower and 110 tulip pictures.  Since the networks' weights are
initialized with random values, inference accuracy fluctuates depending
upon training experiment and terminal epoch number.  But on average, the
CNN classifies the five flower types with 87.7% accuracy for the test set.

## Conditional GAN flower synthesis

Conventional data augmentation yields a relatively limited variety of new
training samples.  As Generative Adversarial Networks $($GANs$)$ have grown
more sophisticated and stable over the past few years, a number of
researchers have recently started investigating whether more substantial
data augmentation can be achieved via GAN imagery synthesis [7-9].  In this
section, we develop a GAN that produces daisy, dandelion, rose, sunflower
and tulip pictures.  In the following section, we assess its utility for
flower classification.

We work with a Conditional GAN $($CGAN$)$ architecture in which class label
metadata is supplied as inputs to both the generator and discriminator
neural networks [10].  Flower labels are first converted into 5-dimensional
one-hot vectors.  For the generator network, the one-hot vector is simply
appended onto the input descriptor vector.  For the discriminator network,
the one-hot vector is promoted to a one-hot set of image planes.  The
flower-class planes are subsequently concatenated with the three RGB planes
for the input image along the channels axis.  As the GAN architecture in
Table 1 illustrates, the conditional metadata flows through both the
generator and discriminator networks:

<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: left">CGAN generator superlayer</th>
      <th style="text-align: center">Output shape [width $\times$ height $\times$ channels] </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Input descriptor + conditional one-hot vector</td>
      <td style="text-align: center">1 $\times$ 1 $\times$ $(Z_{dim}$ + 5$)$</td>
    </tr>
    <tr>
      <td style="text-align: left">Linear proj, Pixelwise norm, Leaky Relu</td>
      <td style="text-align: center">13 $\times$ 13 $\times$ 256</td>
    </tr>

    <tr>
      <td style="text-align: left">Conv3, Pixelwise norm, Leaky Relu</td>
      <td style="text-align: center">13 $\times$ 13 $\times$ 128</td>
    </tr>
    <tr>
      <td style="text-align: left">Upscale, Conv3, Pixelwise norm, Leaky Relu</td>
      <td style="text-align: center">26 $\times$ 26 $\times$ 128</td>
    </tr>

    <tr>
      <td style="text-align: left">Conv3, Pixelwise norm, Leaky Relu</td>
      <td style="text-align: center">26 $\times$ 26 $\times$ 64</td>
    </tr>
    <tr>
      <td style="text-align: left">Upscale, Conv3, Pixelwise norm, Leaky Relu</td>
      <td style="text-align: center">52 $\times$ 52 $\times$ 64</td>
    </tr>

    <tr>
      <td style="text-align: left">Upscale, Conv5, Pixelwise norm, Leaky Relu</td>
      <td style="text-align: center">104 $\times$ 104 $\times$ 32</td>
    </tr>
    <tr>
      <td style="text-align: left">Upscale, Conv5, No norm, Tanh</td>
      <td style="text-align: center">208 $\times$ 208 $\times$ 3</td>
    </tr>

    <!---  Add empty line in final table row -->

    <tr>  
      <td style="text-align: left">$ $</td>
      <td style="text-align: center">$ $</td>
    </tr>
</tbody>
</table>


<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: left">CGAN discriminator superlayer</th>
      <th style="text-align: center">Output shape [width $\times$ height $\times$ channels] </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Input image + conditional image planes</td>
      <td style="text-align: center">208 $\times$ 208 $\times$ $($3 + 5$)$</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu, stride 2</td>
      <td style="text-align: center">104 $\times$ 104 $\times$ 32</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu, stride 2</td>
      <td style="text-align: center">52 $\times$ 52 $\times$ 64</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu, stride 2</td>
      <td style="text-align: center">26 $\times$ 26 $\times$ 128</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv3, Pixelwise norm, Leaky Relu, stride 1</td>
      <td style="text-align: center">26 $\times$ 26 $\times$ 256</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv3, Pixelwise norm, Leaky Relu, stride 2</td>
      <td style="text-align: center">13 $\times$ 13 $\times$ 256</td>
    </tr>
    <tr>
      <td style="text-align: left">Linear proj</td>
      <td style="text-align: center">1 $\times$ 1 $\times$ $Z_{dim}$</td>
    </tr>
    <tr>
      <td style="text-align: left">Linear proj, Sigmoid</td>
      <td style="text-align: center">1 $\times$ 1 $\times$ 1</td>
    </tr>

    <!---  Add empty line in final table row -->

    <tr>
      <td style="text-align: left">$ $</td>
      <td style="text-align: center">$ $</td>
    </tr>
</tbody>
</table>

*Table 1.  Generator and discriminator network architectures used to
synthesize five classes of 208 $\times$ 208 flower images.*

A number of points about this conditional GAN are worth noting:

i.  The generator [discriminator] network contains 6.4M [6.7M] trainable
variables.  The receptive field size equals 61 pixels for both neural
networks.

ii.  During training, we draw input descriptors for the generator network
from a list of descriptors tapped from the discrminator's next-to-last
superlayer.  During inference, we sample input descriptors from a
multivariate gaussian distribution whose mean vector and covariance matrix
are calculated from the list of tapped descriptors.

iii.  We experimented with two different types of "up"-convolution in the
generator network.  We tried transposed convolution as well as upscaling
via nearest neighbor interpolation followed by conventional convolution.
Since the former yielded checkerboard artifacts in synthesized flower
pictures, we adopted the latter approach [11].

iv.  CGAN training depends upon several hyperparameters.
Empirically-derived hyperparameter values are tabulated below:

<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: left">Hyperparameter description</th>
      <th style="text-align: center">Value    </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Number of training images</td>
      <td style="text-align: center"> 8000 </td>
    </tr>
    <tr>
      <td style="text-align: left">Training image augmentation</td>
      <td style="text-align: center"> Random horizontal flipping </td>
    </tr>
    <tr>
      <td style="text-align: left">Number of training epochs</td>
      <td style="text-align: center"> 750 </td>
    </tr>
    <tr>
      <td style="text-align: left">Minibatch size</td>
      <td style="text-align: center">64</td>
    </tr>
    <tr>
      <td style="text-align: left">Image pixel size</td>
      <td style="text-align: center">208 $\times$ 208</td>
    </tr>
    <tr>
      <td style="text-align: left">Initial blur kernel radius in pixels</td>
      <td style="text-align: center">20.8</td>
    </tr>
    <tr>
      <td style="text-align: left">Adam optimizer $\beta_1$</td>
      <td style="text-align: center">0.0 </td>
    </tr>
    <tr>
      <td style="text-align: left">Adam optimizer $\beta_2$</td>
      <td style="text-align: center">0.99</td>
    </tr>
    <tr>
      <td style="text-align: left">Base learning rate</td>
      <td style="text-align: center">0.0001 </td>
    </tr>
    <tr>
      <td style="text-align: left">Descriptor dimension $Z_{dim}$</td>
      <td style="text-align: center">128 </td>
    </tr>
    <tr>
      <td style="text-align: left">Standard deviation for weight-initializing gaussian</td>
      <td style="text-align: center">0.02 </td>
    </tr>
    <tr>
      <td style="text-align: left">Leaky Relu slope coefficient</td>
      <td style="text-align: center">0.2</td>
    </tr>

    <!---  Add empty line in final table row -->

    <tr>  
      <td style="text-align: left">$ $</td>
      <td style="text-align: center">$ $</td>
    </tr>
  </tbody> 
</table>

v.  Pixelwise normalization within generator and discriminator superlayers
significantly improves GAN training stabilization [12].  As figure 2
illustrates, our CGAN's loss curves are well-behaved and do not exhibit
abrupt spikes that often plague GANs with other normalizations.

![FLOWERCGAN]({{site.url}}/blog/images/flower_cgan/learning_curves.png)
*Fig. 2.  Tensorboard plots of generator and discriminator losses as
functions of training iteration.*

GAN generator networks struggle to produce large output images that appear
photo-realistic when trained from scratch.  Researchers have recently found
that training generators to first yield small pictures and then
progressively grow their pixel size and higher spatial frequency contents
provides a much better strategy for synthesizing large images [12, 13].  In
order to generate 208 x 208 images, we follow the gradual resolution
enhancement approach developed in our previous blog posting [14].  Training
inputs to the discriminator are blurred with a gaussian kernel whose radius
initially equals 10% of the images' pixel size.  As the blurring schedule
in figure 3 illustrates, the kernal radius linearly ramps down to zero
during the first third of training.  No low-pass blurring is subsequently
performed on input discriminator images during the last two-thirds of
training.

![FLOWERCGAN]({{site.url}}/blog/images/flower_cgan/blur_schedule.png)
*Fig. 3.  Discriminator's blurring kernel radius linearly decreases from
its maximal starting value to zero during the first 250 epochs of CGAN
training.*

Representative examples of discriminator input images are displayed in
figure 4 at equally spaced epochs during CGAN training.  Corresponding
generator output images are also presented in the figure.  The degree of
focus of the latter lags behind the former since the generator requires
training time to respond to the disciminator.  But after 750 epochs,
synthesized pictures are relatively sharp.  Moreover, we clearly recognize
daisies, dandelions, roses, sunflowers and tulips among the generator
outputs depending upon the discrete conditional one-hot vector generator
input.

![FLOWERCGAN]({{site.url}}/blog/images/flower_cgan/discrim_gen_io.png)

*Fig. 4. [Top row]: Representative examples of discriminator inputs at
equally spaced intervals during training.  [Bottom five rows]:
Representative examples of generator outputs conditioned upon different
flower classes within each row.  Columns in this figure correspond to
training epochs 0, 170, 340, 510 and 680.*

It is interesting to compare these Conditional GAN results with analogous
outputs where no flower class labels are supplied to the GAN.  Our
motivation to do so stems from a remark made by Goodfellow in his 2016 NIPS
tutorial on Generative Adversarial Networks [15]: "Using labels in any way,
shape or form almost always results in a dramatic improvement in the
subjective quality of the samples generated by the model."  To check this
comment, we ran four independent training experiments using the same
network architectures and hyperparameters as described above.  But we
suppressed the conditional metadata inputs to the generator and
discriminator and effectively worked with one combined rather than five
differentiated flower classes.  The learning curves from these four
experiments looked qualitatively similar to those in figure 2.  But
generator outputs from all four experiments exhibited catastrophic mode
collapse after 750 training epochs $($see figure 5$)$.  So these findings
are consistent with Goodfellow's assertion.

![FLOWERCGAN]({{site.url}}/blog/images/flower_cgan/flower_1class.png)

*Fig. 5.  Representative examples of discriminator inputs [top row] and
generator outputs [bottom row] at train epochs 0, 170, 340, 510 and 680.
No conditioning upon flower class was performed for these GAN experiment
results.*

While the CGAN outputs in figure 4 do not suffer from mode collapse, a
sizable fraction are noticeably distorted.  So just as we utilized a
secondary binary classifier to identify badly synthesized bird images in
our previous posting [14], here we similarly classify five flower-class
montages to cull poor CGAN results.  We first combine generated daisy,
dandelion, rose, sunflower and tulip pictures corresponding to a common
$Z_{dim}$ descriptor into a single image.  Next we label 4000 such CGAN
combinations as either acceptable or unacceptable depending upon their
degree of distortion.  Figure 6 displays a few synthesized montages and
their manually assigned labels.  Finally, we use these labels to train a
binary classifier which assigns a softmax score to synthetic flower
montages.

![FLOWERCGAN]({{site.url}}/blog/images/flower_cgan/acceptable_unacceptable.png)

*Fig. 6.  Montages of synthetic daisy, dandelion, rose, sunflower and tulip
pictures manually labeled as acceptable [top 2 rows] and unacceptable
[bottom 2 rows] CGAN outputs.*




![FLOWERCGAN]({{site.url}}/blog/images/flower_cgan/binary_classified_5flowers_montage.png)
*Fig. 7.  Representative examples of conditionally generated flowers with binary
classification softmax scores exceeding 0.99.  Each row in this figure
corresponds to a single descriptor input to the CGAN generator.*



<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: left">Flower class</th>
	<th style="text-align: center">Training images    </th>
	<th style="text-align: center">Synthetic images before binary classification </th>
	<th style="text-align: center">Synthetic images after binary classification </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Daisy</td>
      <td style="text-align: center"> 0.119 </td>
      <td style="text-align: center"> 0.175 </td>
      <td style="text-align: center"> 0.309 </td>
    </tr>
    <tr>
      <td style="text-align: left">Dandelion</td>
      <td style="text-align: center"> 0.141 </td>
      <td style="text-align: center"> 0.211 </td>
      <td style="text-align: center"> 0.339 </td>
    </tr>
    <tr>
      <td style="text-align: left">Rose</td>
      <td style="text-align: center"> 0.132 </td>
      <td style="text-align: center"> 0.178 </td>
      <td style="text-align: center"> 0.293 </td>
    </tr>
    <tr>
      <td style="text-align: left">Sunflower</td>
      <td style="text-align: center"> 0.143 </td>
      <td style="text-align: center"> 0.178 </td>
      <td style="text-align: center"> 0.331 </td>
    </tr>
    <tr>
      <td style="text-align: left">Tulip</td>
      <td style="text-align: center"> 0.119 </td>
      <td style="text-align: center"> 0.169 </td>
      <td style="text-align: center"> 0.270 </td>
    </tr>

    <!---  Add empty line in final table row -->

    <tr>  
      <td style="text-align: left">$ $</td>
      <td style="text-align: center">$ $</td>
    </tr>
  </tbody> 
</table>

*Table 2. Median MS-SSIM scores for 500 random pairs of images per flower class.*





## Flower classification based on genuine plus generated imagery

Automated systems which make mistakes can be useful if they provide
reasonable confidence measures of their performance.  For our bird image
synthesizer, we explored a few different options for quantitatively
evaluating its renderings.  We began with discriminator probabilities which
the GAN itself outputs.  But surprisingly, no correlation between generator
image quality and corresponding discriminator score was evident.  We next
tried passing generated pictures through a trained autoencoder with the
hope that it could distinguish between highly distorted versus reasonable
bird renderings.  This autoencoder approach to GAN quality assessment also
did not pan out.

![FLOWERCGAN]({{site.url}}/blog/images/flower_cgan/cnn_accur_vs_gan_augmentation.png)
*Fig. 8.  Flower CNN classification accuracy plotted as a function of
Conditional GAN data augmentation.*


## References


1.  [A. Krizhevsky, I. Sutskever and G.E. Hinton, "Imagenet Classification
with Deep Convolutional Neural Networks," NIPS
$\small{(2012)}$.](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)

2. [See TensorFlow tutorial "How to Retrain an Image Classifier for New
Categories."](https://www.tensorflow.org/tutorials/image_retraining)

3.  [S. Ioffe and C. Szegedy, "Batch Normalization: Accelerating Deep
Network Training by Reducing Internal Covariate Shift," 32nd ICML 
$\small{(2015)}$.](https://arxiv.org/abs/1502.03167)

4.  [A.L. Maas, A.Y. Hannun and A.Y. Ng, "Rectifier Nonlinearities Improve
Neural Network Acoustic Models", 30th ICML $\small{(2013)}$.](https://web.stanford.edu/~awni/papers/relu_hybrid_icml2013_final.pdf)

5.  [N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever and
R. Salakhutdinov, "Dropout: A Simple Way to Prevent Neural Networks from
Overfitting," Journal of Machine Learning Research $\bf {15}$
$\small{(2014)}$.](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)

6.  [TensorFlow: An open-source software library for Machine Intelligence.](https://www.tensorflow.org/)

7.  [A. Antoniou, A. Storkey and H. Edwards, "Data Augmentation Generative
Adversarial Networks," 
arXiv:1711.043040v3 $\small{(2018)}$.](https://arxiv.org/pdf/1711.04340.pdf)

8.  [G. Mariani, F. Scheidegger, R. Istrate, C. Bekas and C. Malossi,
"BAGAN: Data Augmentation with Balancing GAN,"
arXiv:1803.09655v1 $\small{(2018)}$.](https://arxiv.org/pdf/1803.09655.pdf)

9.  [M. Frid-Adar, E. Klang, M. Amitai, J. Goldberger and H. Greenspan,
"Synthetic Data Augmentation using GAN for Improved Liver Lesion
Classification," arXiv:1801.02385v1
$\small{(2018)}$.](https://arxiv.org/pdf/1801.02385.pdf)

10.  [M. Mirza and S. Osindero, "Conditional Generative Adversarial Nets,"
arXiv:1411.1784v1 $\small{(2014)}$.](https://arxiv.org/pdf/1411.1784.pdf)

11. [A. Odena, V. Dumoulin and C. Olah, "Deconvolution and Checkerboard
Artifacts," Distill
$\small{(2016)}$.](https://distill.pub/2016/deconv-checkerboard/)

12. [T. Karras, T. Aila, S. Laine and J. Lehtinen, "Progressive Growing of
GANs for Improved Quality, Stability and Variation", ICLR $\small{(2018)}$.](https://arxiv.org/pdf/1710.10196.pdf)

13. [H. Zhang, T. Xu, H. Li, S. Zhang, X. Wang, X. Huang and D. Metaxas,
"StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative
Adversarial Networks," arXiv:1612.03242v2
$\small{(2017)}$.](https://arxiv.org/pdf/1612.03242.pdf)

14.  [See "Synthesizing Bird Images via Gradual Resolution Enhancement
during GAN Training" post in this blog series
$\small{(2018)}$.](https://petercho.github.io/blog//deep/learning/2018/04/21/cvml.html)

15.  [I. Goodfellow, "NIPS 2016 Tutorial: Generative Adversarial Networks,"
arXiv:1701.001604v4 $\small{(2017)}$.](https://arxiv.org/pdf/1701.00160.pdf)



*.  [A. Odena, C. Olah and J. Shlens, "Conditional Image Synthesis with
Auxiliary Classifier GANs,"
arXiv:1610.09585v4 $\small{(2017)}$.](https://arxiv.org/pdf/1610.09585.pdf)




2. [I. Goodfellow, J.P.-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
A. Courville and Y. Bengio, "Generative Adversarial Nets," NIPS
$\small{(2014)}$.](https://arxiv.org/pdf/1406.2661.pdf)

6.  [T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford and X.
Chen, "Improved Techniques for Training GANs", NIPS $\small{(2016)}$.](https://arxiv.org/pdf/1606.03498.pdf)

8. [A. Radford, L. Metz and S. Chintala, "Unsupervised Representation
Learning with Deep Convolutional Generative Adversarial Networks," ICLR
$\small{(2016)}$.](https://arxiv.org/pdf/1511.06434.pdf)


14.  [See "Classifying Cat and Dog Images via an Ensemble of CNNs" post in
this blog series $\small{(2017)}$.](https://petercho.github.io/blog//deep/learning/2017/11/25/cvml.html)
