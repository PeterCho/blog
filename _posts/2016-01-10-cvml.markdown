---
layout: post
title:  "Detecting Faces via Deep Semantic Segmentation of Images"
date:   2016-01-10 12:00:0
categories: Deep Learning
---

Face detection is an important problem in computer vision which has been
studied for decades [1 - 4].  In the early 2000s, most methods for
detecting faces generally involved learning templates which could locate
gross facial features such as eyes, noses and mouths.  Template-based
approaches work well on test images containing unobstructed faces of
comparable scale and orientation as those used for training the filters.
But such shallow-learning methods suffer from requiring different facial
templates to be generated at many different poses.

Recent deep learning approaches to face detection can handle a wider
variety of facial poses and expressions as well as tolerate occlusion.  In
this blog entry, we investigate semantic segmentation of images via a deep
convolutional network into facial foreground and non-facial background
classes.  Though our initial face segmentation results are far from
flawless, we believe they demonstrate significant promise.

After briefly surveying publically available imagery datasets on the web
[5], we decided to gather our own "faces-in-the-wild" collection which
exhibits significant variation along multiple dimensions.  To date, we have
quasi-intentionally downloaded 3K+ internet photos containing multiple
different faces.  This set includes people with varying attributes
(e.g. gender, age, ethnicity) performing different activities
(e.g. talking, drinking, sleeping) in disparate settings (e.g. sports
stadiums, rural villages, dance halls).  We have also included faces in
many different poses (e.g. looking upwards, sideways, away from camera) as
well as occluding objects (e.g. hats, sunglasses, hands).  Some historical
pictures were intentionally incorporated along with modern black-and-white
photos.  Finally, we made sure to include several scenes containing crowds
of people whose faces range across multiple scales.

After assembling our "faces-in-the-wild" imagery corpus, we drew bounding
boxes around every face we could identify using Davis King's nice labeling
tool [6].  For faces oriented towards the camera, we usually extended the
bounding box from below the hairline and to above the chin.  For faces in
profile, we typically drew a box from the person's nose to ear.  And for
faces oriented away from the camera, we tried to select bounding boxes
which captured the front part of a person's head but not the rear.
Representative examples of our training images and bounding box labels are
illustrated in figure 1.

![Faces]({{ site.url}}/blog/images/face_segmentation/training/training_faces.png) 
*Fig. 1.  Representative examples of labeled training images containing various
different face types.*

In order to perform semantic image segmentation, we work with the DeepLab
framework which is a deep learning system built on top of Caffe [7 - 9].
DeepLab combines deep neural network (DNN) responses with densely connected
conditional random fields (CRF).  As we seek only to perform gross face
detection, we choose to not employ DeepLab's CRF functionality which is
computationally expensive.

For DNN training purposes, we first decompose our annotated images into a
set of square tiles with 321x321 pixel dimensions.  Border regions are
padded as necessary with zero values so that an integer number of tiles are
created for each image.  Along with each tile, a mask is exported to an
8-bit greyscale PNG file.  Mask pixels are set to either 0 (background) or
1 (foreground) values depending upon whether they lie inside or outside
face annotation bounding boxes.  The image tiles become inputs to Deeplab's
network, while the masks become inputs to Deeplab's loss function.  We also
supply DeepLab with a set of randomly-shuffled associations between image
tiles and masks.  80% of the tile and mask pairs are devoted to network
training, while 20% are reserved for validation.

During training, DeepLab fine-tunes the weight and bias parameters of a
pre-trained DNN which is a variant of the VGG-16 model [10].  We adopt a
batch size of 30 which is the maximum value our hardware memory capacity
can accomodate given our input tile size.  The only training hyperparameter
which we attempted to adjust using training and validation data is the
learning rate.  We did not observe any dramatic change in validation
accuracy metrics as we varied the learning rate parameter from 0.00033 to
0.016 and trained on a subset of input image tiles.  We therefore
conservatively set the base learning rate to 0.0005.  Otherwise, we work
with default DeepLab hyperparameters such as 0.9 momentum and 0.0005 weight
decay values along with a step learning rate policy [8].

The loss function quantifying the discrepancy between our weakly-supervised
bounding box inputs and the network's background/foreground segmentation
outputs is displayed  over 20 training epochs in figure 2a in orange:

![Faces]({{ site.url }}/blog/images/face_segmentation/training/training_performance.jpg)
*Fig. 2a.  Pixel-level performance curves and loss function for training
set of image tiles and masks.*

As the network trains, DeepLab periodically reports pixel-level
segmentation accuracy values for 3 different metrics.  The red, blue and
green curves in figures 2a and 2b illustrate these metrics for our training
and validation data sets.  After 20 epochs, the validation values for the 3
metric values are somewhat smaller than the corresponding training values.
So DeepLab/caffe's use of dropout appears to successfully prevent network
overfitting [11].  We also observe that the validation value for average
class recall is less than one.  But given that we annotated faces using
bounding boxes rather than snug-fitting polygons, we should not expect
pixel-level recall to approach unity.  If human faces were exactly modeled
by ellipses inside each of our rectangular bounding boxes, recall would
equal PI/4 = 0.785. It's amusing to note that the O(0.9) pixel-level recall
value observed in figure 2b lies halfway between unity and the ellipse
approximation.

![Faces]({{ site.url }}/blog/images/face_segmentation/training/validation_performance.jpg)
*Fig. 2b.  Pixel-level performance curves for validation set of image tiles
and masks.*

After fine-tuning asymptotically finishes, we apply the network to an
independent set of internet photos.  Each test image is again decomposed
into tiles.  But by adopting a test batch size of 1, we can increase test
tile pixel size to 1200 without running out of memory.  After each tile
belonging to a particular image is passed through the trained DNN, we
combine their segmentation results into a single mosaic.  The input test
image may then be directly compared with its output faces mask.

We begin a qualitative comparison in figure 3a with a simple example of
unoccluded, well-separated and conventionally oriented faces which all look
straight towards the camera.  The trained network readily segments all 4
faces in this case.  The faces in figure 3b are also generally directed
towards the camera albeit at various different roll angles.  The corresponding
segmentation mask again looks quite good.

![Faces]({{ site.url }}/blog/images/face_segmentation/good_easy/four_men_standing_montage.jpg)
*Fig. 3a.  Segmentation results for 4 faces oriented straight towards the camera.*

![Faces]({{ site.url }}/blog/images/face_segmentation/good_easy/ailey_dancers_montage.jpg)
*Fig. 3b.  Segmentation results for 10 faces oriented towards the camera
at various roll angles.*

Figures 4a - 4d present significantly more challenging tests for face
detection.  They include faces viewed primarily in profile, faces which are
partially occluded or clipped, small faces within a large crowd, and crowd
faces exhibiting significant variation in expression and pose.  While some
detection mistakes can be seen in these figures, the overall performance of
deep semantic segmentation for these examples is qualitatively superior to
results for similar images which we've generated in the past via HOG
template matching [12].

![Faces]({{ site.url }}/blog/images/face_segmentation/good_challenging/runners_montage.jpg)
*Fig. 4a.  Segmentation results for 7 faces not generally  pointing towards
the camera.*

![Faces]({{ site.url }}/blog/images/face_segmentation/good_challenging/beer_drinkers_montage.jpg)
*Fig. 4b.  Segmentation results for 4 faces which are partially occluded or clipped.*

![Faces]({{ site.url }}/blog/images/face_segmentation/good_challenging/commuter_crowd_montage.jpg)
*Fig. 4c.  Segmentation results for many forward-oriented and nearly
overlapping faces in a crowd.*

![Faces]({{ site.url }}/blog/images/face_segmentation/good_challenging/baseball_spectators_montage.jpg)
*Fig. 4d.  Segmentation results for crowd faces exhibiting a wide variety of
expressions and poses.*

Testing reveals that our trained network sometimes makes serious errors.
Figures 5a - 5c illustrate representative failure modes.  They include
segmenting only small parts of faces, missing entire faces, merging two or
more faces into a single segmentation region, and false alarming on
non-face pixel regions.  In the future, we plan to augment our training
data with hard-negative examples in order to reduce false positives.  And
we will add more genuine face examples in order to minimize false
negatives.  It will also be interesting to experiment with different DNN
training schemes beyond the vanilla Stochastic Gradient Descent which we've
employed so far to see if any might yield nontrivial classifier
improvement.  But we'll first need to quantify face detection precision and
recall at the level of objects rather than pixels.  All of these tasks
remain for future work.

![Faces]({{ site.url }}/blog/images/face_segmentation/problems/circle_partial_faces_montage.jpg)
*Fig. 5a.  Problematic segmentation results exhibiting relatively small
overlap with genuine face content.*

![Faces]({{ site.url }}/blog/images/face_segmentation/problems/missed_guys_montage.jpg)
*Fig. 5b.  Problematic segmentation results exhibiting completely missed
faces as well as merging of individual face regions.*

![Faces]({{ site.url }}/blog/images/face_segmentation/problems/false_positives_montage.jpg)
*Fig. 5c.  Problematic segmentation results exhibiting significant false
positive content.*

Before closing, we mention one more interesting capability of semantic
segmentation for face detection.  After seeing the result in figure 4b
where an occluded man's face was partially detected, we became curious
about how much occlusion our trained DNN could tolerate.  So we started by
segmenting the full face in figure 6a that looks straight into the camera.
Reassuringly, the output mask covers the entire face.  We then
progressively removed quarters of the face in both the vertical and
horizontal directions and passed the cropped images into the classifier.
As figures 6b and 6c demonstrate, semantic segmentation can successfully
detect a face even when 75% of it is absent from a picture.  We believe
this example illustrates a major advantage of pixel-level classification
over template-based approaches.

![Faces]({{ site.url }}/blog/images/face_segmentation/single_face/padded_segmented_face.jpg)
*Fig. 6a.  Segmentation result for a canonicially-oriented full face.*

![Faces]({{ site.url }}/blog/images/face_segmentation/single_face/horiz_quarters_montage.jpg)
*Fig. 6b.  Segmentation results for one, two and three quarter 
crops along the horizontal axis of the face in figure 6a.*

![Faces]({{ site.url }}/blog/images/face_segmentation/single_face/vert_quarters_montage.jpg)
*Fig. 6c.  Segmentation result for one, two and three quarter 
cropss along the vertical axis of the face in figure 6a.*


## References

1.  M. A. Turk and A. P. Pentland, *Face recognition using eigenfaces*, CVPR, (1991).

2.  P. Viola and M. Jones, *Robust Real-time Object Detection*, IJCV, (2001).

3.  P. F. Felzenszwalb and D. P. Huttenlocher, *Pictorial structures for object recognition*, IJCV (2005).

4.  S. Zafeiriou, C. Zhang and Z. Zhang, *A survey on face detection in the wild: past, present
and future*,  Computer Vision and Image Understanding, (2015).

5.  See https://facedetection.com/datasets/.

6. D. King, *dlib C++ library*, http://dlib.net/ml.html.

7. L.-C. Chen, G. Papandreou, I Kokkinos, K. Murphy and A. L. Yuille,
*Semantic image segmentation with deep convolutional nets and fully
connected CRFs*, http://arxiv.org/abs/1412.7062, (2015).

8.  G. Papandreou, L.-C. Chen, K. Murphy and A. L. Yuille,
*Weakly and semi-supervised learning of a DCNN for semantic image
segmentation*, http://arxiv.org/1502.02734, (2015).

9.  See https://bitbucket.org/deeplab/deeplab-public/

10.  K. Simonyan and A. Zisserman, *Very Deep Convolutional Networks for
Large-Scale Image Recognition*, arXiv: 1409.1556, (2015).

11.  N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever and
R. Salakhutdinov, *Dropout: A simply way to prevent neural networks from
overfitting"*, Journal of Machine Learning Research 15 (2014), 1929-1958.

12.  See http://blog.dlib.net/2014/02/dlib-186-released-make-your-own-object.html.
