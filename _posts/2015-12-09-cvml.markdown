---
layout: post
title:  "Constructing Image Graphs via Deep Learning Feature Descriptors"
date:   2015-12-09 7:51:0
categories: Deep Learning
---

I have been interested in organizing large sets of unstructured images for
many years.  I regard this problem as akin to spreading out a large set of
photos on table like playing cards.  Then how should the photos be arranged
so that they turn from a random mess into an organized collection which can
be readily searched and extended?

One approach to solving this jigsaw puzzle is to build a graph from the
input images.  Each node in the graph corresponds to some image.  Photo
pairs are connected by an edge whose weight value indicates their
similarity.  An edge with a very large weight links two images which are
nearly identical.  On the other hand, two images connected by an edge with
almost zero weight are totally different.  So given two images, we need a
way to compute their similarity.

## Local SIFT Feature Matching

Several years ago, I followed standard practices within computer vision at
the time for extracting and matching SIFT features to assess image overlap
[1].  Applying this approach 10 "Kermit" photos (taken from one of Noah
Snavely's bundler examples [2]) yields the image network pictured below:

![Kermit SIFT graph]({{ site.url }}/blog/images/kermit_graph.png) 
*Fig. 1.  Image graph generated via SIFT feature matching for 10 kermit 
photos.*


The edge weights in this figure correspond to numbers of matched SIFT
features between pairs of images.  Hot-colored edges indicate greater
numbers of local feature matches than cool-colored edges, and edges with
weights less than a cutoff threshold have been suppressed.  Though the
static world scene is the same in all 10 Kermit pictures, SIFT feature
matching works more robustly for pairs of photos shot at similar
perspectives.  As extrinsic camera parameters for a pair of images diverge,
the number of SIFT features that can be automatically matched between the
two pictures typically decreases.

Image graph construction based upon SIFT feature matching works well
provided there's reasonably dense coverage of the same physical objects
within the input imagery set.  SIFT matching generally handles rigid
objects in world-space better than deformable ones.  Localized features are
also more abundant in highly-textured outdoor scenes than indoor settings.
And SIFT matching generally requires similar illumination conditions and
sensing frequencies.  If these conditions are satisified, large and
interesting networks of images can be formed [3].  But if we want to relax
any of these restrictions on matching photos, we need to use a more
powerful method for assessing image similarity than simply counting numbers
of local SIFT feature pairs.

## Global CNN Feature Matching

A newly popular way to generate descriptors for entire images is based upon
Convolutional Neural Networks [4,5].  We apply this approach to image graph
generation by first downsizing each image within an input set so that its
minimum pixel dimension equals 230.  Working with the Caffe deep learning
framework [6,7], we then pass the downsized picture into the VGG-16 model
[8].  Network values at its next-to-last "fc7" layer form a global image
descriptor.  We rescale the resulting 4096-dimensional descriptor so that
it has unit norm.

The dotproduct between two photos' global descriptors provides a convenient
measure of their semantic similarity.  We ignore any dotproduct value less
than 0.7.  For building large image networks, we must also cut off the
quadratic growth in number of graph edges as a function of graph nodes.  So
given a node, we retain all its edges if their number is less than 10.  But
for a node with more than 10 edges, we retain its top 7 edges with greatest
weights.  In order to encourage matching of images with significantly
different pixel yet similar semantic contents, the last 3 edges for the
node are randomly chosen from among its remaining candidates.

We have applied this image similarity metric based upon deep learning
feature descriptors to several different data sets.  The resulting image
graphs are astonishing!  For example, a network formed from 30K+ photos
shot semi-cooperatively around MIT in summer 2008 is illustrated in figure
2:

<a href="http://www.youtube.com/watch?feature=player_embedded&v=crWc7GS9c3M
" target="_blank"><img src="http://img.youtube.com/vi/crWc7GS9c3M/0.jpg"
alt="IMAGE ALT TEXT HERE" width="720" height="480" border="10" /></a>
*Fig. 2.  Image graph for 30K+ photos shot around MIT.  Click on image
above in order to play the YouTube video.*

In the figure's video clip, we see thousands of photos shot in an urban
setting over a few days.  As the movie plays, pop-up YouTube annotations
point out various interesting aspects of the image network as well as some
mechanics of the synchronized web browser and graph viewer [2].  Note that
deep learning descriptor matching naturally clusters different outdoor
views of river fronts, city skylines, particular building facades plus
trees and vegetation.  More remarkably, global image matching groups
together indoor views.  In contrast, local feature matching generally fails
for pictures containing significant interior wall content due to their lack
of texture.  Deep CNN features also link photos shot on the ground with
aerial video frames.

The MIT photo set was semi-intentionally collected for 3D reconstruction
purposes.  A small band of photographers were instructed to rapidly gather
imagery from multiple views with essentially constant zoom settings.  It's
consequently not surprising that the MIT image graph looks dense.  A much
more challenging imagery set was semi-cooperatively collected over a span
of years around an outdoor nature conservation area in eastern
Massachusetts called Tidmarsh Farms [9].  Its photo network is displayed in
figure 3.

<a href="http://www.youtube.com/watch?feature=player_embedded&v=FnEmbDT5kyM
" target="_blank"><img src="http://img.youtube.com/vi/FnEmbDT5kyM/0.jpg" 
alt="IMAGE ALT TEXT HERE" width="720" height="480" border="10" /></a>
*Fig. 3.  Image graph for 7K+ outdoor nature photos.  Click on image
above in order to play video*

At the start of the figure's video, we observe that the image graph breaks
apart into multiple connected components.  The number of different
components and number of nodes within each component depends upon the edge
weight threshold for the entire network which we again set to 0.7.  After
the virtual camera zooms into the first connected component, we see
pictures of individual flowers appearing within one particular cluster of
graph nodes.  SIFT feature matching cannot match images of flowers that
represent different physical instances of different types.  But CNN
descriptors for these images sufficiently overlap that different examples
of flowers are semantically linked within the deep learning graph.

Colored subgroups of nodes generally represent semantically distinct
clusters.  For example, we observe many different views of streams and
marshes in another part of the image graph's first connected component.  As
the user clicks on nodes in the graph viewer, their corresponding images
are displayed in the web browser's main window.  The thumbnail carousel
located below the browser's main window displays the current node's nearest
neighbors within the image graph.  Carousel thumbnails are ordered
according to their CNN feature descriptors' dotproducts with the current
node's global feature descriptor.  So each carousel update effectively
represents the response to a search query for the image appearing in the
browser's primary window.

Towards the end of the video clip, the graph viewer is translated and its
thumbnails are enlarged so that pictures belonging to different connected
components in the image graph can be seen.  We observe that each connected
component contains semantically-distinct photos that share little
similarity with other components' images.

Unlike the first MIT photo set which was collected over a relatively small
area in a few days, the Tidmarsh Farms pictures were taken over a much
larger region of space and time.  But both data sets were gathered
semi-intentionally and semi-cooperatively for scientific documentation
purposes.  In contrast, pictures harvested from the internet exhibit no *a
priori* coordination in their collection.  Yet image graphs based upon
global CNN feature matching do a surprisingly good job of organizing large
numbers of uncooperatively-collected pictures.

Figure 4 illustrates the image graph constructed from 14K+ photos of the
Grand Canyon downloaded from flickr:

<a href="http://www.youtube.com/watch?feature=player_embedded&v=li9ANCM7PNM
" target="_blank"><img src="http://img.youtube.com/vi/li9ANCM7PNM/0.jpg"
alt="IMAGE ALT TEXT HERE" width="720" height="480" border="10" /></a>
*Fig. 4.  Image graph for 14K+ Grand Canyon flickr photos.  Click on
image above in order to play video.*

After the thick-client viewer zooms into the image graph's first component,
many wide-angle shots of the Grand Canyon taken by different people at
different times are seen clustered together.  As the user clicks on nodes
in the graph, their associated images are displayed in the browser's main
window, and thumbnails for their nearest neighbors are rank-ordered within
the carousel.

Approximately 1.5 minutes into the video clip, a YouTube annotation
proclaims that the currently selected ground photo has an aerial video
frame as one of its nearest neighbors.  Our Grand Canyon data set contains
a few thousand video frames gathered by a FLIR camera over the Grand Canyon
in addition to many thousands of flickr ground photos [2].  Because the
Grand Canyon is so vast, the angular discrepancy between views of distant
points seen from the rim and from 10,000 feet in the air can be 35 degrees
or less.  So local SIFT feature matching can barely succeed to link one
ground photo with one airborne frame [2].  But many more connections
between the FLIR and ground cameras' outputs are established by matching
global CNN features.  Indeed, the movie displays multiple links between one
gold-colored cluster of nodes corresponding mostly to ground photos and an
adjacent purple-colored node cluster containing mostly aerial video frames.

The most intriguing connections within the Grand Canyon image graph are
revealed near the end of the movie in figure 4.  The aerial FLIR camera
collected video frames in both the visible frequency band and mid-wave
infrared (wavelength = 3 - 5 microns).  SIFT features in the infrared (IR)
differ significantly from their electro-optical (EO) counterparts.  So no
matches between Grand Canyon images in the IR and EO bands can be
established via local SIFT matching [2].  But as figure 4 illustrates, CNN
feature matching successfully links IR and EO views.  I believe this
encouraging example motivates future research into multi-spectral imagery
search and fusion.

The final organization problem we consider is based upon our most disparate
data set consisting of 30K+ photos of India downloaded from flickr.  All of
its images were uncooperatively gathered, and we have no obviously useful
*a priori* metadata for any of these internet pictures.  The network
generated by matching CNN features is displayed in figure 5.

<a href="http://www.youtube.com/watch?feature=player_embedded&v=2spSVHPprPs
" target="_blank"><img src="http://img.youtube.com/vi/2spSVHPprPs/0.jpg" 
alt="IMAGE ALT TEXT HERE" width="720" height="480" border="10" /></a>
*Fig. 5.  Image graph for 30K+ India flickr photos.  Click on
image above in order to play video.*

At the start of this figure's video, we again observe the image graph's
decomposition into multiple connected components which is unsurprising
given that its photos were uncooperatively-collected.  The initial picture
appearing in the web browser's main window is of a fireworks explosion
against black sky.  Carousel thumbnails for its closest image graph
neighbors are differently colored and shaped firecracker bursts.  Though we
have not explicitly attempted matching SIFT features on such firecracker
pictures, it's very unlikely local features could connect images with
qualitiately different local features yet similar semantic contents.  We
also observe that one firecracker explosion picture has a photo of an
isolated flower among its nearest neighbors.  Given that the appearance of
a firecracker explosion is similar to a flower, such linking between two
different semantic categories is not unreasonable.  On the other hand,
nodes corresponding to firecracker pictures appear fairly close within the
graph viewer to nodes for very different scenes such as sunsets and oceans.
We believe that such incongruous problems from graph layout rather than
graph formation problems.  Improving graph layout is left for future work.

In other parts of the image graph's first connected component, we observe
good organization of photos containing various groups of people.
Similarly, random pictures of ground transportation (e.g. motorbikes, cars,
trucks) are compellingly clustered together.  Other connected components
beyond the first in the India image graph contain views of qualitatively
different subjects (e.g.  monkeys, food dishes, and artistic smoke swirls).
So the deep learning approach to imagery organization can indeed
reconstruct partial yet sensible "jigsaw puzzles" from large input sets of
seemingly random input pictures.


## References

1.  D.G. Lowe, *Distinctive image features from scale-invariant keypoints*,
International Journal of Computer Vision (2004), 91-110.

2. See www.cs.cornell.edu/~snavely/bundler/ .

3.  P. Cho and M. Yee, *Image Search System*, 41st AIPR workshop, 2012.

4.  P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,
and Y. LeCun. *Overfeat: Integrated recognition, localization
and detection using convolutional networks*, ICLR, 2014.

5.  A.S. Razavian, H. Azizpour, J. Sullivan and S. Carlsson, *CNN features
off-the-shelf: an astounding baseline for recognition*, arXiv 1403.6382v3,
2014.

6.  Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,
S. Guadarrama and T. Darrell, *Caffe: Convolutional Architecture for Fast
Feature Embedding*, arXiv:1408.5093, 2014.

7.  See http://caffe.berkeleyvision.org.

8.  K. Simonyan and A. Zisserman, *Very Deep Convolutional Networks for
Large-Scale Image Recognition*, arXiv: 1409.1556, 2015.

9. See www.tidmarshfarms.com.

{% comment %}
{% endcomment %}
