---
layout: post
title:  "Gradually Increasing Image Resolution During GAN Training"
date:   2018-04-21 12:00:00
categories: Deep Learning
use_math: true
---

## Progressive GAN training

In October 2017, Karras, Aila, Laine and Lehtinen published a paper
entitled "Progressive Growing of GANS for Improved Quality, Stability and
Variation" which presented new high-resolution results for synthetically
generated images [1].  These authors described a novel training methodology
for generative adversarial networks [2].  It begins by training shallow
discriminator and generator networks on low resolution imagery inputs.  New
layers in both networks are subsequently introduced and trained in order to
model higher resolution details.  Working with an enhanced version of the
popular CelebA dataset [3], Karras *et al.* taught their "Progressive GAN"
$\small{\rm (PGGAN)}$ to output 1024 $\times$ 1024 facial images.  Their
stunning synthesized faces were soon featured in the New York Times [4].

Inspired by ref. [1], we explore in this blog posting an alternative
approach to increasing image resolution during GAN training.  Unlike Karras
*et al.*, we keep the discriminator and generator network architectures
fixed throughout training.  But the resolution of images fed into the
discriminator gradually increases as training proceeds.  So our GAN
initially learns gross object shapes, colors and textures from
intentionally blurred input imagery.  It later refines object parts and
details as low-pass filtering is diminished and training proceeds to
completion.

In Appendix G of ref. [1], generated 256 $\times$ 256 views are displayed
of other objects besides faces based upon 30 LSUN categories [5].  Karras
*et al.* trained separate networks for each object class using at least
100K images.  Though the overall quality of the synthetic pictures is
impressive, several exhibit obvious flaws.  Distortions within generated
animal images are particularly noticeable.  Other investigators have
previously noted that GANs often fail to reproduce basic animal anatomy
[6].

Motivated by this animal synthesis challenge, we focus upon bird rather
than face imagery in our investigation.  We work with a 2011 Caltech-UCSD
dataset containing 11K+ bird images [7].  As the representative examples in
figure 1 illustrate, bird pictures from this corpus exhibit much greater
variety in 3D pose, foreground color and background content than do
pre-aligned CelebA photos.  As we shall demonstrate, our simple approach to
gradual GAN training yields synthetic bird images that qualitatively appear
at least as good as those in ref. [1] generated with 9X more input data.

![BIRDGAN]({{site.url}}/blog/images/bird_dcgan/input_bird_chips.png)
*Fig. 1.  Tensorboard display of sample images from the
2011 Caltech-UCSD birds dataset resized to 192 $\times$ 192 pixels.*

## Bird imagery synthesis

We conducted an empirical search for GAN designs and training techniques
which yield qualitatively good synthetic bird pictures.  Our initial
attempts started with the DCGAN architecture of Radford *et al.* that
outputs 64 $\times$ 64 images [8].  Over the course of O$($100$)$
experiments performed within the Tensorflow deep learning framework [9],
our neural networks' forms and training schedules evolved towards more
stable configurations which could accommodate images with larger pixel
sizes.  Table 1 lists the best "super layer" combinations of linear
projections, convolutions, transposed convolutions, normalizations and
nonlinearities we found for a GAN that synthesizes 192 $\times$ 192 images.

<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: left">GAN discriminator super layer</th>
      <th style="text-align: center">Output shape [width $\times$ height $\times$ channels] </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Input image</td>
      <td style="text-align: center">192 $\times$ 192 $\times$ 3</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu, stride 2</td>
      <td style="text-align: center">96 $\times$ 96 $\times$ 32</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu, stride 2</td>
      <td style="text-align: center">48 $\times$ 48 $\times$ 64</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu, stride 2</td>
      <td style="text-align: center">24 $\times$ 24 $\times$ 128</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv3, Pixelwise norm, Leaky Relu, stride 1</td>
      <td style="text-align: center">24 $\times$ 24 $\times$ 256</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv3, Pixelwise norm, Leaky Relu, stride 2</td>
      <td style="text-align: center">12 $\times$ 12 $\times$ 256</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv3, Pixelwise norm, Leaky Relu, stride 1</td>
      <td style="text-align: center">12 $\times$ 12 $\times$ 512</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv3, Pixelwise norm, Leaky Relu, stride 2</td>
      <td style="text-align: center">6 $\times$ 6 $\times$ 512</td>
    </tr>
    <tr>
      <td style="text-align: left">Linear proj</td>
      <td style="text-align: center">$Z_{dim}$</td>
    </tr>
    <tr>
      <td style="text-align: left">Linear proj, Sigmoid</td>
      <td style="text-align: center">1</td>
    </tr>

    <!---  Add empty line in final table row -->

    <tr>
      <td style="text-align: left">$ $</td>
      <td style="text-align: center">$ $</td>
    </tr>
</tbody>
</table>

<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: left">GAN generator super layer</th>
      <th style="text-align: center">Output shape [width $\times$ height $\times$ channels] </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Input descriptor</td>
      <td style="text-align: center">$Z_{dim}$</td>
    </tr>
    <tr>
      <td style="text-align: left">Linear proj, Pixelwise norm, Leaky Relu</td>
      <td style="text-align: center">6 $\times$ 6 $\times$ 512</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv3, Pixelwise norm, Leaky Relu</td>
      <td style="text-align: center">6 $\times$ 6 $\times$ 256</td>
    </tr>
    <tr>
      <td style="text-align: left">Transposed conv3, Pixelwise norm, Leaky Relu</td>
      <td style="text-align: center">12 $\times$ 12 $\times$ 256</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv3, Pixelwise norm, Leaky Relu</td>
      <td style="text-align: center">12 $\times$ 12 $\times$ 128</td>
    </tr>
    <tr>
      <td style="text-align: left">Transposed conv3, Pixelwise norm, Leaky Relu</td>
      <td style="text-align: center">24 $\times$ 24 $\times$ 128</td>
    </tr>
    <tr>
      <td style="text-align: left">Transposed conv5, Pixelwise norm, Leaky Relu</td>
      <td style="text-align: center">48 $\times$ 48 $\times$ 64</td>
    </tr>
    <tr>
      <td style="text-align: left">Transposed conv5, Pixelwise norm, Leaky Relu</td>
      <td style="text-align: center">96 $\times$ 96 $\times$ 32</td>
    </tr>
    <tr>
      <td style="text-align: left">Transposed conv5, No norm, Tanh</td>
      <td style="text-align: center">192 $\times$ 192 $\times$ 3</td>
    </tr>

    <!---  Add empty line in final table row -->

    <tr>  
      <td style="text-align: left">$ $</td>
      <td style="text-align: center">$ $</td>
    </tr>
</tbody>
</table>

*Table 1.  Discriminator and generator network architectures used to
synthesize 192 $\times$ 192 images.*

Several points about this particular GAN architecture are worth noting:

i.  The discriminator [generator] network contains 7.1M [4.9M] trainable
variables.

ii.  The receptive field size equals 125 pixels for both neural networks.

iii.  The discriminator's final Sigmoid nonlinearity yields "real vs
synthetic" probabilities within the interval [0,1].  The generator's final
Tanh nonlinearity yields synthetic images values within the interval
[-1,1].

iv.  After trying out Relu, Leaky Relu [10], Exponential Linear Unit [11]
and Tanh functions for all other nonlinear superlayer activations, we found
Leaky Relu activations led to the best GAN performance.

v.  After trying out batch normalization [12], layer normalization [13] and
pixelwise feature vector normalization [1] within the networks'
superlayers, we observed that pixelwise normalization dramatically improved
GAN training stabilization.  As figure 2 illustrates, loss curves are well
behaved throughout training when discriminator and generator superlayers
are pixelwise normalized.  In contrast, neither batch nor layer
normalization sufficiently temper unstable competition between the two
networks.  Experiments in which these more familiar normalization
techniques were employed often exhibited bad learning curve jumps and/or
catastrophic mode collapse.

![BIRDGAN]({{site.url}}/blog/images/bird_dcgan/learning_curves_annot.png)
*Fig. 2.  Tensorboard plots of discriminator and generator losses as
functions of training iteration.*

In ref. [1], Karras *et al.* describe a GAN training methodology which
starts with 4 $\times$ 4 input pictures and one convolutional superlayer in
both the discriminator and generator networks.  These authors progressively
increase image size and numbers of network superlayers.  Karras *et al.*
persuasively argue that such incremental training enables a GAN to first
discover large-scale structure within an image distribution and slowly
learn finer scale detail.  Such progressive training yields better
performance than requiring a GAN to learn content from all scales at once.

We explore an alternative approach to gradual GAN training in which image
pixel size and network architectures are kept fixed.  Instead, we blur
discriminator input images with a gaussian kernel whose radius is a
function of training epoch.  As the GAN hyperparameters listed in Table 2
indicate, we take the kernel's initial radius to equal 10% of the training
image pixel size.  During the first 200 epochs, the blur kernel linearly
ramps down to zero.  No blurring of input imagery is performed for the last
300 training epochs.  Figure 3 plots this low-pass filtering schedule.

<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: left">Hyperparameter description</th>
      <th style="text-align: center">Value    </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Number of training images</td>
      <td style="text-align: center"> 11013 </td>
    </tr>
    <tr>
      <td style="text-align: left">Number of training epochs</td>
      <td style="text-align: center"> 500 </td>
    </tr>
    <tr>
      <td style="text-align: left">Minibatch size</td>
      <td style="text-align: center">64</td>
    </tr>
    <tr>
      <td style="text-align: left">Square image pixel size</td>
      <td style="text-align: center">192</td>
    </tr>
    <tr>
      <td style="text-align: left">Initial blur kernel radius in pixels</td>
      <td style="text-align: center">19.2</td>
    </tr>
    <tr>
      <td style="text-align: left">Adam optimizer $\beta_1$</td>
      <td style="text-align: center">0.0 </td>
    </tr>
    <tr>
      <td style="text-align: left">Adam optimizer $\beta_2$</td>
      <td style="text-align: center">0.99</td>
    </tr>
    <tr>
      <td style="text-align: left">Base learning rate</td>
      <td style="text-align: center">0.0001 </td>
    </tr>
    <tr>
      <td style="text-align: left">Descriptor dimension $Z_{dim}$</td>
      <td style="text-align: center">128 </td>
    </tr>
    <tr>
      <td style="text-align: left">Standard deviation for weight-initializing gaussian</td>
      <td style="text-align: center">0.02 </td>
    <tr>
      <td style="text-align: left">Leaky Relu slope coefficient</td>
      <td style="text-align: center">0.2</td>
    </tr>
  </tr>

</tbody> >
</table>

*Table 2.  GAN hyperparameter values.*

![BIRDGAN]({{site.url}}/blog/images/bird_dcgan/blur_schedule.png)
*Fig. 3.  Discriminator's blurring kernel radius linearly
decreases from its maximal starting value to zero during the first 200
training epochs.  No low-pass blurring is subsequently performed on input
discriminator images.*

The impact of tapered input image blurring upon output image generation is
qualitatively exhibited in figure 4.  Example discriminator inputs and
generator outputs at equally spaced training epochs along the low-pass
filtering schedule are displayed in the figure's top and bottom rows.
Since the GAN must learn to respond to input imagery with different
resolutions, we observe a delay of approximately 125 epochs between input
and output pictures with comparable bluriness.  But after training for 500
epochs, 192 $\times$ 192 bird images exported by the GAN are grossly
comparable their import counterparts.

![BIRDGAN]({{site.url}}/blog/images/bird_dcgan/discrim_in_gen_out.png)
*Fig. 4.  Example 192 $\times$ 192 discriminator inputs $($top row$)$ and
generator outputs $($bottom row$)$ at training epochs 0, 125, 250, 375 and
500.*

Figure 5 displays 64 examples of synthetic bird images.  These results
exhibit notable strengths and weaknesses.  We regard approximately
one-third of the generated bird pictures to be successful.  Among these
good instances, bird beaks, wings and tails look realistic, and plumage
coloring is highly detailed.  Moreover, the successful synthetic pictures
in the figure exhibit nontrivial variety in their foreground and background
contents.  Most birds appear to be sitting on tree branches or walking on
the ground in figure 5.  But we have also observed other cases where birds
appear to be standing near or floating on bodies of water.  The remaining
two-thirds of the generated birds within figure 5 range from mediocre to
horrible.

![BIRDGAN]({{site.url}}/blog/images/bird_dcgan/expt_325_output_270_birds.png)
*Fig. 5.  Examples of 192 $\times$ 192 synthesized bird images.*

We take some consolation in noting that bird results presented in ref. [1]
exhibit qualitatively similar strengths and weaknesses to ours.  Among the
bird images generated by Kerras *et al.* and copied here in figure 6,
roughly one-third look reasonable while the remaining two-thirds suffer
from significant distortions.  Consistently generating photo-realistic bird
pictures containing O$($200 $\times$ 200$)$ RGB pixels remains an
outstanding challenge.

![BIRDGAN]({{site.url}}/blog/images/bird_dcgan/PGGAN_birds.png)
*Fig. 6.  Screenshot of 256 $\times$ 256 generated bird images from figure
12 of ref. [1].*

## Binary classification of synthetic bird images


![BIRDGAN]({{site.url}}/blog/images/bird_dcgan/labeled_chips.png)
*Fig. 7.  Sample generator outputs manually labeled as acceptable $($top row$)$ and
unacceptable $($bottom row$)$.*


![BIRDGAN]({{site.url}}/blog/images/bird_dcgan/decent_generated_birds.png)
*Fig. 8.  Inference 192 $\times$ 192 bird images with binary classifier
softmax scores exceeding 0.95.*


![BIRDGAN]({{site.url}}/blog/images/bird_dcgan/bird_morphings.png)
*Fig. 9.  Linear descriptor interpolation induces smooth transitions
between starting and ending synthetic bird images.*

## References

1. [T. Karras, T. Aila, S. Laine and J. Lehtinen, "Progressive Growing of
GANs for Improved Quality, Stability and Variation", ICLR $\small{(2018)}$.](https://arxiv.org/pdf/1710.10196.pdf)

2. [I. Goodfellow, J.P.-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
A. Courville and Y. Bengio, "Generative Adversarial Nets," Advances in
Neural Information Processing Systems
$\small{(2014)}$.](https://arxiv.org/pdf/1406.2661.pdf)

3. [Z. Liu, P. Luo, X. Wang and W. Tang, "Large-scale CelebFaces Attributes
$($CelebA$)$ Dataset,"
$\small{(2015)}$.](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)

4.  [C. Metz and K. Collins, "How an A.I. 'Cat-and-Mouse Game' Generates
Believable Fake Photos", New York Times
$\small{(2018)}.$](https://www.nytimes.com/interactive/2018/01/02/technology/ai-generated-photos.html)

5.  [F. Yu, A. Seff, Y. Zhang, S. Song, T. Funkhouser and J. Xiao, "LSUN:
Construction of a Large-Scale Image Dataset using Deep Learning with Humans
in the Loop", $\small{(2015)}.$](https://arxiv.org/pdf/1506.03365.pdf)

6.  [T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford and X.
Chen, "Improved Techniques for Training GANs", NIPS $\small{(2016)}$.](https://arxiv.org/pdf/1606.03498.pdf)

7. [C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. “The
Caltech-UCSD Birds-200-2011 Dataset.” Computation & Neural Systems
Technical Report, CNS-TR-2011-001.](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html)

8. [A. Radford, L. Metz and S. Chintala, "Unsupervised Representation
Learning with Deep Convolutional Generative Adversarial Networks," ICLR
$\small{(2016)}$.](https://arxiv.org/pdf/1511.06434.pdf)

9.  [TensorFlow: An open-source software library for Machine Intelligence.](https://www.tensorflow.org/)

10.  [A.L. Maas, A.Y. Hannun and A.Y. Ng, "Rectifier Nonlinearities Improve
Neural Network Acoustic Models", 30th ICML $\small{(2013)}$.](https://web.stanford.edu/~awni/papers/relu_hybrid_icml2013_final.pdf)

11. [D.-A. Clevert, T. Unterthiner and S. Hochreiter, "Fast and Accurate
Deep Network Learning by Exponential Linear Units," arXiv:1511.07289 $\small{(2016)}$.](
https://arxiv.org/pdf/1511.07289.pdf)

12.  [S. Ioffe and C. Szegedy, "Batch Normalization: Accelerating Deep
Network Training by Reducing Internal Covariate Shift," 32nd ICML 
$\small{(2015)}$.](https://arxiv.org/abs/1502.03167)

13. [J. L. Ba, J. R. Kiros and G.E. Hinton, "Layer Normalization,"
arXiv:1607.06450 $\small{(2016)}$.](https://arxiv.org/abs/1607.06450.pdf)

14.  [See "Classifying Cat and Dog Images via an Ensemble of CNNs" post in
this blog series $\small{(2017)}$.](https://petercho.github.io/blog//deep/learning/2017/11/25/cvml.html)
