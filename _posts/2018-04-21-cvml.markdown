---
layout: post
title:  "Gradually Increasing Image Resolution During GAN Training"
date:   2018-04-21 12:00:00
categories: Deep Learning
use_math: true
---

## Progressive GAN training

In October 2017, Karras, Aila, Laine and Lehtinen published a paper
entitled "Progressive Growing of GANS for Improved Quality, Stability and
Variation" which presented new high-resolution results for synthetically
generated images [1].  These authors described a novel training methodology
for generative adversarial networks [2].  It begins by training shallow
discriminator and generator networks on low resolution imagery inputs.  New
layers in both networks are subsequently introduced and trained in order to
model higher resolution details.  Working with an enhanced version of the
popular CelebA dataset [3], Karras *et al.* taught their "Progressive GAN"
$\small{\rm (PGGAN)}$ to output 1024 $\times$ 1024 facial images.  Their
stunning synthesized faces were soon featured in the New York Times [4].

Inspired by ref. [1], we explore in this blog posting an alternative
approach to increasing image resolution during GAN training.  Unlike Karras
*et al.*, we keep the discriminator and generator network architectures
fixed throughout training.  But the resolution of images fed into the
discriminator gradually increases as training proceeds.  So our GAN
initially learns gross object shapes, colors and textures from
intentionally blurred input imagery.  It later refines object parts and
details as low-pass filtering is diminished and training proceeds to
completion.

In Appendix G of ref. [1], generated 256 $\times$ 256 views are displayed
of other objects besides faces based upon 30 LSUN categories [5].  Karras
*et al.* trained separate networks for each object class using at least
100K images.  Though the overall quality of the synthetic pictures is
impressive, several exhibit obvious flaws.  Distortions within generated
animal images are particularly noticeable.  Other investigators have
previously noted that GANs often fail to reproduce basic animal anatomy
[6].

Motivated by this animal synthesis challenge, we focus upon bird rather
than face imagery in our investigation.  We work with a 2011 Caltech-UCSD
dataset containing 11K+ bird images [7].  As the representative examples in
figure 1 illustrate, bird pictures from this corpus exhibit much greater
variety in 3D pose, foreground color and background content than do
pre-aligned CelebA photos.  As we shall demonstrate, our simple approach to
gradual GAN training yields synthetic bird images that qualitatively appear
at least as good as those in ref. [1] generated with 9X more input data.

![BIRDGAN]({{site.url}}/blog/images/bird_dcgan/input_bird_chips.png)
*Fig. 1.  Tensorboard display of sample images from the
2011 Caltech-UCSD birds dataset.*

## Bird imagery synthesis

The DCGAN architecture we adopted closely follows that of Radford et al
[4].  The particular "super layer" combinations of convolutions,
normalizations and nonlinearities in our DCGAN discriminator and generator
networks for synthesizing 64x64 images are listed in Table 1.  The
receptive fields of these networks equals 61, and they contain O$($36.6M$)$
trainable parameters.  We employed similar discriminator and generator
architectures for WGAN experiments.  But following [8], we utilized a
gradient penalty term within the loss function rather than weight clipping
as in the original WGAN paper [7].  Moreover, we replaced batch
normalization with layer normalization and dispensed with a final sigmoid
activation in the WGAN discriminator.

<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: left">GAN discriminator super layer</th>
      <th style="text-align: center">Output shape [width $\times$ height $\times$ channels] </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Input image</td>
      <td style="text-align: center">192 $\times$ 192 $\times$ 3</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu, stride 2</td>
      <td style="text-align: center">96 $\times$ 96 $\times$ 32</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu, stride 2</td>
      <td style="text-align: center">48 $\times$ 48 $\times$ 64</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv5, Pixelwise norm, Leaky Relu, stride 2</td>
      <td style="text-align: center">24 $\times$ 24 $\times$ 128</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv3, Pixelwise norm, Leaky Relu, stride 1</td>
      <td style="text-align: center">24 $\times$ 24 $\times$ 256</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv3, Pixelwise norm, Leaky Relu, stride 2</td>
      <td style="text-align: center">12 $\times$ 12 $\times$ 256</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv3, Pixelwise norm, Leaky Relu, stride 1</td>
      <td style="text-align: center">12 $\times$ 12 $\times$ 512</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv3, Pixelwise norm, Leaky Relu, stride 2</td>
      <td style="text-align: center">6 $\times$ 6 $\times$ 512</td>
    </tr>
    <tr>
      <td style="text-align: left">Linear proj</td>
      <td style="text-align: center">$Z_{dim}$</td>
    </tr>
    <tr>
      <td style="text-align: left">Linear proj, Sigmoid</td>
      <td style="text-align: center">1</td>
    </tr>

    <!---  Add empty line in final table row -->

    <tr>
      <td style="text-align: left">$ $</td>
      <td style="text-align: center">$ $</td>
    </tr>
</tbody>
</table>

<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: left">GAN generator super layer</th>
      <th style="text-align: center">Output shape [width $\times$ height $\times$ channels] </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Input descriptor</td>
      <td style="text-align: center">$Z_{dim}$</td>
    </tr>
    <tr>
      <td style="text-align: left">Linear proj, Pixelwise norm, Leaky Relu</td>
      <td style="text-align: center">6 $\times$ 6 $\times$ 512</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv3, Pixelwise norm, Leaky Relu</td>
      <td style="text-align: center">6 $\times$ 6 $\times$ 256</td>
    </tr>
    <tr>
      <td style="text-align: left">Transposed conv3, Pixelwise norm, Leaky Relu</td>
      <td style="text-align: center">12 $\times$ 12 $\times$ 256</td>
    </tr>
    <tr>
      <td style="text-align: left">Conv3, Pixelwise norm, Leaky Relu</td>
      <td style="text-align: center">12 $\times$ 12 $\times$ 128</td>
    </tr>
    <tr>
      <td style="text-align: left">Transposed conv3, Pixelwise norm, Leaky Relu</td>
      <td style="text-align: center">24 $\times$ 24 $\times$ 128</td>
    </tr>
    <tr>
      <td style="text-align: left">Transposed conv5, Pixelwise norm, Leaky Relu</td>
      <td style="text-align: center">48 $\times$ 48 $\times$ 64</td>
    </tr>
    <tr>
      <td style="text-align: left">Transposed conv5, Pixelwise norm, Leaky Relu</td>
      <td style="text-align: center">96 $\times$ 96 $\times$ 32</td>
    </tr>
    <tr>
      <td style="text-align: left">Transposed conv5, No norm, Tanh</td>
      <td style="text-align: center">192 $\times$ 192 $\times$ 3</td>
    </tr>

    <!---  Add empty line in final table row -->

    <tr>  
      <td style="text-align: left">$ $</td>
      <td style="text-align: center">$ $</td>
    </tr>
</tbody>
</table>

*Table 1.  Discriminator and generator architectures used to synthesize
bird images.*



The three types of GANs with which we work depend upon a sizable set of
hyperparameters.  Some hyperparameters are particular to only one of the
GANs.  For example, the diversity ratio $\gamma$ regulates the BEGAN
discriminator's balance between autoencoding real images and distinguishing
synthetic from genuine images.  But others like those listed in Table 2
influence all DCGAN, WGAN and BEGAN training experiments.  We settled upon
the tabulated hyperparameter values via trial and error.

<table style="width:100%">
  <thead>
    <tr>
      <th style="text-align: left">Hyperparameter description</th>
      <th style="text-align: center">Value    </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Number of training images</td>
      <td style="text-align: center"> 11013 </td>
    </tr>
    <tr>
      <td style="text-align: left">Number of training epochs</td>
      <td style="text-align: center"> 500 </td>
    </tr>
    <tr>
      <td style="text-align: left">Minibatch size</td>
      <td style="text-align: center">64</td>
    </tr>
    <tr>
      <td style="text-align: left">Adam optimizer $\beta_1$</td>
      <td style="text-align: center">0.0 </td>
    </tr>
    <tr>
      <td style="text-align: left">Adam optimizer $\beta_2$</td>
      <td style="text-align: center">0.99</td>
    </tr>
    <tr>
      <td style="text-align: left">Base learning rate</td>
      <td style="text-align: center">0.0001 </td>
    </tr>
    <tr>
      <td style="text-align: left">Descriptor dimension $Z_{dim}$</td>
      <td style="text-align: center">128 </td>
    </tr>
    <tr>
      <td style="text-align: left">Standard deviation for weight-initializing gaussian</td>
      <td style="text-align: center">0.02 </td>
    <tr>
      <td style="text-align: left">Leaky Relu slope coefficient</td>
      <td style="text-align: center">0.2</td>
    </tr>
  </tr>

</tbody>
</table>

*Table 2.  Hyperparameter values employed for GAN training.*

![BIRDGAN]({{site.url}}/blog/images/bird_dcgan/discrim_in_gen_out.png)
*Fig. 2.  $($Top row$)$: Discriminator's blurring kernel radius linearly
decreases from its maximal starting value to zero during the first 200
training epochs.  No low-pass blurring is subsequently performed on input
discriminator images.  $($Middle and bottom rows$)$: Example discriminator
inputs and generator outputs at training epochs 0, 125, 250, 375 and 500.*


![BIRDGAN]({{site.url}}/blog/images/bird_dcgan/learning_curves_annot.png)
*Fig. 3.  Tensorboard display of discriminator and generator losses plotted
as functions of training iteration.*



![BIRDGAN]({{site.url}}/blog/images/bird_dcgan/expt_325_output_270_birds.png)
*Fig. 4.  Examples of 192 $\times$ 192 generated bird images.*


![BIRDGAN]({{site.url}}/blog/images/bird_dcgan/PGGAN_birds.png)
*Fig. 5.  Screenshot of 256 $\times$ 256 generated bird images from figure
12 of ref. [1].*


## Binary classification of synthetic bird images


![BIRDGAN]({{site.url}}/blog/images/bird_dcgan/labeled_chips.png)
*Fig. 6.  Sample generator outputs manually labeled as acceptable $($top row$)$ and
unacceptable $($bottom row$)$.*


![BIRDGAN]({{site.url}}/blog/images/bird_dcgan/decent_generated_birds.png)
*Fig. 7.  Inference 192 $\times$ 192 bird images with binary classifier
softmax scores exceeding 0.95.*


![BIRDGAN]({{site.url}}/blog/images/bird_dcgan/bird_morphings.png)
*Fig. 8.  Linear descriptor interpolation induces smooth transitions
between starting and ending synthetic bird images.*

## References

1. [T. Karras, T. Aila, S. Laine and J. Lehtinen, "Progressive Growing of
GANs for Improved Quality, Stability and Variation", ICLR $\small{(2018)}$.](https://arxiv.org/pdf/1710.10196.pdf)

2. [I. Goodfellow, J.P.-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
A. Courville and Y. Bengio, "Generative Adversarial Nets," Advances in
Neural Information Processing Systems
$\small{(2014)}$.](https://arxiv.org/pdf/1406.2661.pdf)

3. [Z. Liu, P. Luo, X. Wang and W. Tang, "Large-scale CelebFaces Attributes
$($CelebA$)$ Dataset,"
$\small{(2015)}$.](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)

4.  [C. Metz and K. Collins, "How an A.I. 'Cat-and-Mouse Game' Generates
Believable Fake Photos", New York Times
$\small{(2018)}.$](https://www.nytimes.com/interactive/2018/01/02/technology/ai-generated-photos.html)

5.  [F. Yu, A. Seff, Y. Zhang, S. Song, T. Funkhouser and J. Xiao, "LSUN:
Construction of a Large-Scale Image Dataset using Deep Learning with Humans
in the Loop", $\small{(2015)}.](https://arxiv.org/pdf/1506.03365.pdf)

6.  [T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford and X.
Chen, "Improved Techniques for Training GANs", NIPS $\small{(2016)}$.](https://arxiv.org/pdf/1606.03498.pdf)

7. [C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. “The
Caltech-UCSD Birds-200-2011 Dataset.” Computation & Neural Systems
Technical Report, CNS-TR-2011-001.](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html)



4. [A. Radford, L. Metz and S. Chintala, "Unsupervised Representation
Learning with Deep Convolutional Generative Adversarial Networks," ICLR
$\small{(2016)}$.](https://arxiv.org/pdf/1511.06434.pdf)

5.  [S. Ioffe and C. Szegedy, "Batch Normalization: Accelerating Deep
Network Training by Reducing Internal Covariate Shift," 32nd ICML 
$\small{(2015)}$.](https://arxiv.org/abs/1502.03167)

6.  [A.L. Maas, A.Y. Hannun and A.Y. Ng, "Rectifier Nonlinearities Improve
Neural Network Acoustic Models", 50th ICML $\small{(2013)}$.](https://web.stanford.edu/~awni/papers/relu_hybrid_icml2013_final.pdf)

12.  [See TensorFlow: An open-source software library for Machine Intelligence.](https://www.tensorflow.org/)

13.  [V. Dumoulin and F. Visin, "A Guide to Convolution Arithmetic for Deep
Learning," arXiv:1603.07285 v1 $\small{(2016)}$.](https://ai2-s2-pdfs.s3.amazonaws.com/7918/2aab186f0b68a8432540d8695e1646338479.pdf)

15. [J. L. Ba, J. R. Kiros and G.E. Hinton, "Layer Normalization,"
arXiv:1607.06450 $\small{(2016)}$.](https://arxiv.org/abs/1607.06450.pdf)
